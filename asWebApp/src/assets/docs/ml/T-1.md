# Model representation

### Notation:

>  m= # of training examples   
>  X= input variables or features   
>  y=output or target   
>  (x(i),y(i)) for the i<sup>th</sup> training example

- Training set is the input to learning algorithm  -> generate an hypothesis that will be used to map from x to y
- how to represent h the hypothesis?
    for example h is a linear function of x;  

<img src="http://latex.codecogs.com/svg.latex?\Large h_\Theta(x)=\Theta_0 + \Theta_1  x"></img>


---

### Cost function:

Its goal is to search the value of &Theta;<sub>0</sub> and &Theta;<sub>1</sub> so that h(x) is close to y for the training set (x,y).  
The problem is to minimize the following cost function that could be the sum squared errors between the outcomes and the target label:  

<img src="http://latex.codecogs.com/svg.latex?\Large J(\Theta_0, \Theta_1)= \frac{1}{2 m} \sum_{i=1}^{m} (h_\theta(x^{i}) - y^{i})^2"></img>

hypothesis function is of X while the cost function is of parameter Theta

In python the cost and hypothesis functions are:
```python
def costF(y,output):
  errors = (y - output)
  return (errors**2).sum() / 2.0

def netInput(self,X):
  # calculate the vector dot product wT * x, or equivalent to
  # compute z = sum(x(i) * w(i)) for i from 1 to n, add the threshold
  return np.dot(X,self.weights[1:]) + self.weights[0]
```
The cost function is convex continuous linear and can be derived, so that we can use the gradient descent algorithm to find the local minima.
The weight difference is computed as the negative gradient * the learning rate eta. To compute the gradient of the cost function, we need to compute the partial derivative of the cost function with respect to each weight w(j). So putting all together we have:
<img src="http://latex.codecogs.com/svg.latex?\Large \Delta  w_j = - \eta \frac {\delta J} {\delta w_j} = \eta \sum_{i=1}^{m} (y^i - \phi(z^i) ) x_j^i"></img>

the weight update is calculated based on all samples in the training set (instead of updating the weights incrementally after each sample), which is why this approach is also referred to as "batch" gradient descent.
So basically to minimize the cost function we took steps into the opposite direction of a gradient calculated from the entire training set.

---

### Gradient descent

It is the algorithm to minimize the cost function. It uses the property of the cost function being continuous convex linear, so differentiable:

<img src="assets/docs/ml/images/gradient-descent.png" width=400></img>

It can be describe as climbing down a hill until a local or global cost minimum is reached. In each iteration, we take a step away from the gradient where the step size is determined by the value of the learning rate as well as the slope of the gradient.

* start some &Theta;<sub>0</sub> and &Theta;<sub>1</sub>
* keep changing &Theta;<sub>0</sub> to &Theta;<sub>1</sub> to reduce J(&Theta;<sub>0</sub>, &Theta;<sub>1</sub>) until reaching a minimum:

 <img src="http://latex.codecogs.com/svg.latex?\Large \Theta_j := \Theta_j - \alpha\frac{\delta}{\delta\Theta_j} J(\theta_0, \Theta_1)"></img> (for j = 0 and  j = 1)


&alpha; is the learning rate, and corresponds to the step size to go downhill.
if &Theta;<sub>0</sub> = 0, the problem is to find the min of J(&Theta;<sub>1</sub>).


The approach is to use a 'simultaneous update' to avoid using modified &Theta;<sub>0</sub> to compute new value for &Theta;<sub>1</sub>. So keep &Theta;<sub>0</sub> original value.


The fraction factor is the derivative of the function J: the slope of the tangent at the curve on point &Theta;<sub>j</sub>.

If &alpha; is too big, gradient descent can overshoot the minimum and fails to converge or worse it could diverge.
when J(&Theta;<sub>0</sub>,&Theta;<sub>1</sub>) is already at the local minimum the slope of the tangent is 0 so &Theta;<sub>j</sub> will not change.


When going closer to the local minimum the slope of the tangent will go slower so the algo will automatically take smaller step.


When using linear regression there is a unique global minima for the cost function.


Linear algebra is the notation/ mathematical model to use to define the features: the set of feature is a matrix called X. and Y, representing the trained results, is a vector.

---

### Linear regression with multiple variables

When the number of features is more than one the problem becomes a linear regression
   n      is the number of features
   X<sup>(i)</sup>= features of i<sup>th</sup> training (i<sup>th</sup> row or vector)

   X<sub>j</sub><sup>(i)</sup> = value of feature j in i<sup>th</sup> training

the hypothesis is now becoming:  
<img src="http://latex.codecogs.com/svg.latex?\Large h_\Theta(x)=\Theta_0 X_0+ \Theta_1 X_1 + ... + \Theta_k X_k"></img>

Xo = 1 so a feature is a vector and T is also a row vector of dimension n+1 so H is a matrix multiplication: it is called **multivariate linear regression**

The gradient descent algorithm for a multiple features problem looks like

repeat {  
  <img src="http://latex.codecogs.com/svg.latex?\Large \Theta_j := \Theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{i}) - y^{i}) x_j^{i}"></img>

  simultaneously update &Theta;<sub>j</sub> for j = 0,...n  
}

when the unit of each feature are very different the gradient descent will take a lot of time to find the minima. So it is important to transform each feature so they are in the same scale. (close to: from -1 to 1 range)


> Normalization of ratings means adjusting values measured on different scales to a notionally common scale, often prior to averaging. In another usage in statistics, normalization refers to the creation of shifted and scaled versions of statistics, where the intention is that these normalized values allow the comparison of corresponding normalized values for different datasets in a way that eliminates the effects of certain gross influences, as in an anomaly time series.

Feature scaling used to bring all values into the range [0,1]. This is also called unity-based normalization


X'=(X-X<sub>min</sub>)/(X<sub>max</sub>-X<sub>min</sub>)

---

### Polynomial regression  

the hypothesis function could be a polynomial. One problem is to select the pertinent features.
There are algorithms to help to pick up features automatically.
See notebook [polynomial regression](https://github.com/jbcodeforce/ml-basics/blob/master/DataScience-Python3/PolynomialRegression.ipynb)

---

### Normal equation

is the method to solve &Theta; analytically. It is a real.
As an example to minimize a quadratic function:  

<img src="http://latex.codecogs.com/svg.latex?\Large J(Theta) = a \Theta^2 + b\Theta + c"></img>

is to find the value of &Theta; where the derivative of J is 0

The Normal Equation:

<img src="http://latex.codecogs.com/svg.latex?\Large Theta = ( X^T X)^{-1} X^T y "></img>

the advantage of normal equation over gradient descent is that you do not need to choose alpha, and do not need to iterate. It could be slow for n > 10,000 as it needs to compute the inverse of X<sup>T</sup>*X, which is close to O(n<sup>3</sup>)

In normal equation X may not be invertible. This could happen when two features are redundant or when the number of feature is too high.

```
With n=200000 features, you will have to invert a 200001Ã—200001 matrix to compute the normal equation.
Inverting such a large matrix is computationally expensive, so gradient descent is a good choice.
```
