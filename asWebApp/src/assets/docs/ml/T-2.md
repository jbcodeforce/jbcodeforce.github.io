# Classification

### Perceptron
See jupyter notebook, [classifiers](https://github.com/jbcodeforce/ml-basics/blob/master/ml-python/classifiers/classifiers.ipynb), to get the full explanation of the Perceptron and ADALine classifiers.  
Perceptron is based on the human neuron model, Frank Rosenblatt proposed an algorithm that would automatically learn the optimal weight coefficients that are then multiplied with the input features in order to make the decision of whether a neuron fires or not. In the context of supervised learning and classification, such an algorithm could then be used to predict if a sample belonged to one class or the other.
The problem is reduced to a binary classification (-1,1), and an activation function that takes a linear combination of input X, with corresponding weights vector W, to compute the net input

<img src="http://latex.codecogs.com/svg.latex?\Large z = \sum_{i=1}^{n}w_i * x_i"></img>

If the value is greater than a threshold the output is 1, -1 otherwise.
The function is called **unit step** function.

A second implementation in pure python is in [this class](https://github.com/jbcodeforce/ml-basics/blob/master/ml-python/classifiers/Perceptron.py)

It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small.

---

### ADAptive LInear NEuron classifier

In ADALINE the weights are updated based on a linear activation function (the identity function) rather than a unit step function like in the perceptron.
See second party of the jupyter notebook, [classifiers](https://github.com/jbcodeforce/ml-basics/blob/master/ml-python/classifiers/classifiers.ipynb) and the class: [Adaline.py](https://github.com/jbcodeforce/ml-basics/blob/master/ml-python/classifiers/Adaline.py)

When the features are standardized (each feature value is reduced by the mean and divided by the standard deviation) the ADALine algorithm converges more quickly.

```
X_std = np.copy(X)
X_std[:,0]=(X[:,0]-np.mean(X[:,0]))/np.std(X[:,0])
X_std[:,1]=(X[:,1]-np.mean(X[:,1]))/np.std(X[:,1])
```

When the data set includes millions of records a more efficient approach  is to take the stochastic gradient descent. It is used with online training, where the algorithm is trained on-the-fly, while new training set arrives.
The weights are computed with:   
<img src="http://latex.codecogs.com/svg.latex?\Large \eta (y^i - \phi(z^i) ) x_j^i"></img>

```Python
def updateWeights(self,xi,target):
        output = self.netInput( xi)
        error = (target - output)
        self.weights[1:] += self.eta * xi.dot( error)
        self.weights[0] += self.eta * error
        cost = (error** 2)/ 2.0
        return cost
```

To obtain accurate results via stochastic gradient descent, it is important to present it with data in a random order, which is why we want to shuffle the training set for every epoch to prevent cycles.


### Naive Bayes Spam Classifier
Apply the Bayes's theorem as:

<img src="http://latex.codecogs.com/svg.latex?\Large P(A|B) = P(A) * (B | A) / P(B)"></img>

where P(A|B) is the probability of A being true given B is true. P(B) the probability of having B true under all possible cases.  

The Naive Bayes algorithm computes the probability for each attribute to belong to each class. It is using a supervised learning approach. It assumes that the probability of each attribute belonging to a given class is independent of all other attributes.

See code [NaiveBayesClassifier.py](https://github.com/jbcodeforce/ml-basics/blob/master/ml-python/classifiers/NaiveBayesClassifier.py) and the Jupyter notebook: [NaiveBayes](https://github.com/jbcodeforce/ml-basics/blob/master/ml-python/DataScience-Python3/NaiveBayes.ipynb) is using sklearn to do the spam.

For spam becomes:
<img src="http://latex.codecogs.com/svg.latex?\Large P(spam | free) = P(spam) * (free | spam) / P(free)"></img>
The numerator is the probability of a message being a spam and containing the word free. The denominator is the overall probability that a message has the word free in it.

### Entropy
To assess if a condition within a decision tree (a node) is helpful or not we can use the entropy. Entropy: helps to compute the amount of unpredictability of a data set. The higher the entropy is, the events being measured are less predictable. Mathematically it is the function:

<img src="http://latex.codecogs.com/svg.latex?\Large entropy = - sum_{i=1}^{n}p_x*log_2(p_x)"></img>

Px is the probability of event to occur. For a Die 6, the entropy is  -6 & (1/6 * log 2 (1/6))  = 2.58... it is more unpredictable than flipping a coin for example that has a entropy of 1.

### Decision Tree
It is a supervised classification algorithm used as a flowchart to decide about the class. To build an efficient decision tree we will pick the attribute with the highest information gain: the one reducing the entropy the most, and place condition on this attribute at the top of the tree.
The algorithm to build dynamically a decision tree is the [ID3 algorimth in wikipedia](http://en.wikipedia.org/wiki/ID3_algorithm) or [the a tutorial video](https://www.youtube.com/watch?v=wL9aogTuZw8).

Decision trees are prone to overfitting. To address this problem we can construct several alternate trees and let them 'vote' for the final classification. To build alternate solution it is possible to randomly re-sample the input data (bootstrap aggregating or bagging) and/or randomly select a subset of the attributes to asses at each step of the tree construction.
See [this notebook](https://github.com/jbcodeforce/ml-basics/blob/master/ml-python/DataScience-Python3/DecisionTree.ipynb)

### Ensemble Learning
Random forest is an example ensemble learning, this means we are using multiple models to solve the same problem and we let them to vote on the results. It is an efficient way to find a best model.
**Bagging** (randomly-drawn subsets of the data) is one implementation of the ensemble learning. **Boosting** is an alternate technique where each subsequent model boosts attributes that address data mis-classified by the previous model.
**A bucket of models** trains several different models using training data and picks the one that works best with the test data. The model that wins.
**Stacking** runs multiple models at once on the data and combines the results together.

### Support Vector Machine
Used to classify higher dimensional data with a lot of features.  
In SVM the goal is now to maximize the margin: the distance between the decision boundary and the training samples.

<img src="assets/docs/ml/images/svn.png" width=430></img>

The rationale behind having decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting.

To train a SVM model
```
from sklearn.svm import SVC
svm = SVC(kernel='linear',C=1.0,random_state=0)
svm.fit(X_train_std,y_train)
```
