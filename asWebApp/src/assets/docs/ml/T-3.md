# Unsupervised learning - clustering

The training set does not have labels. So the goal becomes find cluster of data from the training set. It can be used:

* for market segmentation
* social network analysis
* organize computing cluster
* astronomy data analysis
* use the K-means algorithm for image compression by reducing the number of colors that occur in an image to only those that are most common in that image.

## K-Means Clustering

The K-means algorithm is a method to automatically split data into K groups that are closest to the K centroids. It is an iterative procedure that starts by guessing the initial centroids, and then improves this guess by repeatedly assigning examples to their closest centroids and then recomputing the centroids based on the assignments.

Algorithm input: K is the number of clusters to use, and the training set X with i from 1 to m

- randomly pick K centroids u(1) to u(k)
- repeat until points stop changing assignment to centroids
    - assign X(i) to cluster centroid of index k, closest to X(i). This means
    <img src="http://latex.codecogs.com/svg.latex?\Large min( ||X(i) - U(k)||^2)"></img>

    - recompute the centroid: for k: 1 to K move centroids to the average of points assigned to cluster k

The K-means algorithm will always converge to some nominal set of means for the centroids. The converged solution may not always be ideal and depends on the initial setting of the centroids. Therefore, in practice the K-means algorithm is usually run a few times with different random initializations.

The notebook to illustrate how to do k-means with sklearn:  [KMeans](https://github.com/jbcodeforce/ml-basics/blob/master/ml-python/DataScience-Python3/KMeans.ipynb)

The challenges are:

* to choose K: To choose the number of cluster, there is always ambiguity, and not always a clear cut. The  Elbow method could be used:
 - vary K and compute the cost function J
 - at the elbow point the number of cluster may be the right number.
* Avoiding local minima: If we run k-means using k=3 and k=5, but found the cost function J is much higher at k=5, so it means that k-means was stuck in a local minimum, and it is better to try re-running the k-means with multiple random initializations
* Labeling the clusters: needs to dig into the data to figure out.


Better way to find, is to assess for what purpose the clustering is needed. So the number of cluster can be easily determine from the goals of the classification.

## Entropy

A measure of data set's disorder. It is 0 if all the classes in the data are the same. It is higher, when classes are different.

## Recommender Systems

Help users discover things that they might not have known before.

### User based collaborative filtering

The first approach is called user-based collaborative filtering: the goal is to build a matrix of things each user bought/viewed/rated and compute similarity scores between users, find users close to you and recommend stuff they bought/viewed/rated that you haven't yet done. Columns are items, row users.    
Comparing people to people lead to some problems, people are fickle, their tastes change, and may do bad thing too (bias to build fake persona, schilling attack). The computation to  find user is huge as they are much more rows than columns.

### Item based collaborative filtering

Focus on relationship between items when user likes or do not like those items. Find every pair of item a user view or bought. Measure the similarity of their ratings across all users who bought/view the same two items. Sort by items and strength.

Items are more stable than human, and they are less. It is also harder to trick the system.
See [ItemBasedCF.ipynb](https://github.com/jbcodeforce/ml-basics/blob/master/DataScience-Python3/ItemBasedCF.ipynb)
 
