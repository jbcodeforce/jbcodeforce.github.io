{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This web site groups a set of studies, notes, technology summaries, references that I am using on my day to day work as IT architect and developer. This is a \"pense bete\". It can be considered as a set of blogs, but without the deep quality of a blog on Medium or Wordpress. Some of the links in the navigation bar goes to other sites, I developed over time to help customer, and more tailored to a specific subject like Event-driven architecture, AWS, messaging or AI.</p> <p>If you want to leverage this content, better use the search or navigate into the top level menu.</p> <p>Feel free to contact me if you want some help on one of those subjets. I may be able to help.</p> <p>A bientot.</p>"},{"location":"about/","title":"Home page","text":""},{"location":"about/#about-me","title":"About me","text":"<p> I am an accomplished technology professional with a diverse background in event-driven architecture (EDA), hybrid cloud solutions (kubernetes and AWS services), business process and business rule implementation. With a strong track record as a former IBM Distinguished Engineer and my current role as a Principal Solution Architect at AWS, my expertise lies in guiding customers through their hybrid cloud journey, including messaging, data pipeline, decision systems and AI model development and integration.</p> <p>In my career, I have had the pleasure of assisting customers in a consultative manner, helping them adopt decision management, business process automation, data streaming, and complex event processing. I have a deep understanding of lean and agile methodologies, having authored books and blogs on agile business rule solution development, and being part of IBM Garage from the creation. Additionally, I have engaged with customers to facilitate the adoption of asynchronous and reactive systems based on IBM middleware, AWS serverless services, containers, and Kubernetes deployments.</p> <p>I led a lot of event-storming, and domain-driven design workshops with customers willing to innovate and break their monolithic applications. I also created an agile methodology for developing intelligent automation solutions, and used by IBM automation product consultants for years.</p> <p>Furthermore, I have contributed to the definition of the next generation of several middleware products, showcasing my ability to drive innovation. I have led the development of IBM reference architectures for event-driven solution, Cognitive, and Data and AI. </p>"},{"location":"about/#how-i-can-help","title":"How I can help","text":"<p>Here is a summary of how I can help your organization:</p> <ul> <li> <p>Leading Intelligent Automation Solutions: I specialize in guiding customers towards developing cutting-edge automation solutions by leveraging machine learning, LLM, expert systems, and agents. My expertise lies in creating deployable solutions in hybrid cloud environments (AWS and on-premises or kubernetes deployment).</p> </li> <li> <p>Enabling Cloud Adoption: I have a proven track record of assisting strategic customers in embracing cloud technologies and deploying their own software as a service (SaaS) solutions. I can provide valuable insights and guidance to help organization leverage the power of the cloud effectively.</p> </li> <li> <p>Architectural Workshops and Prototyping: By leading architecture workshops, I ensure that solutions are aligned with business requirements and demonstrate their business value through the creation of minimum viable products (MVPs). This approach allows for rapid validation and helps in making informed project decisions.</p> </li> <li> <p>Event Storming and Domain-Driven Design: I excel in facilitating workshops focused on event storming and domain-driven design, which enable the identification of key business domains, bounded context and microservices. These workshops foster collaboration and innovation within teams.</p> </li> <li> <p>Technical Relationships and Mentoring: I have extensive experience in building and nurturing long-term, deep technical relationships with C level audience. Furthermore, I enjoy mentoring developers, consultants, and technical sellers, empowering them to enhance their skills and excel in their roles.</p> </li> <li> <p>Knowledge Sharing and Best Practices: I strongly believe in capturing and sharing best practices and tribal knowledge within teams. By doing so, I actively contribute to the accelerated learning and increased productivity of team members.</p> </li> <li> <p>Customer-Facing Publications: I have a passion for authoring, presenting, and contributing to customer-facing publications, particularly for conferences. This allows me to share insights, experiences, and innovative solutions with a broader audience.</p> </li> </ul> <p>As a technical expert, I sell, create, and lead First of a Kind cloud projects and resolve customers\u2019 critical issues. I harvest architecture, best practices, and lessons learned from customer's experiences.</p> <p>I try to commit myself to be a trusted advisor, so I focus to continuously deeply learn new technologies, programming habits, and agile practices. I try to code and play with new things on a daily basis. The goal is to help customers to innovate and bring business values with new technologies.</p> <p>Outside of work, I am a devoted parent to three children, with my youngest daughter at WashU in St Louis studying environmental engineering, my sone currently pursuing a master in aerospace engineering at Cornell University, and my oldest daughter working as a software engineer in France. In my leisure time, I enjoy skiing, golfing, sailing, hiking, and engaging in family board games. </p> <p>Thank you for taking the time to read my bio. I look forward to the opportunity to contribute my skills and experience to new and exciting projects. </p>"},{"location":"about/#publications","title":"Publications","text":""},{"location":"about/#books","title":"Books","text":"<ul> <li>Co Author: Sustainable IT Architecture Wisley</li> <li> <p>Co Author: Agile Business Rule Development Springer.</p> <p></p> </li> <li> <p>Event-driven solutions virtual book, I think a more flexible way to develop a book, and more agile to change content.</p> <p></p> </li> <li> <p>AWS Studies for Solution Architect Professional Certification</p> </li> <li>Machine Learning studies, same approach, it may become a book.</li> </ul>"},{"location":"about/#assets","title":"Assets","text":"<ul> <li>Messaging in AWS, Active MQ</li> <li>AWS Serverless - Lambda studies </li> </ul>"},{"location":"about/#blogs","title":"Blogs","text":"<ul> <li>Medium - Unveiling the Future with Insight Storming: Enhancing Event Driven Architecture (EDA) Solutions</li> <li>Medium - Event-driven solution is still a hot topic</li> <li>Medium - Updated EDA reference architecture - 08/2021</li> <li>Medium - Developer\u2019s experience with an event-driven solution implementation - 03/2022</li> <li> <p>Linkedin- Why Event-Driven Architecture is important in 2020s Multiple articles on IBM developers site, and BPM journal.</p> </li> <li> <p>Strategies for managing reference data in a business rules application using IBM Operational Decision Manager.</p> </li> <li> <p>Best practices for designing and implementing decision services, Part 1: An SOA approach to creating reusable decision services.</p> </li> <li>Best practices for designing and implementing decision services, Part 2: Integrating IBM Business Process Manager and IBM Operational Decision Management.</li> <li>Using IBM Operational Decision Manager DVS simulation features for risk scoring analysis use cases.</li> <li>Best practices for designing and implementing decision services, Part 3: Integrating IBM Business Process Manager and IBM Operational Decision Manager RESTful protocol.</li> <li>Policies and Rules \u2013 Improving business agility: Part 1: Support for business agility</li> <li>Policies and Rules \u2013 Improving Business Agility: Part 2: Demonstrating the value that can be derived from using Policy and Rules.</li> </ul>"},{"location":"about/#what-is-in-this-site","title":"What is in this site","text":"<p>The goal of this site is to keep references and notes from my different studies and may be define some best practices I have used to implement customer's solutions. It addresses some of the common problems I encountered while trying new technologies.</p> <p>All views expressed on this site are mine only, and do not represent the views of my current and past employers and customers.</p>"},{"location":"about/#study-repositories","title":"Study Repositories","text":"<p>Repository list</p>"},{"location":"assets/","title":"Assets developed","text":"<p>This is the list of assets, I developed or co-developed over the years.</p>"},{"location":"assets/#methodology","title":"Methodology","text":"<ul> <li>Event storming workshop</li> <li>Domain Driven Design </li> <li>Develop data intensive app</li> <li>Agile business rules development</li> </ul>"},{"location":"assets/#eda","title":"EDA","text":""},{"location":"assets/#architecture","title":"Architecture","text":"<ul> <li>Global EDA site - started in 10/2018 - continuously updated</li> <li>Reefer Container shipment EDA solution implementation - started 10/2018</li> <li>Reference Architecture and integration with analytics and machine learning</li> <li>Advantages of Event-Driven Reference Architectures - Microservice decoupling</li> <li>Advantages of Event-Driven Reference Architectures - Reactive systems </li> <li>Advantages of Event-Driven Reference Architectures- Resiliency </li> <li>Strangler design pattern in the context of EDA</li> <li>CQRS pattern discussion - documented 01/2019 - updated 04/2020</li> <li>Event sourcing design pattern 01/2019</li> <li>Dead letter queue design pattern -1/2020</li> <li>SAGA design pattern- 01/2019, updated Q1 - 2020</li> <li>SAGA orchestration with MQ - producer app</li> <li>SAGA orchestration with MQ - mq consumer app (voyage)</li> <li>SAGA orchestration with MQ - knative mq consumer app (reefer)</li> <li>Modern data lake: point of view - Q2 2020</li> <li>Legacy integration - Q1 2020 / Updated Q3 2020</li> <li>Business Automation products on top of EDA</li> <li>integration with a potential MQ transactional framework with kafka / life insurance demo</li> <li>EDA governance with Apache Atlas</li> </ul>"},{"location":"assets/#development-practices","title":"Development practices","text":"<ul> <li>Different Data Models</li> <li>CQRS pattern implementation query and command are part of two separate folders. - started in dec 2018 - updated Q1 2020 with new integration tests.</li> <li>Topic replication with mirror maker 2 Q2 2020 - updated 09/2020</li> <li>Mirror Maker 2  Studies  04/2020</li> <li>Mirror maker 2 labs - Q2 2020 - updated 09/2020</li> <li>Event Streams on Cloud hands on lab introductory hands-on lab on IBM Event Streams on Cloud with topic creation. It was done in the context of customer's Kafka bootcamp. </li> <li>Store item sale simulator to produce messages to Rabbit MQ, IBM MQ or Kafka backends 10/2020</li> <li>Realtime Analytics pattern. Created for EDA</li> <li>Reefer simulator in python - updated 02/2021</li> <li>Reefer Container Shipment Container Management springboot</li> <li>Container inventory management - legacy mockup</li> <li>Gitops with kustomize for Vaccine solution - 03/2021</li> <li>Spring cloud stream project template with apicurio. 05/2021</li> <li>EDA quickstart code templates</li> <li>IBM Tech academy</li> </ul>"},{"location":"assets/#technology","title":"Technology","text":"<ul> <li>Kafka Summary</li> <li>Kafka Producers &amp; Consumers best practices</li> <li>Kafka Streams Summary</li> <li>Kafka Streams lab 3 - 09/2020 - Q4 2020</li> <li>Inventory view with Kafka Streams, interactive queries, and quarkus</li> <li>Kafka Connect</li> <li>Kafka connect with Cloud Object storage</li> <li>IBM MQ in the context of EDA started 09/2020</li> <li>Event Streams on Cloud - security with IAM - labs Q1 2020</li> <li>Event Streams on Cloud - Consumer group lab Q1 -2020</li> <li>Event Streams on Cloud - monitoring  Q1 -2020</li> <li>Flink studies</li> </ul>"},{"location":"assets/#integration","title":"Integration","text":"<ul> <li>Hybrid Integration Reference Architecture q3 2019</li> <li>Inventory API with API management</li> <li>Inventory Data Access Layer</li> <li>DB2 Inventory Database 04/2017</li> <li>MQ Messaging Solution</li> <li>Customer management microservice</li> <li>Inventory Flow - Integration Bus The goal of this project is to demonstrate how an IBM Integration Bus runtime can be deployed on premise or on IBM Cloud Private, running gateway flows to expose REST api from SOAP back end services.</li> </ul>"},{"location":"assets/#ai-data","title":"AI - Data","text":"<ul> <li>Cognitive Reference Architecture</li> <li>Data and AI Reference Architecture</li> <li>Reefer Predictive Maintenance Solution</li> <li>Vaccine Cold Chain Monitoring - solution - 07/2020</li> <li>Vaccine Order Manager event-driven microservice - 07/ 2020</li> <li>Vaccine Order &amp; Reefer Optimization python flask app to integrate with cplex to build a model</li> <li>Watson Discovery enablement</li> <li>Watson discovery broker app</li> <li>Watson conversation training</li> <li>Watson conversation broker</li> <li>Context Driven Dialog</li> <li>Product Recommendations with Watson Assistant and Decision Management - 12/2018</li> <li>Cognitive Architecture: Supplier On Boarding Business Process</li> <li>Customer analysis with cognitive and analytics in hybrid cloud</li> </ul>"},{"location":"assets/#business-process-automation","title":"Business process automation","text":"<ul> <li>Digital Business Automation site</li> </ul>"},{"location":"code-template/","title":"Some starting code template","text":"<ul> <li> <p>Microprofile 3.x based event driven microservice using kafka streams API - Status: under construction - need readme on how to use it. 04/30/2020</p> </li> <li> <p>quarkus-event-driven-producer-microservice-template - Status: to do all. 04/30/2020</p> </li> <li>Event Driven Kafka Consumer with Quarkus - Status: to improve kafka config. 04/30/2020</li> </ul>"},{"location":"skills/","title":"Skills","text":"<p>As a seasoned senior consultant, I have had the privilege of immersing myself in a wide range of technologies throughout my career. My insatiable passion for learning continually drives me to explore new concepts and expand my knowledge base. In this section, I would like to provide you with a high-level overview of the breadth and depth of my expertise in certain subjects.</p> <p>My Git repositories showcase my commitment to delving deeper into specific solutions, as I consistently develop and refine assets that contribute to these areas.</p> <ul> <li>3/2024 Skill Map</li> </ul> <p>The following figure illustrates the two main pillars for my knowledge base: software architecture and AI, with sub-pillars, I am continuously working on over time:</p> <p></p> <p>I will try to assess the skill level using the following definitions:</p> Level Description 100 Foundational I can present the feature overview, address value proposition, pro-cons, fit for purpose 200 Intermediate Being able to articulate best practices, demonstrate the product feature, code using the product 300 Advanced Bbe able to deep dive into a topic, a feature, an integration, and support deeper architecture discussions 400 Expert Product dev experience. Good knowledge on how a product works. Understand parameters. Code around the product deployment. Support long term consulting engagement."},{"location":"skills/#architecture","title":"Architecture","text":""},{"location":"skills/#eda","title":"EDA","text":"<ul> <li>My book on Event Driven Architecture and solutions.</li> <li> <p>Event processing: Kafka Stream, Flink study, AWS Managed Service for Flink; Spark Streaming.</p> </li> <li> <p>Messaging studies: ActiveMQ open source, Amazon MQ, IBM MQ, Kafka, Amazon SQS</p> </li> <li>Distributed systems, Reactive Systems</li> <li>Vaccine delivery solution</li> <li>Real time inventory solution</li> <li>A demonstration for an event-driven solution of autonomous car rides.</li> </ul>"},{"location":"skills/#cloud","title":"Cloud","text":"<ul> <li>Cloud architecture: AWS body of knowledge (useful to keep level 100 to 200 knowledge on AWS service) Certified AWS SA Associate (May 23), and AWS SA Professional (Dec 23)</li> <li>Serverless: AWS Lambda with its complementary prototypes git repo, Step function, API Gateway.  Joined AWS Serverless technical competency group in September 2023.</li> <li>Container (docker) and Kubernetes / OpenShift</li> <li>Infrastructure as code: CDK, SAM</li> <li> <p>DevOps - GitOps - ArgoCD</p> </li> <li> <p>Java, JEE, Java Microprofile, Quarkus, Spring boot</p> </li> </ul>"},{"location":"skills/#data","title":"Data","text":"<ul> <li>Main content is in this repository.</li> </ul>"},{"location":"skills/#analytics","title":"Analytics","text":"<ul> <li>Spark study, Amazon EMR</li> <li>Flink study, AWS Managed Service for Flink</li> </ul>"},{"location":"skills/#ai","title":"AI","text":""},{"location":"skills/#machine-learning","title":"Machine Learning","text":""},{"location":"skills/#deep-learning","title":"Deep Learning","text":"<ul> <li>Amazon SageMaker</li> <li>PyTorch</li> <li>Generative AI, Langchain</li> <li>Amazon Bedrock</li> </ul>"},{"location":"skills/#development","title":"Development","text":"<ul> <li>Java, Python, nodejs, bash</li> <li>Quarkus</li> <li>Web Development Vuejs, Angular JS</li> <li>Business rules based system</li> <li>SQL Database with JPA and JTA: Postgresql, DB2</li> <li>Document oriented DB: DynamoDB, Mongodb, Cassandra</li> <li>Kafka Streams API, Kafka API, reactive messaging</li> </ul>"},{"location":"skills/#methodology","title":"Methodology","text":"<ul> <li>Agile development, Lean Startup, Design Thinking, Event Storming and Domain Driven Design</li> <li>Agile business rules development (Creator)</li> <li>DevOps, Gitops</li> </ul>"},{"location":"skills/#list-of-contributions","title":"List of contributions","text":"<p>See the assets list</p>"},{"location":"architecture/DR/","title":"Disaster recovery assessment","text":"<p>Disaster recovery consists of well-defined strategies to back up the primary data center and restore its data to a secondary data center. During normal operations, the live system is used. The backup programs run in the background to save environmental information and application data. When the live system goes down, the backup system is restored from the backed up data. End users can use the system again.</p>"},{"location":"architecture/DR/#concepts","title":"Concepts","text":"<p>We are talking about a major disaster like site down, or deep data center failure. Two concepts are then important to consider:</p> <p></p>"},{"location":"architecture/DR/#rto-recovery-time-objective","title":"RTO - Recovery Time Objective","text":"<p>RTO is related to downtime and measure the time required to restore the environment after the primary site becomes unavailable. This is the maximum time the business can be unavailable. RTO refers, to the amount of time the system's data is unavailable or inaccessible preventing normal service.</p>"},{"location":"architecture/DR/#rpo-recovery-point-objective","title":"RPO - Recovery Point Objective","text":"<p>RPO refers to the amount of data at risk. It's determined by the amount of time between data protection events and reflects the amount of data that potentially could be lost during a disaster recovery. The metric is an indication of the amount of data at risk of being lost. Usually measured in the time elapsed, between the state of the runtime data at the source and at the replica.</p> <p>Synchronous replication could deliver a replica with zero lag and, therefore, a recovery point could match the source exactly. </p> <p>Because asynchronous replication does not guarantee that all updates to the source are simultaneously applied at the replica, the recovery point metric is important. The amount of lag will depend upon the details of the replication management software, but generally a smaller recovery point measure (less lag) is more expensive.</p>"},{"location":"architecture/DR/#different-implementations-at-different-costs","title":"Different implementations at different costs","text":"<p>Assessing the business dimensions of DR is a mandatory activity to select the best strategy for the cost to operate. </p> <p>The first curve illustrates the cost of the business impact depending of the length of the service interruption: The RTO vertical line is when the time of the service interruption is acceptable, on the right side of it, the business impact is too high.</p> <p></p> <p>A less expensive solution that reduces the cost of recovery, has also an higher length of interruption. \"Acceptable recovery cost\" is the amount of $ the business is accepting to pay for a solution. From those curves we can deduct that backup and restore and warm standby will be acceptable solution. This assessment should be done for all application, as not all of them needs to be active-active. Enterprises use a 3-level classification mechanism: low = not strategic applications, high=  highly strategic to the company revenue and branding (e-commerce, fullfilment, inventory, payment), medium = the rest.</p> <p>When the RPO line is going to the right and the Acceptable recovery cost line going too low, there may be no acceptable solution to support the DR strategy.</p>"},{"location":"architecture/DR/#disaster-recovery-patterns","title":"Disaster Recovery patterns","text":"<p>Below are some common types of disaster recovery solutions:</p> <ul> <li>Database-based disaster recovery solutions: Database systems such as IBM DB2\u00ae have specific features that can be used to replicate the primary database data to a standby database. These solutions are commonly adopted by applications that interact with only a single database.</li> <li>Host-based disaster recovery solutions: Modern operating systems such as AIX\u00ae and Linux\u00ae have built-in features that can provide point-in-time snapshots. These features are widely used by applications that directly use files to store data and don't require a short recovery point objective (RPO) and recovery time objective (RTO).</li> <li>Fabric-based disaster recovery solutions: These methods are focused on transferring data within the storage network from a source fabric to a destination fabric using special hardware like Storage Area Network products which provides both Global and Metro mirror capabilities to implement data protection. These solutions are attractives because they can accommodate different storage subsystems. </li> <li>Storage subsystem-based disaster recovery solutions: These are also called controller-based solutions because they use a storage controller to transfer data from a source storage subsystem to a destination storage subsystem. Usually controller-based solutions are homogeneous, with data copied between disk arrays from the same manufacturer and often within the same family of products. A dedicated transport channel is commonly required to link the two sites. For example, Global or Metro Mirror can be configured between two IBM System Storage DS series systems to implement disaster recovery.</li> </ul>"},{"location":"architecture/DR/#what-should-be-considered-to-prepare-for-disaster-recovery","title":"What should be considered to prepare for disaster recovery?","text":"<ul> <li>Not all applications need to be active - active.</li> <li>Active/active really means that a user will loose connection to a site and be routed to another site in milliseconds. Session data as cookies may be recovered.</li> <li>Distributed reads is also a requirement for active - active </li> <li>The biggest challenge is to avoid data collision in the same data store: entity created in two regions in less than 100ms can collide when replicated. </li> </ul> <p>There are three types of data to consider:</p> <ul> <li>Installation data is the data associated with the installation of a given product, Operating System, and database.</li> <li>Configuration data is the data associated with profile configuration, applications, resource configuration.</li> <li>Runtime data is the data associated with transaction logs, messages saved in the database table,  process instance information persisted in the database table, and other persistent business states.</li> </ul> <p>For installation and configuration data, the change is infrequent, and consistency is important therefore data copy via snapshots before and after changes done is a viable solution.</p> <p>With the adoption of environment as scripts and DevOps, configurations are no more manual and can be kept as source code in Git repository.</p>"},{"location":"architecture/DR/#runtime-data","title":"Runtime Data","text":"<p>Application requirements determine replication strategy:</p> <ul> <li>Aggressive RPO &amp; RTO objectives: Favor asynchronous replication via Storage System tooling where replication may be done while servers are online and active.</li> <li>Lenient RPO &amp; RTO objectives: Favor synchronous replication via Operating System tooling (file copy) where replication must be done off line, while the different runtime solutions are quiesced (a maintenance window)</li> <li>DB-managed replication: This approach leverages transaction logs to be stored in a DB. Remark that this approach is applicable only if the single database stores all application resources that need to be replicated</li> </ul> Read more <ul> <li>Data replication</li> </ul>"},{"location":"architecture/DR/#product-specifics","title":"Product specifics","text":"<ul> <li>In most Java app server transaction engine, the transaction logs are located in the database itself, so will be replicated with database HA replicas.</li> <li>In active-active settings, Kubernetes ETCD does not need to be replicated, content is local to the clusters.</li> </ul>"},{"location":"architecture/architectural-decisions/","title":"Architectural Decisions Guidelines","text":"<p>The following sections provide guidelines on selecting Technology Components for each Application Component.  So it is explained what Technology Component should be chosen based on given requirements and if the Application Component needs to be included at all.</p>"},{"location":"architecture/architectural-decisions/#application-component-data-source","title":"Application Component: Data Source","text":"<p>The data source is a private or public data source that includes relational databases; web pages; CSV, XML, JSON, or text files; and video and audio data.</p>"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines","title":"Technology Component mapping guidelines","text":"<p>With the data source, there is not much to decide because in most cases, the type and structure of a data source is already defined and controlled by other stakeholders. However, if there is some control over the process, the following Architectural Principles should be considered:</p> <ul> <li> <p>How does the delivery point look like?</p> <p>Enterprise data mostly lies in relational databases serving OLTP systems. It\u2019s typically a bad practice to access those systems directly, even in read-only mode because ETL processes are running SQL queries against those systems, which can hinder performance. One exception for example is IBM DB2 Workload Manager because it allows OLAP and ETL workloads to run in parallel with an OLTP workload without performance degradation of OLTP queries using intelligent scheduling and prioritizing mechanisms.</p> </li> <li> <p>Does real-time data need to be considered?</p> <p>Real-time data comes in various shapes and delivery methods. The most prominent include MQTT telemetry and sensor data (for example, data from the IBM Watson IoT Platform), an event stream, a simple REST HTTP endpoint that needs to be polled, or a TCP or UDP socket. If no downstream real-time processing is required, that data can be staged (for example, using Cloud Object Store). If downstream real-time processing is necessary, read the section on Streaming analytics further down in this document.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#application-component-enterprise-data","title":"Application Component: Enterprise data","text":"<p>Cloud-based solutions tend to extend access of the enterprise data model. Therefore, it might be necessary to continuously transfer subsets of enterprise data to the cloud environment or access those in real time through a VPN API gateway.</p>"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_1","title":"Technology Component mapping guidelines","text":"<p>Moving enterprise data to the cloud can be costly. Therefore, it should be considered only if necessary. For example, if user data is handled in the cloud is it sufficient to store an anonymized primary key. If transfer of enterprise data to the cloud is unavoidable, privacy concerns and regulations must be addressed. Then, there are multiple ways to access it:</p> <ul> <li>Batch sync from an enterprise data center to the cloud</li> <li>Real-time access to subsets of data using VPN and an API gateway</li> <li>Expose data via an event backbone</li> </ul>"},{"location":"architecture/architectural-decisions/#technology-mapping","title":"Technology Mapping","text":""},{"location":"architecture/architectural-decisions/#secure-gateway","title":"Secure gateway","text":"<p>Secure gateway lets cloud applications access specified hosts and ports in a private data center though an outbound connection. Therefore, no external inbound access is required. You can go to the Secure Gatewayservice on IBM Cloud for more information.</p>"},{"location":"architecture/architectural-decisions/#lift","title":"Lift","text":"<p>Lift allows you to migrate on-premises data to cloud databases in a very efficient manner. Read more about the IBM Lift CLI.</p>"},{"location":"architecture/architectural-decisions/#rocket-mainframe-data","title":"Rocket Mainframe Data","text":"<p>The Rocket Mainframe Data service uses similar functions for batch-style data integration as Lift, but is dedicated to IBM Mainframes. You can read more information about the service.</p>"},{"location":"architecture/architectural-decisions/#apache-kafka","title":"Apache Kafka","text":"<p>Apache Kafka, and IBM Event Streams expose data for other to consume with strong decoupling, and long term persistence. It can scale well for big data needs, and represent a well adopted platform to integrate microservices. See our deep dive into event driven solution here.</p>"},{"location":"architecture/architectural-decisions/#application-component-streaming-analytics","title":"Application Component: Streaming analytics","text":"<p>The current state-of-the-art is batch processing. But, sometimes the value of a data product can be increased tremendously by adding real-time analytics capabilities because most of world\u2019s data loses value within seconds. Think of stock market data or the fact that a vehicle camera captures a pedestrian crossing a street. A streaming analytics system allows for real-time data processing. Think of it like running data against a continuous query instead of running a query against a finite data set.</p>"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_2","title":"Technology Component mapping guidelines","text":"<p>There is a relatively limited set of technologies for real-time stream processing. The most important questions to be asked are:</p> <ul> <li>What throughput is required?</li> <li>What latency is accepted?</li> <li>Which data types must be supported?</li> <li>What type of algorithms run on the system? Only relational algebra or advanced modeling?</li> <li>What\u2019s the variance of the workload and what are the elasticity requirements?</li> <li>What type of fault tolerance and delivery guarantees are necessary?</li> </ul>"},{"location":"architecture/architectural-decisions/#technology-mapping_1","title":"Technology Mapping","text":"<p>On IBM Cloud, there are many service offerings for real-time data processing that I explain in the following sections along with guidelines for when to use them.</p>"},{"location":"architecture/architectural-decisions/#apache-spark-and-apache-spark-structured-streaming","title":"Apache Spark and Apache Spark Structured Streaming","text":"<p>Apache Spark is often the primary choice when it comes to cluster-grade data processing and machine learning. If you\u2019re already using it for batch processing, Apache Spark Structured Streaming should be the first thing to evaluate. This way, you can have technology homogeneity and batch and streaming jobs can be run together (for example, joining a stream of records against a reference table).</p> <ul> <li> <p>What throughput is required? Apache Spark Structured Streaming supports the same throughput as in batch mode.</p> </li> <li> <p>What latency is accepted? In Apache Spark v2.3, the Continuous Processing mode has been introduced, bringing latency down to one millisecond.</p> </li> <li> <p>Which data types must be supported? Apache Spark is strong at structured and semi-structured data. Audio and video data can\u2019t benefit from Apache Spark\u2019s accelerators Tungsten and Catalyst.</p> </li> <li> <p>What type of algorithms run on the system? Only relational algebra or advanced modeling? Apache Spark Structured Streaming supports relational queries as well as machine learning, but machine learning is only supported on sliding and tumbling windows.</p> </li> <li> <p>What\u2019s the variance of the workload and what are the elasticity requirements? Through their fault tolerant nature, Apache Spark clusters can be grown and shrunk dynamically.</p> </li> <li> <p>What type of fault tolerance and delivery guarantees are necessary? Apache Spark Structured Streaming supports exactly once delivery guarantees and depending on the type of data source, complete crash fault tolerance.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#node-red","title":"Node-RED","text":"<p>Node-RED is a lightweight streaming engine. Implemented on top of Node.js in JavaScript, it can even run on a 64 MB memory footprint (for example, running on a Raspberry PI).</p> <ul> <li> <p>What throughput is required? Node-RED\u2019s throughput is bound to processing capabilities of a single CPU core, through Node.js\u2019s event processing nature. For increased throughput, multiple instances of Node-RED have been used in parallel. Parallelization is not built in and needs to be provided by the application developer.</p> </li> <li> <p>What latency is accepted? Latency is also dependent on the CPU configuration and on the throughput because high throughput congests the event queue and increases latency.</p> </li> <li> <p>Which data types must be supported? Node-RED best supports JSON streams, although any data type can be nested into JSON.</p> </li> <li> <p>What type of algorithms run on the system? Only relational algebra or advanced modeling? Node-RED has one of the most extensive ecosystems of open source third-party modules. Although advanced machine learning is not supported natively.</p> </li> <li> <p>What\u2019s the variance of the workload and what are the elasticity requirements? Because parallelization is a responsibility of the application developer, for independent computation a round-robin load balancing scheme supports linear scalability and full elasticity.</p> </li> <li> <p>What type of fault tolerance and delivery guarantees are necessary? NodeRED has no built-in fault tolerance and no delivery guarantees.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#apache-nifi","title":"Apache Nifi","text":"<p>Apache Nifi is maintained by Hortonworks and is part of the IBM Analytics Engine Service.</p> <ul> <li> <p>What throughput is required? Nifi can handle hundreds of MBs on a single node and can be configured to handle multiple GBs in cluster mode.</p> </li> <li> <p>What latency is accepted? Nifi\u2019s latency is in seconds. Through message periodization, the tradeoff between throughput and latency can be tweaked.</p> </li> <li> <p>Which data types must be supported? Nifi best supports structured data streams, although any data type can be nested.</p> </li> <li> <p>What type of algorithms run on the system? Only relational algebra or advanced modeling? Nifi supports relational algebra out of the box, but custom processors can be built.</p> </li> <li> <p>What\u2019s the variance of the workload and what are the elasticity requirements? Nifi can be easily scaled up without restarts, but scaling down requires stopping and starting the Nifi system.</p> </li> <li> <p>What type of fault tolerance and delivery guarantees are necessary? Nifi supports end-to-end guaranteed exactly once delivery.</p> </li> </ul> <p>Also, fault tolerance can be configured, but automatic recovery is not possible. Another important feature is backpressure and pressure release, which causes the upstream nodes to stop accepting new data and discarding unprocessed data if an age threshold is exceeded.</p>"},{"location":"architecture/architectural-decisions/#application-component-data-integration","title":"Application Component: Data Integration","text":"<p>In the data integration stage, data is cleansed, transformed, and if possible, downstream features are added</p>"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_3","title":"Technology Component mapping guidelines","text":"<p>There are numerous technologies for batch data processing, which is the technology used for data integration. The most important questions to be asked are:</p> <ul> <li>What throughput is required?</li> <li>Which data types must be supported?</li> <li>What source systems must be supported?</li> <li>What skills are required?</li> </ul>"},{"location":"architecture/architectural-decisions/#technology-guidelines","title":"Technology Guidelines","text":"<p>IBM Cloud has many service offerings for data integration, and the following section explains them and gives guidelines on which one to use.</p>"},{"location":"architecture/architectural-decisions/#apache-spark","title":"Apache Spark","text":"<p>Apache Spark is often the first choice when it comes to cluster-grade data processing and machine learning. Apache Spark is a flexible option that also supports writing integration processes in SQL. But it\u2019s missing a user interface.</p> <ul> <li> <p>What throughput is required? Apache Spark scales linearly, so throughput is just a function of the cluster size.</p> </li> <li> <p>Which data types must be supported? Apache Spark works best with structured data, but binary data is supported as well.</p> </li> <li> <p>What source systems must be supported? Apache Spark can access various SQL and NoSQL data as well as file sources. A common data source architecture allows adding capabilities, and it has third-party project functions as well.</p> </li> <li> <p>What skills are required? Advanced SQL skills are required and you should have some familiarity with either Java programming, Scala, or Python.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#ibm-data-stage-on-cloud","title":"IBM Data Stage on Cloud","text":"<p>IBM Data Stage is a sophisticated ETL (Extract Transform Load) tool. Its closed source and supports visual editing.</p> <ul> <li> <p>What throughput is required? Data Stage can be used in cluster mode, which supports scale-out.</p> </li> <li> <p>Which data types must be supported? Data Stage has its roots in traditional Data Warehouse ETL and concentrates on structured data.</p> </li> <li> <p>What source systems must be supported? Again, Data Stage concentrates on relational database systems, but files can also be read, even on Object Store. In addition, data sources can be added using plug-ins that are implemented in the Java language.</p> </li> <li> <p>What skills are required? Because Data Stage is a visual editing environment, the learning curve is low. No programming skills are required.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#others","title":"Others","text":"<p>It\u2019s important to know that data integration is mostly done using ETL tools, plain SQL, or a combination of both. ETL tools are mature technology, and many ETL tools exist. On the other hand, if streaming analytics is part of the project, it\u2019s a good idea to check whether one of those technologies fits your requirements because reuse of such a system reduces technology heterogeneity.</p>"},{"location":"architecture/architectural-decisions/#application-component-data-repository","title":"Application Component: Data Repository","text":"<p>This is the persistent storage for your data.</p>"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_4","title":"Technology Component mapping guidelines","text":"<p>There are lots of technologies for persisting data, and most of them are relational databases. The second largest group are NoSQL databases, with file systems (including Cloud Object Store) forming the last one. The most important questions to be asked are:</p> <ul> <li>What is the impact of storage cost?</li> <li>Which data types must be supported?</li> <li>How well must point queries (on fixed or dynamic dimensions) be supported?</li> <li>How well must range queries (on fixed or dynamic dimensions) be supported?</li> <li>How well must full table scans be supported?</li> <li>What skills are required?</li> <li>What\u2019s the requirement for fault tolerance and backup?</li> <li>What are the constant and peak ingestion rates?</li> <li>What amount of storage is needed?</li> <li>What does the growth pattern look like?</li> <li>What are the retention policies?</li> </ul>"},{"location":"architecture/architectural-decisions/#technology-mapping_2","title":"Technology Mapping","text":"<p>IBM cloud has numerous service offerings for SQL, NoSQL, and file storage. The following section explains them and provides guidelines on when to use which one.</p>"},{"location":"architecture/architectural-decisions/#relational-databases","title":"Relational databases","text":"<p>Dash DB is the Db2 BLU on the Cloud offering from IBM that features column store, advanced compression, and execution on SIMD instructions sets (that is, vectorized processing). But there are other options in IBM Cloud such as Informix, PostgreSQL, and MySQL.</p> <ul> <li> <p>What is the impact of storage cost? Relational databases (RDBMS) have the highest requirements on storage quality. Therefore, the cost of relational storage is always the highest.</p> </li> <li> <p>Which data types must be supported? Relational databases are meant for structured data. Although there are column data types for binary data that can be swapped for cheaper storage, this is just an add-on and not a core function of relational databases.</p> </li> <li> <p>How well must point queries (on fixed or dynamic dimensions) be supported? RDBMS are great for point queries because an index can be created on each column.</p> </li> <li> <p>How well must range queries (on fixed or dynamic dimensions) be supported? RDBMS are great for range queries because an index can be created on each column.</p> </li> <li> <p>How well must full table scans be supported? RDBMS are trying to avoid full table scans in their SQL query optimizers. Therefore, performance is not optimized for full table scans (for example, contaminating page caches).</p> </li> <li> <p>What skills are required? You should have SQL skills, and if a cloud offering isn\u2019t chosen, you should also have database administrator (DBA) skills for the specific database.</p> </li> <li> <p>What\u2019s the requirement for fault tolerance and backup? RDMBS support continuous backup and crash fault tolerance. For recovery, the system might need to go offline.</p> </li> <li> <p>What are the constant and peak ingestion rates? Inserts using SQL are relatively slow, especially if the target table contains many indexes that must be rebalanced and updated. Some RDBMS support bulk inserts from files by bypassing the SQL engine, but then the table usually needs to go offline for that period.</p> </li> <li> <p>What amount of storage is needed? RDMBS perform very well to around 1 TB of data. Going beyond that is complex and needs advanced cluster setups.</p> </li> <li> <p>What does the growth pattern look like? RDBMS support volume management, so continuous growth usually isn\u2019t a problem, even at run time. For shrinking, the system might need to be taken offline.</p> </li> <li> <p>What are the retention policies? RDBMS usually support automated retention mechanisms to delete old data automatically.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#nosql-databases","title":"NoSQL databases","text":"<p>The most prominent NoSQL databases like Apache CouchDB, MongoDB, Redis, RethinkDB, ScyllaDB (Cassandra), and InfluxCloud are supported.</p> <ul> <li> <p>What is the impact of storage cost? NoSQL databases are usually storage fault-tolerant, by default. Therefore, quality requirements on storage are less, which brings down storage cost.</p> </li> <li> <p>Which data types must be supported? Although NoSQL databases are meant for structured data as well, they usually use JSON as a storage format, which can be enriched with binary data. Although, a lot of binary data attached to a JSON document can bring the performance down as well.</p> </li> <li> <p>How well must point queries (on fixed or dynamic dimensions) be supported? Some NoSQL databases support the creation of indexes, which improves point query performance.</p> </li> <li> <p>How well must range queries (on fixed or dynamic dimensions) be supported? Some NoSQL databases support the creation of indexes, which improves range query performance.</p> </li> <li> <p>How well must full table scans be supported? NoSQL databases perform very well at full table scans. The performance is only limited by the I/O bandwidth to storage.</p> </li> <li> <p>What skills are required? Typically, special query language skills are required for the application developer and if a cloud offering isn\u2019t chosen, database administrator (DBA) skills are needed for the specific database.</p> </li> <li> <p>What\u2019s the requirement for fault tolerance and backup? NoSQL databases support backups in different ways. But some aren\u2019t supporting online backup. NoSQL databases are usually crash fault tolerant, but for recovery, the system might need to go offline.</p> </li> <li> <p>What are the constant and peak ingestion rates? Usually, no indexes need to be updated and data doesn\u2019t need to be mapped to pages. Ingestion rates are usually only bound to I/O performance of the storage system.</p> </li> <li> <p>What amount of storage is needed? RDMBS perform well to approximately 10 \u2013 100 TB of data. Cluster setups on NoSQL databases are much more straightforward than on RDBMS. Successful setups with &gt;100 nodes and &gt; 100.000 database reads and writes per second have been reported.</p> </li> <li> <p>What does the growth pattern look like? The growth of NoSQL databases is not a problem. Volumes can be added at run time. For shrinking, the system might need to be taken offline.</p> </li> <li> <p>What are the retention policies? NoSQL databases don\u2019t support automated retention mechanisms to delete old data automatically. Therefore, this must be implemented manually, resulting in range queries on the data corpus.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#object-storage","title":"Object storage","text":"<p>Cloud object storage makes it possible to store practically limitless amounts of data. It is commonly used for data archiving and backup, for web and mobile applications, and as scalable, persistent storage for analytics. So, let\u2019s take a look.</p> <ul> <li> <p>What is the impact of storage cost? Object storage is the cheapest option for storage.</p> </li> <li> <p>Which data types must be supported? Because object storage resembles a file system, any data type is supported.</p> </li> <li> <p>How well must point queries (on fixed or dynamic dimensions) be supported? Because object storage resembles a file system, external indices must be created. However, it\u2019s possible to access specific storage locations through folder and file names and file offsets.</p> </li> <li> <p>How well must range queries (on fixed or dynamic dimensions) be supported? Because object storage resembles a file system, external indices must be created. However, it\u2019s possible to access specific storage locations through folder and file names and file offsets. Therefore, range queries on a single defined column (for example, data) can be achieved through hierarchical folder structures.</p> </li> <li> <p>How well must full table scans be supported? Full table scans are bound only by the I/O bandwidth of the object storage.</p> </li> <li> <p>What skills are required? On a file level, working with object storage is much like working with any file system. Through Apache SparkSQL and IBM Cloud SQL Query, data in Object storage can be accessed with SQL. Because object storage is a cloud offering, no administrator skills are required. IBM Object Storage is available for on-premises as well using an appliance box.</p> </li> <li> <p>What\u2019s the requirement for fault tolerance and backup? Fault tolerance and backup is completely handled by the cloud provider. Object storage supports intercontinental data center replication for high-availability out of the box.</p> </li> <li> <p>What are the constant and peak ingestion rates? Ingestion rates to object storage is bound by the uplink speed to the object storage system.</p> </li> <li> <p>What\u2019s the amount of storage needed? Object storage scales to the petabyte range.</p> </li> <li> <p>What does the growth pattern look like? Growth and shrinking on object storage is fully elastic.</p> </li> <li> <p>What are the retention policies? Retention of data residing in object storage must be done manually. Hierarchical file and folder layout that is based on data and time helps here. Some object storage support automatic movement of infrequently accessed files to colder storage (colder means less cost, but also less performance, or even higher cost of accesses to files).</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#application-component-discovery-and-exploration","title":"Application Component: Discovery and exploration","text":"<p>This component allows for visualization and creation of metrics of data.</p>"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_5","title":"Technology Component mapping guidelines","text":"<p>In various process models, data visualization and exploration is one of the first steps. Similar tasks are also applied in traditional data warehousing and business intelligence. So when choosing a technology, ask the following questions:</p> <ul> <li>What type of visualizations are needed?</li> <li>Are interactive visualizations needed?</li> <li>Are coding skills available or required?</li> <li>What metrics can be calculated on the data?</li> <li>Do metrics and visualization need to be shared with business stakeholders?</li> </ul>"},{"location":"architecture/architectural-decisions/#technology-mapping_3","title":"Technology Mapping","text":"<p>IBM cloud has many service offerings for data exploration. Some of the offerings are open source, and some aren\u2019t.</p>"},{"location":"architecture/architectural-decisions/#jupyter-python-pyspark-scikit-learn-pandas-matplotlib-pixiedust","title":"Jupyter, Python, pyspark, scikit-learn, pandas, Matplotlib, PixieDust","text":"<p>Jupyter, Python, pyspark, scikit-learn, pandas, Matplotlib, PixieDust are all open source and supported in IBM Cloud. Some of these components have overlapping features and some of them have complementary features. This can be determined by answering the architectural questions.</p> <ul> <li> <p>What type of visualizations are needed? Matplotlib supports the widest range of possible visualizations including run chars, histograms, box-plots, and scatter plots. PixieDust (as of V1.1.11) supports tables, bar charts, line charts, scatter plots, pie charts, histograms, and maps.</p> </li> <li> <p>Are interactive visualizations needed? Matplotlib creates static plots and PixieDust supports interactive ones.</p> </li> <li> <p>Are coding skills available or required? Matplotlib requires coding skills, but PixieDust does not. For computing metrics, some code is necessary.</p> </li> <li> <p>What metrics can be calculated on the data? Using scikit-learn and pandas, all state-of-the-art metrics are supported.</p> </li> <li> <p>Do metrics and visualization need to be shared with business stakeholders? Watson Studio supports sharing of Jupyter Notebooks, also using a fine-grained user and access management system.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#spss-modeler","title":"SPSS Modeler","text":"<p>SPSS Modeler is available in the cloud and also as stand-alone product.</p> <ul> <li> <p>What type of visualizations are needed? SPSS Modeler supports the following visualizations out of the box:</p> <ul> <li>Bar</li> <li>Pie</li> <li>3D Bar</li> <li>3D Pie</li> <li>Line</li> <li>Area</li> <li>3D Area</li> <li>Path</li> <li>Ribbon</li> <li>Surface</li> <li>Scatter</li> <li>Bubble</li> <li>Histogram</li> <li>Box</li> <li>Map</li> </ul> <p>You can get more information in the IBM Knowledge Center.</p> </li> <li> <p>Are interactive visualizations needed? SPSS Modeler Visualizations are not interactive.</p> </li> <li> <p>Are coding skills available or required? SPSS Modeler doesn\u2019t require any coding skills.</p> </li> <li> <p>What metrics can be calculated on the data? All state-of-the-art metrics are supported using the Data Audit node.</p> </li> <li> <p>Do metrics and visualization need to be shared with business stakeholders? Watson Studio supports sharing of SPSS Modeler Flows, also using a fine-grained user and access management system. However, those might not be suitable to stakeholders.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#application-component-actionable-insights","title":"Application Component:\u00a0Actionable insights","text":"<p>This is where most of your work fits in. It\u2019s where you create and evaluate your machine learning and deep learning models.</p>"},{"location":"architecture/architectural-decisions/#technology-component-mapping-guidelines_6","title":"Technology Component mapping guidelines","text":"<p>There are numerous technologies for creating and evaluating machine learning and deep learning models. Although different technologies differ in function and performance, those differences are usually miniscule. Therefore, the questions you should ask yourself are:</p> <ul> <li>What are the available skills regarding programming languages?</li> <li>What are the costs of skills regarding programming languages?</li> <li>What are the available skills regarding frameworks?</li> <li>What are the costs of skills regarding frameworks?</li> <li>Is model interchange required?</li> <li>Is parallel- or GPU-based training or scoring required?</li> <li>Do algorithms need to be tweaked or new algorithms be developed?</li> </ul>"},{"location":"architecture/architectural-decisions/#technology-mapping_4","title":"Technology Mapping","text":"<p>Because there\u2019s an abundance of open and closed source technologies, we are highlighting the most relevant ones in this article. Although it\u2019s the same for the other sections as well, decisions made in this section are very prone to change due to the iterative nature of this process model. Therefore, changing or combining multiple technologies is no problem, although the decisions that led to those changes should be explained and documented.</p>"},{"location":"architecture/architectural-decisions/#spss-modeler_1","title":"SPSS Modeler","text":"<p>This article has already introduced SPSS Modeler.</p> <ul> <li> <p>What are the available skills regarding programming languages? As a complete UI-based offering, SPSS doesn\u2019t need programming skills, although it can be extended using R scripts.</p> </li> <li> <p>What are the costs of skills regarding programming languages? As a complete UI-based offering, SPSS doesn\u2019t need programming skills, although it can be extended using R scripts.</p> </li> <li> <p>What are the available skills regarding frameworks? SPSS is an industry leader, so skills are generally available.</p> </li> <li> <p>What are the costs of skills regarding frameworks? Expert costs are usually lower in UI-based tools than in programming frameworks.</p> </li> <li> <p>Is model interchange required? SPSS Modeler supports PMML.</p> </li> <li> <p>Is parallel- or GPU-based training or scoring required? SPSS Modeler supports scaling through IBM Analytics Server or IBM Watson Studio using Apache Spark.</p> </li> <li> <p>Do algorithms need to be tweaked or new algorithms be developed? SPSS Modeler algorithms can\u2019t be changed, but you can add algorithms or customizations using the R language.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#rr-studio","title":"R/R-Studio","text":"<p>R and R-Studio are standards for open source-based data science. They\u2019re supported in IBM Cloud through IBM Watson Studio as well.</p> <ul> <li> <p>What are the available skills regarding programming languages? R programming skills are usually widely available because it\u2019s a standard programming language in many natural science-based university curriculums. It can be acquired rapidly because it is a procedural language with limited functional programming support.</p> </li> <li> <p>What are the costs of skills regarding programming languages? Costs of R programming are usually low.</p> </li> <li> <p>What are the available skills regarding frameworks? R is not only a programming language but also requires knowledge of tooling (R-Studio), and especially knowledge of the R library (CRAN) with 6000+ packages.</p> </li> <li> <p>What are the costs of skills regarding frameworks? Expert costs are correlated with knowledge of the CRAN library and years of experience and in the range of usual programmer costs.</p> </li> <li> <p>Is model interchange required? Some R libraries support exchange of models, but it is not standardized.</p> </li> <li> <p>Is parallel- or GPU-based training or scoring required? Some R libraries support scaling and GPU acceleration, but it is not standardized.</p> </li> <li> <p>Do algorithms need to be tweaked or new algorithms be developed? R needs algorithms to be implemented in C/C++ to run fast. So, tweaking and custom development usually involves C/C++ coding.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#python-pandas-and-scikit-learn","title":"Python, pandas and scikit-learn","text":"<p>Although R and R-Studio have been the standard for open source-based data science for a while, Python, pandas, and scikit-learn are right behind them. Python is a much cleaner programming language than R and easier to learn. Pandas is the Python equivalent to R data frames, supporting relational access to data. Finally, scikit-learn nicely groups all necessary machine learning algorithms together. It\u2019s supported in IBM Cloud through IBM Watson Studio as well.</p> <ul> <li> <p>What are the available skills regarding programming languages? Python skills are very widely available because Python is a clean and easy to learn programming language.</p> </li> <li> <p>What are the costs of skills regarding programming languages? Because of Python\u2019s properties mentioned above, the cost of Python programming skills is very low.</p> </li> <li> <p>What are the available skills regarding frameworks? Pandas and scikit-learn are very clean and easy-to-learn frameworks. Therefore, skills are widely available.</p> </li> <li> <p>What are the costs of skills regarding frameworks? Because of the properties mentioned above, the costs of skills are very low.</p> </li> <li> <p>Is model interchange required? All scikit-learn models can be (de)serialized. PMML is supported through third-party libraries.</p> </li> <li> <p>Is parallel- or GPU-based training or scoring required? Neither GPU nor scale-out is supported, although scale-up capabilities can be added individually to make use of multiple cores.</p> </li> <li> <p>Do algorithms need to be tweaked or new algorithms be developed? scikit-learn algorithms are very cleanly implemented. They all stick to the pipelines API, making reuse and interchange easy. Linear algebra is handled throughout with the numpy library. Therefore, tweaking and adding algorithms is straightforward.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#python-apache-spark-and-sparkml","title":"Python, Apache Spark and SparkML","text":"<p>Although Python, pandas, and scikit-learn are more widely adopted, the Apache Spark ecosystem is catching up, especially because of its scaling capabilities. It\u2019s supported in IBM Cloud through IBM Watson Studio as well.</p> <ul> <li> <p>What are the available skills regarding programming languages? Apache Spark supports Python, Java programming, Scala, and R as programming languages.</p> </li> <li> <p>What are the costs of skills regarding programming languages? The costs depend on what programming language is used, with Python typically the cheapest.</p> </li> <li> <p>What are the available skills regarding frameworks? Apache Spark skills are in high demand and usually not available.</p> </li> <li> <p>What are the costs of skills regarding frameworks? Apache Spark skills are in high demand and are usually expensive.</p> </li> <li> <p>Is model interchange required? All SparkML models can be (de)serialized. PMML is supported through third-party libraries.</p> </li> <li> <p>Is parallel- or GPU-based training or scoring required? All Apache Spark jobs are inherently parallel. However, GPU\u2019s are only supported through third-party libraries.</p> </li> <li> <p>Do algorithms need to be tweaked or new algorithms be developed? As in scikit-learn, algorithms are very cleanly implemented. They all stick to the pipelines API, making reuse and interchange easy. Linear algebra is handled throughout with built-in Apache Spark libraries. Therefore, tweaking and adding algorithms is straightforward.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#apache-systemml","title":"Apache SystemML","text":"<p>When it comes to relational data processing, SQL is a leader, mainly because an optimizer takes care of optimal query executions. Think of SystemML as an optimizer for linear algebra that\u2019s capable of creating optimal execution plans for jobs running on data parallel frameworks like Apache Spark.</p> <ul> <li> <p>What are the available skills regarding programming languages? SystemML has two domain-specific languages (DSL) with R and Python syntax.</p> </li> <li> <p>What are the costs of skills regarding programming languages? Although the DSLs are like R and Python, there is a learning curve involved.</p> </li> <li> <p>What are the available skills regarding frameworks? SystemML skills are very rare.</p> </li> <li> <p>What are the costs of skills regarding frameworks? Due to the high learning curve and skill scarcity, costs might get high.</p> </li> <li> <p>Is model interchange required? SystemML models can be (de)serialized. PMML is not supported. SystemML can import and run Caffe2 and Keras models.</p> </li> <li> <p>Is parallel- or GPU-based training or scoring required? All Apache Spark jobs are inherently parallel. SystemML uses this property. In addition, GPU\u2019s are supported as well.</p> </li> <li> <p>Do algorithms need to be tweaked or new algorithms be developed? Although SystemML comes with a large set of pre-implemented algorithms for machine learning and deep learning, its strengths are in tweaking existing algorithms or implementing new ones because the DSL allows for concentrating on the mathematical implementation of the algorithm. The rest is handled by the framework. This makes it an ideal choice for these kind of tasks.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#keras-and-tensorflow","title":"Keras and TensorFlow","text":"<p>TensorFlow is one of the most widely used deep learning frameworks. At its core, it is a linear algebra library supporting automatic differentiation. TensorFlow\u2019s Python-driven syntax is relatively complex. Therefore, Keras provides an abstraction layer on top of TensorFlow. Both frameworks are seamlessly supported in IBM Cloud through Watson Studio and Watson Machine Learning.</p> <ul> <li> <p>What are the available skills regarding programming languages? Python is the core programming language for Keras and TensorFlow.</p> </li> <li> <p>What are the available skills regarding frameworks? Keras and TensorFlow skills are relatively rare.</p> </li> <li> <p>What are the costs of skills regarding frameworks? Due to the high learning curve and skill scarcity, costs might get high.</p> </li> <li> <p>Is model interchange required? Keras and TensorFlow have their own model exchange formats. There are converters from and to ONNX.</p> </li> <li> <p>Is parallel- or GPU-based training or sc oring required? Running TensorFlow on top of ApacheSpark is supported through TensorFrames and TensorSpark. Keras models can be run on ApacheSpark using DeepLearning4J and SystemML. Both of the latter frameworks also support GPUs. TensorFlow (and therefore, Keras) support GPU natively as well.</p> </li> <li> <p>Do algorithms need to be tweaked or new algorithms be developed? TensorFlow is a linear algebra execution engine. Therefore, it\u2019s optimally suited for tweaking and creating new algorithms. Keras is a very flexible deep learning library that supports many neural network layouts.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#application-component-applications-and-data-products","title":"Application Component:\u00a0Applications and Data Products","text":"<p>Models are fine, but their value rises when they can be consumed by the ordinary business user. Therefore, you must create a data product. Data products don\u2019t necessarily need to stay on the cloud. They can be pushed to mobile or enterprise applications.</p> <p>In contrast to machine learning and deep learning frameworks, the space of frameworks to create data product is tiny. This might reflect what the current state-of-the-art technology in data science concentrates on. Depending on the requirements, data products are relatively visualization-centric after a lot of user input data has been gathered. They also might involve asynchronous workflows as batch data integration and model training and scoring is performed within the workflow. </p> <p>Questions to ask about the technology are:</p> <ul> <li>What skills are present for developing a data product?</li> <li>What skills are necessary for developing a data product?</li> <li>Is instant feedback required or is batch processing accepted?</li> <li>What\u2019s the degree of customization needed?</li> <li>What\u2019s the target audience? Is cloud scale deployment for a public use base required?</li> </ul>"},{"location":"architecture/architectural-decisions/#technology-mapping_5","title":"Technology Mapping","text":"<p>Currently, only a limited set of frameworks and technologies is available in different categories. In the following section, I\u2019ve explained the most prominent examples.</p>"},{"location":"architecture/architectural-decisions/#r-shiny","title":"R-Shiny","text":"<p>R-Shiny is a great framework for building data products. Closely tied to the R language, it enables data scientists to rapidly create a UI on top of existing R-scripts.</p> <ul> <li> <p>What skills are present for developing a data product? R-Shiny requires R development skills. So, it best fits into an R development ecosystem.</p> </li> <li> <p>What skills are necessary for developing a data product? Although based on R, R-Shiny needs additional skills. For experienced R developers, the learning curve is steep and additional knowledge to acquire is minimal.</p> </li> <li> <p>Is instant feedback required or is batch processing accepted? The messaging model of R-Shiny supports instant UI feedback when server-side data structures are updated. Therefore, the response time is independent of the framework and should be considered and resolved programmatically on the server side.</p> </li> <li> <p>What\u2019s the degree of customization needed? Although R-Shiny is an extensible framework, extending it requires a deep understanding of the framework and R-Technology. Out of the box, there is a large set of UI widgets end elements supported, allowing for very customizable applications. If requirements go beyond those capabilities, costly extensions are required.</p> </li> <li> <p>What\u2019s the target audience? Is cloud scale deployment for a public use base required? R-Shiny applications look very professional, although quite distinguishable. Therefore, the target audience must accept the UI design limitations. R-Shiny is best dynamically scaled horizontally in a container environment like Kubernetes. Ideally, every user session runs in its own container because R and R-Shiny are very sensitive to main memory shortages</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#node-red_1","title":"Node-RED","text":"<p>Although Node-RED is a no-code/low-code data flow/data integration environment, because of its modular nature it supports various extensions including the dash boarding extension. This extension allows for fast creation of user interfaces including advanced visualizations that are updated in real-time.</p> <ul> <li> <p>What skills are present for developing a data product? Due to the completely graphical user interface-based software development approach, only basic programming skills are required to build data products with Node-RED.</p> </li> <li> <p>What skills are necessary for developing a data product? Any resource familiar with flow-based programming as used in many state-of-the-art ETL and data mining tools will have a fast start with Node-RED. Basic JavaScript knowledge is required for creating advanced flows and for extending the framework.</p> </li> <li> <p>Is instant feedback required or is batch processing accepted? The UI instantly reflects updates of the data model. Therefore, all considerations regarding feedback delay should be considered when developing the data integration flow or potentially involved calls to synchronous or asynchronous third-party services.</p> </li> <li> <p>What\u2019s the degree of customization needed? Node-RED is a Node.js/JavaScript-based framework. Custom UI widgets require advanced Node.js development skills.</p> </li> <li> <p>What\u2019s the target audience? Is cloud scale deployment for a public use base required? The Node-RED dashboard can be deployed for a pubic user base as long as the limitations regarding UI customization are acceptable. Because Node.js runs on a single threaded event loop, scaling must be done horizontally, preferably using a containerized environment. Note: The Internet of Things Starter kit in IBM Cloud supports horizontal scaling out of the box.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#d3","title":"D3","text":"<p>When it comes to custom application development, D3 is one of the most prominent and most widely used visualization widget frameworks with a large open source ecosystem contributing a lot of widgets for every desirable use case. There\u2019s a good chance that you can find a D3 widget to fit your use case.</p> <ul> <li> <p>What skills are present for developing a data product? D3 fits best into an existing, preferably JavaScript-based developer ecosystem, although JavaScript is only required on the client side. Therefore, on the server side, any REST-based endpoints in any programming language are supported. One example is REST endpoints accessed by a D3 UI provided by Apache Livy that encapsulates Apache Spark jobs.</p> </li> <li> <p>What skills are necessary for developing a data product? D3 requires sophisticated D3 skills and at least client-side JavaScript skills. Skills in a JavaScript AJAX framework like AngularJS are highly recommended. On the server side, capabilities of providing REST endpoints to the D3 applications are required.</p> </li> <li> <p>Is instant feedback required or is batch processing accepted? The UI instantly reflects updates of the data model. Therefore, all considerations regarding feedback delay should be considered when developing the data integration flow or potentially involved calls to synchronous or asynchronous third-party services.</p> </li> <li> <p>What\u2019s the degree of customization needed? D3 applications usually are implemented from scratch. Therefore, this solution provides the most flexibility to the end user.</p> </li> <li> <p>What\u2019s the target audience? Is cloud scale deployment for a public use base required? As a cloud native application, a D3-based data product can provide all capabilities for horizontal and vertical scaling and full adoption to user requirements.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#application-component-security-information-governance-and-systems-management","title":"Application Component:\u00a0Security, information governance and systems management","text":"<p>This important step can be easily forgotten. It\u2019s important to control who has access to which information for many compliance regulations. In addition, modern data science architectures involve many components that require operational aspects as well.</p> <p>Data privacy is a major challenge in many data science projects. Questions that you should ask are:</p> <ul> <li>What granularity is required for managing user access to data assets?</li> <li>Are existing user registries required to be integrated?</li> <li>Who is taking care of operational aspects?</li> <li>What are the requirements for data retention?</li> <li>What level of security against attacks from hackers is required?</li> </ul>"},{"location":"architecture/architectural-decisions/#technology-mapping_6","title":"Technology Mapping","text":"<p>Again, there\u2019s a lot of software for this as well as ways to solve the requirements involved in this topic. We have chosen representative examples for this article.</p> <p>Deploying a productive client-facing web application brings with it serious risks. IBM Cloud Internet Services provides global points of presence (PoPs). It includes domain name service (DNS), global load balancer (GLB), distributed denial of service (DDoS) protection, web application firewall (WAF), transport layer security (TLS), and caching.</p> <ul> <li>What level of security against attacks from hackers is required? Internet Services is using services from CloudFlare, the world leader in this space.</li> </ul>"},{"location":"architecture/architectural-decisions/#ibm-app-id","title":"IBM App ID","text":"<p>Identity Management allows for cloud-based user and identity management for web and mobile applications, APIs, and back-end systems. Cloud users can sign up and sign in with App ID\u2019s scalable user registry or social login with Google or Facebook. Enterprise users can be integrated using SAML 2.0 federation.</p> <ul> <li> <p>What granularity is required for managing user access to data assets? IBM App ID supports user management but no group/roles. Therefore, fine-grained access must be managed within the application.</p> </li> <li> <p>Are existing user registries required to be integrated? IBM App ID supports registry integration using SAML 2.0 federation.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#object-storage_1","title":"Object Storage","text":"<p>Object Storage is a standard when it comes to modern, cost-effective cloud storage.</p> <ul> <li> <p>What granularity is required for managing user access to data assets? IBM Identity and Access Management (IAM) integration allows for granular access control at the bucket-level using role-based policies.</p> </li> <li> <p>What are the requirements for data retention? Object storage supports different storage classes for frequently accessed data, occasionally accessed data, and long-term data retention with standard, vault, and cold vault. The Flex class allows for dynamic data access and automates this process. Physical deletion of data still must be triggered externally.</p> </li> <li> <p>Who is taking care of operational aspects? Regional and cross-region resiliency options allow for increased data reliability.</p> </li> <li> <p>What level of security against attacks from hackers is required? All data is encrypted at rest and in flight by default. Keys are automatically managed by default, but optionally can be self-managed or managed using IBM Key Protect.</p> </li> </ul>"},{"location":"architecture/architectural-decisions/#ibm-cloud-paassaas","title":"IBM Cloud PaaS/SaaS","text":"<p>IBM Cloud PaaS/SaaS eliminates operational aspects from data science projects because all components involved are managed by IBM.</p> <ul> <li>Who is taking care of operational aspects? In PaaS/SaaS, cloud operational aspects are being taken care of by the cloud provider.</li> </ul>"},{"location":"architecture/ddd/","title":"Domain-Driven Design notes","text":"<p>Event storming description is in my EDA web site with a summary of Domain-Driven Design.</p>"},{"location":"architecture/ddd/#analysis-practices-to-keep-in-mind","title":"Analysis Practices to keep in mind","text":""},{"location":"architecture/ddd/#identifying-domains-and-core-domains","title":"Identifying domains and core domains","text":"<p>Questions to consider:</p> <ul> <li>why the custom software is being written rather than opting for a commercial off-the-shelf product?</li> <li>How does it fit within the strategy of the company?</li> <li>Does part of the software give the business a competitive edge?</li> <li>What are the motivations behind a requirement? is it constrained by existing system?</li> </ul> <p>Large problem domains can be partitioned into subdomains to manage complexity. Subdomains are abstract concepts. Subdomains are typically reflecting the business capabilities of the business organizational structure.  Subdomains represent areas of capability, define business processes, and represent the functionality of a system.</p> <p>For core domain focus on the product rather than view it as a standalone project. Your product is an evolving succession of feature enhancements and tweaks.</p> <p>Distillation is used to break down a large problem domain to discover the core, supporting, and generic domains.</p> <p>Consider outsourcing, buying in, on the supporting and generic domains.</p> <p>A model is represented as an analysis model and a code model. They are one and the same.</p> <p>A Model-Driven Design binds the analysis model and a code model through the use of the shared language.</p> <p>Ubiquitous language is about collaboration and not the development team just adopting the language of the business.</p>"},{"location":"architecture/ddd/#event-storming","title":"Event storming","text":"<ul> <li>A domain event is something meaningful to the experts that happened in the domain.</li> <li>Always keep time ordering in the event discovery</li> <li>Add commands that cause events. A single command may actually cause multiple domain events</li> <li>Stack events arriving at the same time</li> <li>If users are not comfortable with aggregate, talk as business entity</li> <li>Classical DDD was in the OOD too, so aggregate has behavior, and handles commands and emits domain events.  More REST oriented it can be mapped to a Resource.</li> <li>By grouping commands, events and aggregate, keep the process in time order. Repeat the aggregate sticker if the same aggregate receives a different command stimulus over time.</li> <li>Keep the time in correct order so that the time is accurately represented as things occur in your model over time</li> <li> <p>4<sup>th</sup> step is to derive context boundaries: from the aggregate, commands and events you realize that some of these apply to the core domain and some of them actually don't.  Some of the commands and events will happen to entities that belong in other bounded context.</p> <ul> <li>Within a business context every use of a given domain terms, phrases, or sentences, the Ubiquitous Language, inside the boundary has a specific contextual meaning</li> <li>You will fail if you represent the same concept into three different context the same way: a \"Insurance Policy\" to cover Underwriting, Claim or Inspection. </li> <li>Bounded context defines the applicability of the model and ensures that its integrity is retained </li> </ul> </li> <li> <p>Assess the core domains, that are significant and competitively advantageous to the organization.</p> </li> <li>Whatever is supporting or generic, move off to their own bounded context</li> <li>Big problem domain is split up into subdomains for easier understanding. A model in a context is created for each subdomain.</li> <li> <p>Context maps help you to understand the bigger picture</p> </li> <li> <p>5<sup>th</sup> step is to understand certain views that are important to the user, as well as policy and process that consume events and act on command. </p> </li> </ul> <p>If you have areas of an application that resemble the Big Ball of Mud (BBoM) pattern then the best thing to do is to put  a boundary around them to prevent the mud spreading into new areas of the application.</p> <p>An anti-corruption layer wraps the communication with legacy or third-party code to protect the integrity of a bounded context.  An anti-corruption layer manages the transformation of one context\u2019s view to another, retaining the integrity of new code and preventing it from becoming a BBoM</p>"},{"location":"architecture/ddd/#some-other-notes","title":"Some other notes","text":"<p>May only use events, commands and aggregates in the event storming.</p> <p>This is more the ownership of the data view that will drag the coupling back to the aggregate.</p> <p>Think about developing scenario to validate the model and test cases to verify aggregate correctness</p> <p>Context boundaries can be influenced by:</p> <ul> <li>Ambiguity in terminology and concepts of the domain</li> <li>Alignment to subdomains and business capabilities</li> <li>Team organization and physical location</li> <li>Legacy code base</li> <li>Third party integration</li> </ul>"},{"location":"architecture/ddd/#ensuring-consistency-in-the-ubiquitous-language","title":"Ensuring consistency in the Ubiquitous Language","text":"<ul> <li>Ensure that you have linguistic consistency. If you are using a term in code that the domain expert doesn\u2019t say,  you need to check it with her. It could be that you have found a concept that was required,</li> <li>Create a glossary of domain terms with the domain expert to avoid confusion</li> <li>Ensure that you use one word for a specific concept.</li> <li>Avoid overloaded terms, avoid design pattern name (like DTO)</li> <li>Naming is very important. Validate your code design by speaking to your business users about classes. </li> <li>The UL should be visible everywhere, from namespaces to classes, and from properties to method names</li> <li>Refactor your code to embrace the evolution by using more intention-revealing method names.</li> </ul>"},{"location":"architecture/ddd/#context-mapping","title":"Context mapping","text":"<p>A context map reflects the way things are right now. It provides a holistic view of the technical  integration methods and relationships between bounded contexts. Without them, models can be  compromised, and bounded contexts can quickly change to balls of mud as integration blurs the  lines of a model\u2019s applicability.</p> <p>An anti-corruption layer provides isolation for a model when interfacing with another context.  The layer ensures integrity is not compromised by providing translation from one context to another.</p> <p>Integration using the shared kernel pattern is for contexts that have an overlap and shared a common model.</p> <p>Integration via an open host service exposes an external API instead of requiring clients to transform from one model to another.  Typically, this creates a published language for clients to work with.</p> <p>The conformist pattern describes the relationship between an upstream and downstream team where the upstream team  will not collaborate with the downstream team. This is often the case when the upstream team is a third-party.</p>"},{"location":"architecture/ddd/#implementation-related-patterns","title":"Implementation related patterns","text":"<p>A domain model\u2019s structure is composed of entities and value objects that represent concepts in the problem domain. Associations that can be traversed in more than one direction also increase complexity. Spend time to design such associations.</p> <p>Aggregate represents a consistency boundary that decomposes large models into smaller clusters of domain objects that are technically easier to manage.</p> <p>Bidirectional relationship adds technical complexity and obscures domain concepts. Who own the relationship in which context?</p> <p>When defining object relationships, ask at least the following:</p> <ul> <li>What is the behavior that an association is fulfilling?</li> <li>who needs the relationship to function?</li> <li>how to qualify a relation to limit the  umber of object to be hydrated? </li> </ul>"},{"location":"architecture/ddd/#value-objects","title":"Value Objects","text":"<p>Value objects are DDD modeling constructs that represent descriptive quantities like magnitudes, value and measurements.  (e.g. Money, Currency) They do not have identity. Value objects are immutable; their values cannot change.</p> <p>Value objects are cohesive; they can wrap multiple attributes to fully encapsulate a single concept.</p> <p>Value objects can be combined to create new values without altering the original.</p> <p>Value objects should never be in an invalid state</p> <p>Remember that value objects in one domain might be entities in another and vice versa.</p> <p>You can persist the value of value objects directly in a de-normalized form.</p>"},{"location":"architecture/ddd/#entities","title":"Entities","text":"<p>Entities are domain concepts that have a unique identity in the problem domain. Choosing an identifier is a major implementation concern. They may have a life cycle.</p> <p>Entities should always be valid for the given context.</p> <p>Invariants are fundamental truths about an entity, so they should always be enforced.</p> <p>Be careful of modeling physical concepts as single entities. The typical Customer entity can often be logically split across multiple bounded contexts into numerous entities.</p> <p>Behavior can be pushed in domain service object or value objects. For example the <code>HolidayBooking</code> entity delegates behavior and business rules to the <code>Stay</code> VO. </p> <p>Avoid bloating any classes with more than one responsibility: use Single Responsibility Principle</p> <p>You should be careful about how much of your object graph to expose, because clients will couple themselves to it. (See the law of demeter dot counting)</p> <p>Entity\u2019s interface should expose expressive methods that communicate domain behaviors instead of exposing state</p>"},{"location":"architecture/ddd/#aggregates","title":"Aggregates","text":"<p>Domain invariants are statements or rules that must always be adhered to. </p> <p>Aggregate is an explicit grouping of domain objects designed to support the behaviors and invariants of a domain model  while acting as a consistency and transactional boundary. </p> <p>Aggregate is technical implementation concept that help to better manage bi-directional associations and ensure consistency. </p> <p>By effect, aggregates decompose large object graphs into small clusters of domain objects to reduce the complexity.</p> <p>When all domain objects involved in an invariant reside within the same aggregate, it is easier to compare and coordinate  them to ensure that their collective states do not violate a domain invariant.</p> <p>An object reference is necessary if the association is supporting a domain invariant for a specific use case: </p> <ul> <li>a Customer has Orders, but an Order only needs a Customer\u2019s ID to meet invariants.</li> <li>The order lines do not exist or make sense outside the concept of an order</li> </ul> <p>Align aggregate boundaries with domain invariants to help enforce them. </p> <p>Aggregates ensure transactional boundaries are set at the right level of granularity. </p> <p>To enforce consistency all interaction needs to go through a single entity known as the aggregate root. </p> <p>Domain objects outside the aggregate can only hold a reference to the aggregate root.</p> <p>When an aggregate is deleted all the domain objects within it must be removed as well.</p> <p>Objects outside of the aggregates can have no access to any of the internal objects of the aggregate;  this ensures control of the domain objects and ensures consistency within the aggregate.</p> <p>No parts of the aggregates can be separately pulled from the data store unless it is purely for reporting.  This leads to eventual consistency between aggregates.</p> <p>To demonstrate an eventually consistent rule spanning multiple aggregates, consider a loyalty policy:  If a customer has spent more than $100 in the past year, she gets 10% off all further purchases. When an order is placed,  the Order aggregate is updated inside a transaction exclusively. At that point, the Loyalty aggregate does not have a  consistent view of the customer\u2019s purchase history because it was not updated in the same transaction. However,  the Order aggregate can publish an event signalling the order was created, which the Loyalty aggregate can subscribe to. In time the loyalty object will be able to update the customer\u2019s loyalty when it handles the event.</p> <p>When designing aggregate, try starting small and justifying the addition of each new concept to the aggregate. An aggregate represents a concept in your domain and is not a container for items. Ensure that each object  is required to define the behavior of the aggregate instead of just being related to the aggregate. Why would you need to load all the items to add another?</p> <p>A large aggregate is likely to have more than one responsibility, meaning it is involved in multiple business use cases.  Subsequently, there is greater opportunity for multiple users to be making changes to a single aggregate, increasing concurrency conflict.</p> <p>If you find that you are modifying more than one aggregate in a transaction, it may be a sign that your aggregate boundaries can be better aligned with the problem domain.</p> <p>Aggregates should not be designed around UIs. Instead map from multiple aggregates onto a single view model that contains all the data a page needs. But if we need to go to 3 or more repository to build the view, then we may want to consider CQRS.</p> <p>Persistence, consistency, and concurrency are all important implementation details that can be tricky to get right and may cause you to rethink your aggregate boundaries.</p> <p>Only aggregate roots can be obtained directly with database queries. The domain objects that are inner components of the  aggregate can be accessed only via the aggregate root. Each aggregate has a matching repository that abstracts the   underlying database and that will only allow aggregates to be persisted and hydrated</p> <p>A Delete Operation Must Remove Everything within the Aggregate Boundary at Once.</p> <p>Avoid lazy loading except if you really need so, and mostly for performance reason.</p>"},{"location":"architecture/ddd/#application-architecture","title":"Application Architecture","text":"<p>Application architecture structures the application to keep clear separation of concerns between technical capabilities from the business / domain logic. Presentation, persistence, and domain logic concerns of an application will change at different rates.</p> <p>Different pattern can be used, the layered architecture shows the domain model to be on its own layer isolated from the rest. Domain logic focuses on domain rules, concepts, information, and work flows. When looking at the use cases to develop the application service, it may be possible to discover the domain model may not be required at all, if for example the behavioral logic is just to do CRUD on data.</p> <p></p> <p>Surrounding the domain layer is an application layer that abstracts the low-level details of the domain   behind a coarse-grained API.</p> <p>The darker blue circle represents infrastructure layer, it adapts to the external world. </p> <p>Domain depends on nothing, application layer depends only on the domain, and adapters and integration  dependencies face inward.</p> <p>Application services operate at a higher level of abstraction than the domain objects, exposing a coarse-grained set of services  using the Facade pattern.</p> <p>The application service layer is responsible to coordinate the retrieval of domain objects from a data store, delegating work to them,  and then saving the updated state.</p> <p>The application service layer enables the support of disparate clients without compromising the domain layer\u2019s integrity.  New clients must adapt to the input defined by the application\u2019s API. They must also transform the output of the application service into a format that is suitable for them.  In this way, the application layer can be thought of as an anti-corruption layer, ensuring that the domain layer stays  pure and unaffected by external technical details. Application logic is all about coordination and orchestration through delegation to domain and infrastructural services.</p> <p>Infrastructure layer supports APIs (gateway), messaging, persistence, integration with other bounded contexts, and supports cross-cutting concerns as logging and security...</p> <p>Data transfer objects (DTOs), presentation models, and application event objects are used to communicate changes or actions in the domain.</p> <p>A good isolated application architecture is validated by using unit tests for the app and domain layers combined with mocks for external resources and integration tests for the infrastructure layer.</p> <p>An application that is composed of two or more bounded contexts may have an architectural style for the user interfaces and  different architectures for each of the bounded contexts.</p> <p>The figure below illustrates that those layers can be conceptual when integrating with multiple bounded contexts: the application layer is spread over the user interface, the back end for front end component and over a set of other microservices / bounded contexts: </p> <p></p> <p>Inside of the microservice app, we can use the layered architecture as a way to organize the code, for example, infrastructure  supports JPA frameworks, messaging API, integrates external systems like a distributed cache, data base, document oriented DB...</p> <p>Now with Java framework with annotation mechanism the infrastructure is going into the application layer quite easily. It should  not be a big concern, but developers need to take care of what stays in the app layer, and when to add classes in infrastructure one.   </p>"},{"location":"architecture/eda/","title":"Event Driven Architecture","text":"<p>Info</p> <p>Updated 7/7/2022</p> <ul> <li>Main EDA reference implementation</li> <li>Vaccine reference implementation</li> <li>Kafka notes</li> <li>Kafka Streams</li> <li>Kafka Connect with hand-on lab</li> <li>Mirror maker 2</li> <li>Consumer group</li> </ul>"},{"location":"architecture/eda/#labs","title":"Labs","text":"<ul> <li>Event Streams on Cloud lab</li> <li>Security and access control with IBM Cloud lab</li> <li>Real time inventory management with kafka streams and kafka connect:</li> </ul>"},{"location":"architecture/eda/#demos","title":"demos","text":"<ul> <li>Real-time inventory with flink, elasticsearch, kafka, cloud object storage</li> </ul> App Type Build Registry Git workflow Local OCP Store sale event producer simulator Quarkus 2.7.1 ok quay.io/ibmcase/eda-store-simulator ko ok gitops ok store-inventory Quarkus 2.7.1 test ko quay.io/ibmcase/store-aggregator ok ok item-inventory Quarkus 2.7.1 test ko quay.io/ibmcase/item-aggregator ok ok <ul> <li>GitOps eda-rt-inventory-gitops includes the deployment of the 3 apps + kafka connectors and docker-compose to run the solution local.</li> <li> <p>Demo scripts in: refarch-eda/scenarios/realtime-inventory</p> </li> <li> <p>Order demos based on developer experience article:</p> <ul> <li>Order demo gitops  The readme is up to date</li> <li>Order producer</li> <li>Order consumer</li> </ul> </li> <li> <p>Angular app with nodejs BFF to present the container shipment demonstration</p> </li> <li>A fleet simulator microservice</li> <li>Real time analytics as streaming application demonstrates real time event processing applied to inputs from the ships and containers used in the K Container Shipment Use Case</li> <li>Event sourcing and CQRS pattern illustrated</li> <li>Event Streams Samples for IBM messaging github</li> </ul>"},{"location":"architecture/flow-architectures/","title":"Flow architecture","text":"<p>From the James Urquhart's book: Flow architecture and personal studies.</p> <p>As more and more of our businesses \u201cgo digital\u201d, the groundwork is in place to fundamentally change how real-time data is exchanged across organization boundaries. Data from different sources can be combined to create a holistic view of a business situation.</p> <p>Flow is networked software integration that is event-driven, loosely coupled, and highly adaptable and extensible.</p> <p>Value is created by interacting with the flow, and not just the data movement.</p> <p>Since the beginning of IT as an industry, we are digitizing and automating the exchanges of value, and we spend a lot of time and money to execute key transactions with less human intervention. However, most of the integrations we execute across organizational boundaries today are not in real time. Today, most\u2014perhaps all\u2014digital financial transactions in the world economy still rely on batch processing at some point in the course of settlement.</p> <p>There is no consistent and agreed-upon mechanism for exchanging signals for immediate action across companies or industries.</p> <p>It is still extremely rare for a company to make real-time data available for  unknown consumers to process at will.</p> <p>This is why modern event-driven architecture (EDA) will enable profound changes in  the way companies integrate. EDAs are highly decoupled architectures, meaning there  are very few mutual dependencies between the parties at both ends of the exchange.</p>"},{"location":"architecture/flow-architectures/#1-flow-characteristics","title":"1- Flow characteristics","text":"<ul> <li>Consumers requests data streams through self-service interfaces, and get the data continuously.</li> <li>Producers maintain control of relevant information to transmit and when.</li> <li>Event packages information of data state changes, with timestamp and unique ID.  The context included with the transmitted data allows the consumer to better understand the nature of that data.</li> <li>The transmission of a series of events between two parties is called an event stream.</li> </ul> <p>The more streams there are from more sources, the more flow consumers will be drawn to  those streams and the more experimentation there will be. Over time, organizations will find  new ways to tie activities together to generate new value.</p> <p>Composable architectures allow the developer to assemble fine grained parts using  consistent mechanisms for both inputting data and consuming the output. In contextual architectures, the environment provides specific contexts in  which integration can happen. Developer must know a lot about the data that is available,  the mechanism by which the data will be passed, the rules for coding and deploying   the software.</p> <p>EDA provides a much more composable and evolutionary approach to building event and data streams.</p>"},{"location":"architecture/flow-architectures/#2-business-motivations","title":"2- Business motivations","text":"<ul> <li>Do digital transformation to improve customer experiences. Customers expect their data to  be used in a way that is valuable to them, not just to the vendors. Sharing data between organizations  can lead to new business opportunities.</li> <li>Improve process automation, to drive efficiencies and profitability. The most limiting constraint in the  process hides any improvements made to other steps. Finding constraints is where value stream mapping shines: it uses lead time (queue time) and actual time to do the work. EDA will help to get time stamp and data  for steps in the process that are not completely in scope of a business process: may be cross business boundaries.</li> <li>Extract innovative value from data streams. Innovation as better solution for existing problem, or as new solution to emerging problems.</li> </ul> <p>To improve process time, software needs accurate data at the time to process the work. As business evolve, having a rigid protocol to get the data, impacts process time. A business will need to experiment with new data sources  when they are available and potentially relevant to their business.</p> <p>Stream processing improves interoperability (exchange data)</p> <p>Innovation is not adaptation. Companies must adapt constantly just to survive, like adding features on a product to pace with competition. Digital transformation aimed at avoiding competitive disruption is not innovation.</p> <p>As the number of stream options grows, more and more business capabilities will be  defined in terms of stream processing. This will drive developers to find easier ways  to discover, connect to, and process streams.</p>"},{"location":"architecture/flow-architectures/#enabler-for-flow-adoption","title":"Enabler for flow adoption","text":"<ul> <li>Lowering the cost of stream processing: Integration costs dominate modern IT budgets. For many integrations, the cost of creating interaction between systems is simply too high for what little value is gained. With common interfaces and protocols that enable flows, the integration cost will be lower and people will find new uses for streaming that will boost the overall demand for streaming technologies. The Jevons paradox at work</li> <li>Increasing the flexibility in composing data flows: \"pipe\" data streams from one processing  system to another through common interfaces and protocols.</li> <li>Creating and utilizing a rich market ecosystem around key streams. The equities markets have all moved entirely to electronic forms of executing their marketplaces. Health-care data streams for building services around patient data. Refrigerators streaming data to grocery delivery services. </li> </ul> <p>Flow must be secure (producers maintain control over who can access their events),  agile (change schema definitions),  timely (Data must arrive in a time frame that is appropriate for the context to which it is being applied),  manageable and retain a memory of its past. </p> <p>Serverless, stream processing, machine learning, will create alternative to batch processing.</p>"},{"location":"architecture/flow-architectures/#3-market","title":"3- Market","text":"<p>SOA has brought challenges for adoption and scaling. Many applications have their own interfaces and even protocols to expose their functionality, so most integrations need protocol and  data model translations. </p> <p>The adoption of queues and adaptors to do data and protocol translation was a scalable solution.  Extending this central layer of adaptation was the Enterprise Service Bus, with intelligent pipes / flows. </p> <p>Message queues and ESBs are important to the development of streaming architectures but to support scalability and address complexity more decoupling is needed between  producers and consumers.</p> <p>For IoT MQTT is the standard for messaging protocols in a lightweight pub/sub  transport protocol. MQTT supports 3 service levels: 0 - at most once, 1- at least once, 2 - exactly once. It allows for messaging between device to cloud and cloud to device. It supports for persistent sessions  reduces the time to reconnect the client with the broker. The MQTT broker manages a list of topics, which enable it to identify groups of subscribers interested in a collection of messages.</p> <p>For event processing three type of engines:</p> <ul> <li>Functions (including low-code or no-code processors): WAS lambda, Knative eventing, Flink, Storm. Mendix and Vantiq have event-driven low code platform.</li> <li>log-based event streaming platforms: Apache Kafka, Apache Pulsar, AWS Kinesis, and Microsoft Azure Event Hubs. Topic becomes a system of record, as event sourcing pattern implementation.</li> <li>real-time stateful systems: Digital twins are software agents supporting the problem domain in a stateful manner. Behavior is supported by code or rules, and relationship between agents.  Agents can monitor the overall system state. Swim.ai builds its model dynamically from the event stream and provides built-in machine learning capabilities that enable both continuous learning and high performance model execution</li> </ul> <p>Mainstream adoption of flow itself will be five to ten years from now (2020). Flow will have to prove that  it meets security criteria for everything from electronic payments, to health-care data, to classified  information. The CNCF\u2019s CloudEvents specification, for instance, strongly suggests payloads be encrypted. There is no single approach to defining an event with encryption explicitly supported  that can be read by any event-consuming application (MQTT, AMQP, have different encryption and TLS add more for  TCP connection).</p> <p>Consumers need assurances that the data they receive in an event is valid and accurate, a practice  known as data provenance.  </p> <p>Data provenance is defined as \u201ca record of the inputs, entities,  systems, and processes that influence data of interest, providing a historical record of the  data and its origins\"</p> <p>Provenance has to maintained by the producer as a checksum number created by parsing the event data, and encrypted by the producer's key. CloudEvent has metadata about the message. When sent to Kafka they are  immutable record. Now the traceability of the consumers in kafka world is a major challenge. Blockchain may also be used to track immutable record with network parties attest its accuracy.</p> <p>Applying the concept of data loose value over time, it is important to act on data as early as possible, close to creation time. After a period of time data becomes less valuable.</p> <p>Two time factors are important in this data processing: latency (time to deliver data to consumers) and retention (time to keep data). For latency try to reduce the number of network segment between producer and consumers. Considering edge computing as a way to bring event processing close to the source. The event processing add time to the end to end latency. Considering constraining the processing time frame.</p> <p>Retention is a problem linked to the business requirements, and we need to assess for each topic how long an event is still valuable for the consumers. Not keeping enough events will impact correctness of consumer state,  projection views... keeping for too long, increase the cost of storage, but also the time to rebuild data  projection. </p> <p>Finally, producers will want to trust that only authorized consumers are using the events they produce. Also it may be possible to imagine a way to control the intellectual property of the data so producer can keep  its ownership. Data consumption should be done via payment like we do with music subscription.</p>"},{"location":"architecture/flow-architectures/#flow-patterns","title":"Flow patterns:","text":""},{"location":"architecture/flow-architectures/#collector-pattern","title":"Collector pattern","text":"<p>The Collector pattern is a pattern in which a single consumer subscribes to topics from multiple producers. </p>"},{"location":"architecture/flow-architectures/#distributor-pattern","title":"Distributor pattern","text":"<p>Each event in a stream is distributed to multiple consumers. It could be a hard problem to solve when doing it across geographically distributed systems. Edge computing can be used to distribute  streaming endpoints closer to the consumers that need those streams. Alternate  is to moving the event processing close to the source. For many Distributor use cases,   partitioning the data by region is probably smart, and flow interfaces will need to take   this into account.</p>"},{"location":"architecture/flow-architectures/#signal-pattern","title":"Signal pattern","text":"<p>The Signal pattern is a general pattern that represents functions that exchange data between  actors based on a distinct function or process, in can be seen as a traffic cop.   It supports multiple producers and multiple consumers. The signal pattern is supported  by multiple event processing each handling one aspect of the event processing.</p> <p>Stream processing may route event streams between several distributed edge computing services as  well as core shared services, but then we need management layer to get global view of the systems. They need to be integrated into observability tool. But the \"single pane of glass\" is often a lure as distributed systems require distributed decision-making. More local solutions are more agile, flexible and better address local problems for improved resilience.  </p> <p>One of the challenge of complex adaptive systems is that any agent participating in  the system has difficulty seeing how the system as a whole operates, because of its limited connections to other neighbor agents.</p>"},{"location":"architecture/flow-architectures/#facilitator-pattern","title":"Facilitator pattern","text":"<p>A specialized form of Signal pattern, facilitator is a \"broker\" to match producers' events to consumers' demands. It is like matching sellers with buyers.</p>"},{"location":"architecture/flow-architectures/#4-identifying-flow-in-your-business","title":"4- Identifying flow in your business","text":"<p>The classical usee case categories:</p> <ul> <li> <p>Addressing and discovery: In modern complex systems environments, multiple systems need to be informed of the new entity, be able to utilize to assign work to it. Addressing and discovery happens across organizational boundaries (for example in real-time inventory SKU is used to identify item for both supplier and retailers). To seek such use cases, look at tracking problems like who or what is involved in a problem domain that is difficult to scale. With event stream centric approach, A&amp;D is done via a registry service used by new agents to indicate their existence, and the service publishes an event to a topic to broadcast the information about the new agent. A second option is to use a discovery service to watch specific event stream for certain transmissions that indicate the presence of an agent. Swim.ai continuously process and analyze  streaming data in concert with contextual data to inform business-critical, operational decisions. See also SwimOS or Apache Flink.</p> </li> <li> <p>Command and control: sources are connected to key decision-making and action-taking services to complete a business task. So they are everywhere in any business. A typical example of such use case, is the Supervisory Control And Data Acquisition, used in manufacturing, or energy production. Try to ask: where does the organization depend on timely responses to changes in state? C&amp;C can be supported by centralized control with events come from multiple sources to stateless or stateful services, to apply real-time analysis and decision-making algorithms to those streams. Output events are published to sinks for future processing. Scaling with a centralized control approach is not straightforward, as getting the right events to the right processing instances can be a challenge. Also when we need the compute global aggregates by looking at the state of various agents in the systems is more complex, as it needs to integrate with stateful stream processing. Actions can be triggered by state changes, triggers that fire at specific times, or even API requests from other applications or services.</p> </li> </ul> <p>An alternate is to use distributed control, like applying the decision-making logic at the edge. </p> <ul> <li>Query and observability: querying or monitoring individual agents or specific groups of agents. The problem is to locate the right agent target of the query, and get current state or history from that agent. </li> <li>Telemetry and analytics: focuses on understanding systems behavior, and get real-time big data insights (e.g. Click streams).  Need to assess which insights require understanding the emerging behavior of a system of agents emitting vast amounts of data.</li> </ul> <p>Interesting presentations:</p> <ul> <li>Voxxed Athens 2018 - Eventing, Serverless, and the Extensible Enterprise by Clemens Vasters</li> </ul>"},{"location":"architecture/flow-architectures/#5-model-flow","title":"5- Model Flow","text":"<p>Use Event storming to build a timeline of events that are required to complete a complex task, and to get and understanding of the people, systems, commands and policies that affect the event flow.  The Event Storming process is a highly interactive endeavor that :brings subject matter experts  together to identify the flow of events in a process or system</p> <ol> <li>Identify the business activities that you would like to model in terms of event flow</li> <li>Begin by asking the group to identify the events that are interesting and/or required for that business activity</li> <li>Place events along a timeline from earliest action to latest action</li> <li> <p>Capture:</p> <ul> <li>The real-world influences on the flow, such as the human users or external systems that produce or consume events</li> <li>What commands may initiate an event</li> <li>What policies are activated when an event takes place. A policy usually initiates a new command. Events always result in a policy action unless the event is generating output.</li> <li>What are the outputs from the event flow.</li> </ul> </li> </ol> <p>When designing the solution assess:</p> <ul> <li>When the event is simply to broadcast facts for consumption by interested parties. The producer contract is  simply to promise to send events as soon as they are available.</li> <li>If consumers can come and go, and experiment with the consumption of a stream with little risk of consequences if they choose not to continue</li> <li>When event is part of an interaction around an intent, requiring a conversation with the consumer</li> <li>Is the event a stand-alone communication, discrete, or is it only useful in a context that includes a series of events.  Series applications are where log-based queueing shines</li> <li>Is the processing involve one simple action per event, or is there a group of related actions, a workflow, required to complete processing</li> </ul> <p>When building a flow-ready application for messaging, the \u201ctrunk-limb-branch-leaf\u201d pattern is a  critical tool to consider: use edge computing to distribute decision-making close to the related  groups of agents, computing local aggregates, and propagate to larger more central flows. Using messaging middleware to manage interaction between agents, to isolate message distribution to just the needed servers and agents, and propagate aggregates to the trunk, greatly reducing traffic between the original agents and the core.</p> <p>Another consideration is to assess if the consumers need to filter events from a unique topic before doing its own processing, in this case the event payload may include metadata and URL to get the payload. If the metadata indicates an action is required, the consumer can then call the data retrieval URL.</p> <p>Whether or not you include payload data depends a bit on the volume of events being published and the security and latency requirements of your application.</p> <p>Log-based queues can play the role of \u201csystem of record\u201d for both event values and sequence, especially for systems that need both the most recent event and an understanding of the recent history of events received</p> <p>For single action processing, serverless, knative eventing are technologies to consider. Solution needs to route events to the appropriate processor. But if your event processing needs require  maintaining accurate state for the elements sending events then stateful streaming platform are better fit.</p> <p>For workflow, modern solutions, simplify creating and managing process definitions independent of the actions taken in that process. It supports for stepping an event through multiple interdependent actions. Workflow may require to wait for another related event occurs or a human completes his action.</p>"},{"location":"architecture/flow-architectures/#6-today-landscape","title":"6- Today landscape","text":"<ul> <li>Standards are important for flow:  TLS, WebSockets, and HTTP from IETF, MQTT and AMQP from OASIS,  CloudEvents and the Serverless Working Group from CNCF</li> <li> <p>Open sources projects: </p> <ul> <li>Apache Kafka and Apache Pulse for log-based queueing</li> <li>Apache Beam, Flink, Heron, Nifi, Samza, and Storm for stream processing</li> <li>Apache Druid as a \u201cstream-native\u201d database</li> <li>gRPC may play a key role in any future flow interface</li> <li>NATS.io, a cloud-native messaging platform</li> <li>Argo, a Kubernetes-based workflow manager that theoretically could act as the core of an event-driven process automation bus</li> </ul> </li> <li> <p>Opportunities:</p> <ul> <li>Data provenance and security for payloads passed between disparate parties</li> <li>Tracking event data distribution across the world wide flow. Where does the data generated by an event end up being consumed or processed?</li> <li>Platforms for coordinating event processing, aggregation, and synchronization between core data center event processors, edge computing environments, and end-user or IoT devices</li> <li>Monetization mechanisms for all types of event and messaging streams</li> </ul> </li> </ul> <p>The adoption of a technology is not the delivery that makes it valuable, but the ecosystem that consumes it.</p> <p>Look at existing streams and determine how to add value for the consumers of that stream.  Can you automate valuable insights and analytics in real time for customers with shared needs?  Would it be possible to recast the stream in another format for an industry that is currently  using a different standard to consume that form of data? </p>"},{"location":"architecture/gcp/","title":"Google cloud platform","text":"<p>https://cloud.google.com/ - 24 regions, 73 zones - 200 countries.</p> <p>Ex: europe-west is a region, europe-west2-a, europe-west2-b, europe-west2-c are zones around london, so 3 data centers.</p> <p>Each data center is connected with fiber optical network and a private network on the same subnet. G. connects its network to the rest of the internet via Point of Presence.</p> <p>Cloud console</p> <p>A project is also used as a billing entity. You can define alerting rules as part of a budget on an account.</p> <ul> <li>VMs are compute engine</li> <li>Cloud run is for deploying containerized app.</li> <li>Function for event-driven serverless</li> </ul>"},{"location":"architecture/gcp/#tools","title":"Tools","text":"<ul> <li>Google cloud shell - it is a small VM with 5G disk. </li> <li>gcloud CLI to interact with the G. Cloud. (installed under ~/google-cloud-sdk). See install doc. </li> <li><code>gcloud components list</code></li> <li><code>gcloud components install componentid</code> for example </li> <li>SDK getting started</li> </ul>"},{"location":"architecture/gcp/#identify-access-management","title":"Identify &amp; Access Management","text":"<p>control who has access to what.</p> <ul> <li>Members (who): google account, group, service account for app and machine, GSuite domain, allAuthenticatedUsers (google accounts holders), allUsers (anyone on the internet)</li> <li>Roles (what): give access to things. PredefinedRoles, PrimitiveRoles, CustomRoles.</li> <li>Permissions</li> <li>IAM Policy hierarchy: define policy at the different level of the hierarchy: organization -&gt; project -&gt; resources. Resources are compute engine, app engine, cloud storage, pub/sub,...</li> </ul> <p>We can create custom role, to define specific, reusable, permissions.</p>"},{"location":"architecture/gcp/#compute-engine","title":"Compute engine","text":"<p>Some interesting characteristics:</p> <ul> <li>files are split in shrunk and encrypted at rest and distributed within cloud storage: so it is mostly impossible to rebuild a file as each shrunk has its own key</li> <li>VM can be preemptable (terminated after 24 or 10 first minutes) or committed (1 year).</li> <li>Public IP will change when stopping VM.</li> <li>Linux or windows OS, quite a lot of configurable parameters.</li> </ul>"},{"location":"architecture/gcp/#vpc-networking","title":"VPC networking","text":"<ul> <li>same flat optical fibers between data centers.</li> <li>G. network is connected to internet by multiple point of presence, via peering (interconnection between internet networks). 100 interconnections.</li> <li>Use global cache to get static content available for frequently access content.</li> <li>BBR - Bottleneck Bandwidth and RTT congestion control algorithm models the network to send as fast as the available bandwidth so it is 2700x faster than previous TCP</li> </ul> <p>The networking delivers a set of services * With VPC, you can define your subnets (100sb / VPC), public IP, own firewall, routes, peering and VPN. It is a global network. Each VM has 8 interfaces, on IPv4 unicast. No broadcast. * NW services for load balancer, DNS and CDN * Cloud interconnect to connect on-premise servers to cloud via VPN, dedicated connection and routers.</p>"},{"location":"architecture/ibmcloud/","title":"IBM Public Cloud","text":"<p>Bring elasticity to IT infrastructure, software services, data replication and app deployments. </p>"},{"location":"architecture/ibmcloud/#ibmcloud-cli-summary","title":"ibmcloud CLI summary","text":"<pre><code># login with sso\nibmcloud login --sso\n</code></pre>"},{"location":"architecture/ibmcloud/#vpc-virtual-private-cloud","title":"VPC - Virtual Private Cloud","text":"<p>Virtual Private Cloud (VPC) lets an enterprise establishes its own private cloud-like computing environment on shared public cloud infrastructure.</p> <p>A VPC\u2019s logical isolation is implemented using virtual network functions and security features that give an enterprise customer  granular control over which IP addresses.</p> <p>The resources can be compute (virtual server or vCPU), storage (block storage quota per account) and networking with public  gateways, load balancers, routers, direct or dedicated links.</p> <p>See this tutorial to understand how VPCs are created. By default three subnets are created, and admin user needs to configure ACL to limit inbound and outbound traffic to the subnet. ACL can be setup to authorize TCP traffic on port 80 on any IP, deny all other protocol on any IP and any port...</p> <p>Once done we can add virtual server instance. A boot volume is allocated, ssh key needs to be created and different linux images are supported. A VSI is attached to a subnet via its network interface. We can also select which security groups to attach to each interface. Block storage can be added as volume and added to a VSI. We can configure the security group to define the inbound and outbound traffic that is allowed for the instance.</p> <p>If we want your instance to be reachable from the internet, we need to get floating IP address. </p> <p>A typical 3 tiers application can have each tier allocated on its own subnet, with its own IP range.</p>"},{"location":"architecture/ibmcloud/#vpe-virtual-private-endpoints","title":"VPE Virtual Private Endpoints","text":"<p>VPEs are virtual IP interfaces that are bound to an endpoint gateway created on a per service, or service instance.</p> <p>VPE has multiple benefits like:</p> <ul> <li>Public connectivity is not required and has no public data egress charges.</li> <li>Reaches IBM Cloud assets through a private service provider.</li> <li>A VPE lives in your network address space, extending your private and multicloud into the IBM Cloud.</li> <li>You can apply security through Network Access Control Lists (NACLs).</li> <li>The endpoint IP deploys in a customer-defined, virtual network.</li> </ul> <p>Tutorial: Leveraging Virtual Private Endpoint in IBM Cloud VPC to Connect IBM Cloud Object Storage to present how to share the data across availability zones in VPC and DR site using IBM Cloud object storage. The buckets are mounted inside VSIs from source site to DR AZ:</p> <p></p>"},{"location":"architecture/ibmcloud/#cloud-object-storage","title":"Cloud Object Storage","text":"<p>Very similar AWS S3 service. The product doc.</p> <ul> <li>Create a COS service to keep n buckets. </li> <li>IBM Cloud Object Storage is a multi-tenant system, and all instances of Object Storage share physical infrastructure</li> <li>Bucket is like a unique folder to content files</li> <li>Stores encrypted and dispersed data across multiple geographic locations</li> <li> <p>Different level of resiliency. Resiliency refers to the scope and scale of the geographic area across which your data is distributed.</p> <ul> <li>Cross Region resiliency spreads your data across several metropolitan areas</li> <li>Regional resiliency spreads data across a single metropolitan area.</li> <li>A Single Data Center distributes data across devices within a single site only.</li> </ul> </li> <li> <p>Billing based on storage class, which reflect how to read and write data. We can transition from any of the storage tiers (Standard, Vault, Cold Vault and Flex) to long-term offline archive.</p> </li> <li>Different roles can be used to access the object.</li> <li>SQL Query can be used on top of COS objects</li> </ul>"},{"location":"architecture/ibmcloud/#use-cases","title":"Use cases","text":"<ul> <li>as part of a data lake to keep a lot of data</li> <li>serving static websites</li> </ul>"},{"location":"architecture/ibmcloud/#ibm-cloud-for-vmware","title":"IBM Cloud for VMware","text":"<p>Two models to bring your own VMs.</p> <ul> <li>Shared: Pay per use. Mostly used Development and DR</li> <li>Dedicated</li> </ul> <p>Use cases:</p> <ul> <li>cost take our and quick consolidation: maximize existing licensing, immediate ROI versus running on premise and other clouds (37% TCO saving)</li> <li>DR, automated Dev+Test pipeline on a consistent VMware hypervisor platform, spin-up hourly on demand for resiliency</li> <li>Regulators mount up: FS cloud</li> </ul>"},{"location":"architecture/ibmcloud/#function-as-a-service","title":"Function As A Service","text":"<p>Function as Service is the current service for 'serverless'.</p> <ul> <li>pay for the time code actually runs, which means no excess capacity or idle time. </li> <li>scales to fit exact demand, from once a day to thousands of parallel requests per second</li> <li>use the concept of namespace to group related functions</li> <li>Actions are function as code to perform a task. You provide your action to Cloud Functions either as source code or a Docker image.</li> <li>A sequence is a chain of actions, invoked in order, where the output of one becomes the input to the next. This allows you to combine existing actions together for quick and easy re-use. A sequence can then be invoked just like an action, through a REST API or automated in a rule.</li> <li>A trigger is a declaration that you want to react to a certain type of event, whether from a user or by an event source. Triggers fire when they receive an event. Events are always occurring, and it's your choice on how to respond to them.</li> </ul> <p>Nice tutorial for a static web app accessing FaaS and Cloudant. </p> <p></p> <ul> <li>Deploy a Cloudant database</li> <li>Define actions and sequences of actions in the Function as service:</li> <li>Define APIs and API gateway to expose the action sequences as external public route</li> <li>Deploy static web page with HTML et js to access the exposed APIs</li> </ul> <p>## Cloud Engine</p>"},{"location":"architecture/integration-ra/","title":"Integration reference architecture notes","text":"<p>See the agile integration concepts in EDA site.</p>"},{"location":"architecture/integration-ra/#how-resilience-is-supported","title":"How resilience is supported?","text":""},{"location":"architecture/integration-ra/#ibm-reference-architecture","title":"IBM Reference architecture","text":"<p>IBM integration reference</p>"},{"location":"architecture/integration-ra/#_1","title":"Integration reference architecture notes","text":""},{"location":"architecture/opendatahub/","title":"Open Data Hub Studies","text":"<p>Open Data Hub main site</p>"},{"location":"architecture/opendatahub/#summary","title":"Summary","text":"<p>The goal of Open Data Hub is to provide open source AI tools for running large and distributed AI workloads on OpenShift Container platform.  While AI Library is to provide ML models as a service on OpenShift.</p> <p>In general, an AI workflow includes most of the steps shown in figure below:</p> <p></p> <p>For data storage and availability, ODH provides Ceph, with multi protocol support including block,  file and S3 object API support, both for persistent storage within the containers and as a scalable  object storage data lake that AI applications can store and access data from.</p> <p>Notes</p> <pre><code>[Ceph](https://docs.ceph.com/docs/master/start/intro/) delivers a self-managed, self-scaling, and self-healing storage infrastructure using storage cluster. A key Ceph architectural tenet is to have no single point of failure (SPoF) in the system.\n</code></pre>"},{"location":"architecture/opendatahub/#installation-on-openshift","title":"Installation on Openshift","text":"<p>The latest version of the Open Data Hub operator project is located here: https://gitlab.com/opendatahub/opendatahub-operator</p> <p>Install Ceph with the rook operator  using these instructions.</p> <p>The following github has the configurations: https://github.com/rook</p> <p>To validate pods </p> <pre><code>oc get pods -n rook-ceph-system\noc get pods -n rook-ceph\n</code></pre>"},{"location":"architecture/satellite/","title":"IBM Cloud Satellite","text":"<p>Quick Product Overview.</p>"},{"location":"architecture/satellite/#goal","title":"Goal","text":"<p>The main goal for Satellite is to run workloads  where it makes the most sense. Based on kubernetes, it is a API driven  cloud services to achieve consistent application deployment and performance  across any environments (own data center, cloud or edge location).</p> <p>Based on the concept of location that groups your data center and IBM cloud. </p> <ul> <li>Expend the reach of IBM Cloud (management of services, and user experience) to infrastructure outside of IBM data centers.</li> <li>Bring the IBM cloud services to your data center</li> <li>Bring consistency of services and content, and security: bring your TLS certificates</li> <li>Use one control plane on IBM Cloud data center. Run day 2 operations on the same tools on IBM Cloud. </li> <li>Consider them as mini IBM cloud regions</li> <li>It gives us a central view of all our k8s / OCP clusters</li> </ul>"},{"location":"architecture/satellite/#challenges-to-address","title":"Challenges to address:","text":"<ul> <li>Latency: data processing close to te data</li> <li>Data residency: stay in country</li> <li>Lack of agility: app run across many locations</li> <li>Lack of visibility: ops operate 5 to 8 clouds and need visibility for platform and apps</li> <li>Local management</li> <li>Lack of version control</li> <li>Inconsistent talent availability</li> <li>Inability to instantly customize</li> </ul> <p>K8s: maintenance is quickly becoming difficult.</p>"},{"location":"architecture/satellite/#features","title":"Features","text":"<ul> <li>Use a single API to create a Satellite location and add host machines from your on-prem data center, edge devices, or other cloud providers. Then, the hosts power IBM Cloud services, such as IBM Cloud Databases or OpenShift clusters, anywhere you need them.</li> <li>Consistent app deployments: Create, version control, update, and review your app configurations across all your OpenShift clusters from a single pane.</li> <li>Control and audit the network traffic and communication flow between your OpenShift clusters with a built-in, application-level firewall, and use IBM Cloud monitoring and logging services to consolidate your logs, metrics, and alerts</li> </ul>"},{"location":"architecture/satellite/#use-cases","title":"Use cases:","text":"<ul> <li>consume cloud services butt use your data locally</li> <li>location to install small box, considered as hosts</li> <li>flexible infrastructure</li> <li>as a service consumption: Cloud database, openshift, CP4Data </li> <li>do not need to manage those services</li> </ul>"},{"location":"architecture/satellite/#installation-on-azure","title":"Installation on Azure","text":"<p>Provision enough nodes at the start.</p> <ol> <li>Create a location: Get minimum 9 MultiZone nodes: 3 control plane, 6 worker nodes </li> <li>Create a cluster: For example OCP 4.7 with three 16x64 work. </li> <li>Update Azure: Add public IP to one or more VMs. Create disk volumes for each worker done. Attach to the nodes</li> <li>Install OpenShift Data Foundation from CLI. See Satellite doc on setting up storage</li> <li> <p>Install as usual</p> </li> <li> <p>Video from support and training: \"How to configure log-in to a Satellite Cluster\"</p> </li> <li>Video from support and training: \"How to configure storage for a Satellite Cluster\"</li> </ol>"},{"location":"architecture/streaming/","title":"Stream processing","text":"<p>This article is a summary of stream processing with links to concrete studies of specific technologies. There is a need to understand the problems related to data processing in distributed computing.</p>"},{"location":"architecture/streaming/#concepts","title":"Concepts","text":"<ul> <li>Stream processing is about processing every event as soon it happens.</li> <li>In reality, a lot of data is unbounded because it arrives gradually over time</li> <li>The problem with daily batch processes is that changes in the input are only reflected in the output a day later</li> <li>In batch, the input and outputs are files. In stream processing a record is the event: a self contained, immutable object, representing what happened at some point in time.</li> <li>Event is produced once, and processed by multiple consumers.</li> <li>Anumber of messaging systems use direct network communication between producers and consumers without going via intermediary nodes: brokerless like ZeroMQ, nanomsg</li> </ul>"},{"location":"architecture/streaming/#challenges","title":"Challenges","text":"<ul> <li>Using a database to keep events may work for small scale, but polling becomes expensive. Notifying consumers is one way to solve this problem, but then it means using subscription. DBs were not designed for notifications. Triggers may help but do not scale.</li> <li>Messaging system is the technology to push event to consumers. The pub/sub model support multile consumers.</li> </ul>"},{"location":"architecture/streaming/#questions-to-assess","title":"Questions to assess","text":"<ul> <li> <p>What happens if the producers send messages faster than the consumers can process them?</p> <ul> <li>drop messages</li> <li>buffer messages - (see later section on append log)</li> <li>apply backpressure, or flow control to slow down the producer</li> </ul> </li> <li> <p>What happens if nodes crash or temporarily go offline \u2014 are any messages lost?</p> <ul> <li>when losing messages the throughput is higher and even the latency is lower </li> <li>if you are counting events, it is more important that they are delivered reliably, since every lost message means incorrect counters.</li> </ul> </li> </ul>"},{"location":"architecture/streaming/#high-level-technology-concepts","title":"High level technology concepts","text":""},{"location":"architecture/streaming/#message-brokers","title":"Message brokers","text":"<ul> <li>Message broker is essentially a kind of database that is optimized for handling message streams. It runs as a server, with producers and consumers connecting to it as clients. Consumers can connect at any time, and reconnect.  Producer does not wait for consumer to process the messages. Just get acknowledge the brokers got them.</li> <li>Databases usually keep data until it is explicitly deleted, whereas most message brokers automatically delete a message when it has been successfully delivered to its consumers. Such message brokers are not suitable for long-term data storage.</li> <li>Message may have filtering capability using pattern. They do not support arbitrary queries or indexing.</li> <li> <p>Two main patterns of message delivery (and they can be combined too):</p> <ul> <li>Load balancing: Each message is delivered to one of the consumers, so the consumers can share the work of processing the messages in the topic. </li> <li>Fan-out: Each message is delivered to all of the consumers.</li> </ul> </li> <li> <p>To ensure messages are not lost, acknowledge is used by consumers to signal end of the message processing.</p> </li> <li>In case of not receiving an ack on time, the broker could resend the message to another consumer. when using load balancing, this means message order can be lost. A consumer B crashes while processing message m10, while consumer A is processing M11, if broker reallocate m10 to A then m10 will be done after m11. To avoid that we need one queue per consumer. (one partition per consumer in Kafka)</li> </ul>"},{"location":"architecture/streaming/#partitioned-logs","title":"Partitioned logs","text":"<p>With AMQP/JMS-style messaging: receiving a message is destructive if the acknowledgment causes it to be deleted from the broker, so you cannot run the same consumer again and expect to get the same result.</p> <p>A log is simply an append-only sequence of records on disk. And it can be used to implement a message broker: producer append to the log, consumer read sequentially.</p> <p>In order to scale to higher throughput than a single disk can offer, the log can be partitioned.</p> <p>Log-based message broker support fan-out, and for load blaancing they assign one partitions to nodes with consumer group.</p> <p></p> <p>In situations where messages may be expensive to process and you want to parallelize processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable.</p> <p>For high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.</p> <p>With offset numbering schema, brokers know that all messages with an offset less than a consumer\u2019s current offset have already been processed. But they do not to track ack at the single message, they need to get at a period or during a batch messaging ack.</p> <p>Without more control, it means also that messages can be processed twice in case of consuner crashes and partition reallocation or reassignment: non ack messages are resent.</p> <p>To avoid run out of disk space, the log can be circular so older messages are removed. For a 5.5TB harddisk with a write thoughput of 150MB/s it will take 10h to fill the log. </p> <p>The throughput is quite constant as it needs a disk write. While messaging systems that keep messages in memory by default and only write messages to disk if the queue grows too large, are slowing when they start writing to disk, and throughput depends on the history to retain. </p> <p>When consumer lag time is higher than retention time, then consumer will not process messages, they are lost. This is important to monitor lag and act before too late.</p> <p>Read offset is under the consumer control, and it can be positioned where needed. Log-based messaging looks like batch processing where derived data is clearly separated from input data.</p>"},{"location":"architecture/streaming/#cdc-and-log-based-message-broker","title":"CDC and log-based message broker","text":"<p>A log-based message broker is well suited for transporting the change events from the source database to the derived systems, since it preserves the ordering of messages.</p> <p>To avoid spending a lot of time to reconstruct the state of the data change, or when you do not have the full history of change, it is relevant to start from a snapshot.</p> <p>New databases are supporting change streams interface, so queries can subscribe to notifications when the results of a query change *see CouchDB, MongoDB oplog).</p>"},{"location":"architecture/streaming/#event-sourcing","title":"Event sourcing","text":"<p>Event sourcing involves storing all changes to the application state as a log of change events. Events are designed to reflect things that happened at the application level, rather than low-level state changes. Event sourcing makes it easier to evolve applications over time.</p> <p>Applications that use event sourcing need to take the log of events (representing the data written to the system) and transform it into application state that is suitable for showing to a user.</p> <p>For performance improvement, they have some mechanism for storing snapshots of the current state that is derived from the log of events, so they don\u2019t need to repeatedly reprocess the full log.</p> <p>When a request from a user first arrives, it is initially a command: at this point it may still fail, for example because some integrity condition is violated. The application must first validate that it can execute the command. If the validation is successful and the command is accepted, it becomes an event, which is durable and immutable.</p> <p>A consumer of the event stream is not allowed to reject an event: by the time the consumer sees the event, it is already an immutable part of the log, and it may have already been seen by other consumers. Thus, any validation of a command needs to happen synchronously, before it becomes an event\u2014for example, by using a serializable transaction that atomically validates the command and publishes the event. Alternatively, the user request to reserve a seat could be split into two events: first a tentative reservation, and then a separate confirmation event once the reservation has been validated.</p> <p>Using immutable event log, we can build several different read-oriented view of the data. Having an explicit translation step from an event log to a database makes it easier to evolve your application over time.</p>"},{"location":"architecture/system-design/","title":"Conduct system design","text":""},{"location":"architecture/system-design/#methodology","title":"Methodology","text":"<ul> <li> <p>Verify the goals</p> <ul> <li>Why we are doing this application / solution?</li> <li>Who is the end user?</li> <li>How they will use the system and for what purpose?</li> <li>What are the expected outcomes and what are the inputs?</li> <li>How to measure success?</li> </ul> </li> <li> <p>Think about end user's experience - working backward</p> </li> <li> <p>Establish scope</p> <ul> <li>list some potential features</li> <li>Are we looking at end to end experience or API design</li> <li>type of client application or device</li> <li>Do we require authentication? Analytics? Integrating with existing systems?</li> </ul> </li> <li> <p>Design according to scale</p> <ul> <li>What is the expected read-to-write ratio?</li> <li>what volumes in term of transactions/ requests </li> <li>How many concurrent requests should we expect?</li> <li>What\u2019s the average expected response time?</li> <li>What\u2019s the limit of the data we allow users to provide?</li> <li>Do we need to expose read data in different geographies?</li> </ul> </li> <li> <p>Defining latency requirements</p> <ul> <li>how fast is fast enough?</li> <li>Try to express in SLA language: 100ms at 3-nines - 99.9% of availability is around 9h down per year. 5 nines is 5 minutes down.</li> </ul> </li> </ul> Information <ul> <li>AWS offers TCP connections up to the edge and then use AWS private backbone (global accelerator), which improves performance by 60% </li> <li>Try to use Real User Monitoring tool to measure network performance</li> <li> <p>Try to measure: </p> <ul> <li>Throughput \u2013 the amount of data or number of data packets that can be delivered in a predefined timeframe, </li> <li>Latency in connection \u2013 also called round-trip times or RTT, </li> <li>Network jitter \u2013 the variability over time of the network latency, and </li> <li>Packet loss. Get 1,000 samples every hour for a day.</li> </ul> </li> <li> <p>Apache Bench helps to test throughput and latency of HTTP servers</p> </li> <li>Round-Trip Time (RTT) is the total time that it takes a data packet to travel from one point to another on the network and for a response to be sent back to the source. It is a key perf metric for latency. Ping measure ICMP RTT.</li> </ul> <ul> <li> <p>From high level to drill down</p> <ul> <li>Start by high level view</li> <li>Cover end to end process</li> <li>Go when necessary to detail of an API, data stores,..</li> <li>Look at potential performance bottle neck</li> </ul> </li> <li> <p>Review data structure and algorithms to support distributed system and scaling</p> </li> <li> <p>Be able to argument around</p> <ul> <li>What type of database would you use and why?</li> <li>What caching solutions are out there? Which would you choose and why?</li> <li>What frameworks can we use as infrastructure in your ecosystem of choice?</li> </ul> </li> <li> <p>Do not be defending</p> </li> <li> <p>Demonstrate perceverance - determination: internal drive to search for a solution - collaboratively. </p> <ul> <li>Behavioral interviewing, tell the stories when you demonstrate perceverance.</li> <li>Independent thought: getting your solution from your own experience</li> <li>Independent learning</li> <li>Never give up and never surrender</li> </ul> </li> <li> <p>Tech skills matter but they are just table stakes</p> </li> <li>Demonstration self motivated: bring with your own. </li> <li>Do not be the guy \"let me google for you!\"</li> <li>They do not want to see people following step by step instructions/ recipes, because it demonstrates you cannot solve new problem</li> <li>Work as no value until demonstrate to customers</li> </ul>"},{"location":"architecture/system-design/#scalability","title":"Scalability","text":""},{"location":"architecture/system-design/#single-server-design","title":"Single server design","text":"<ul> <li>Unique server with HTTP and Database on a unique server. Simple but has a single point of failure: impact is to change DNS routing with new server.</li> <li>Separate out the DB help to scale DB and server independently. </li> </ul>"},{"location":"architecture/system-design/#vertical-scaling","title":"Vertical scaling","text":"<p>Use bigger server. Get a limit by hardware size. Still single point of failure. Pros is the limited number of things to maintain.</p>"},{"location":"architecture/system-design/#horizontal-scaling","title":"Horizontal scaling","text":"<p>Load balancer sends traffic to a farm of servers. This is easier if the web server is stateless, that means we do not keep state of the conversation. Any server can get request at any time.</p> <p></p> <p>A master database generally only supports write operations. A slave database gets copies of the data from the master database and supports read operations.</p> <p>By replicating data across different locations, the website remains in operation even if a database is offline as it can access data stored in another database server.</p> <p>This is important to remember that user session needs to be kept, for example to avoid re-authenticate if the new request reaches another web server. Stateful web tier is not encouraged for scalability. So stateless architecture is used, where HTTP requests from users can be sent to any web servers, which fetch state data from a shared data store. The shared data store could be a relational database, Memcached/Redis, NoSQL,... </p>"},{"location":"architecture/system-design/#failover","title":"Failover","text":"<p>See also DR article and data replication blog.</p> <p>To access transparently server on two different data centers, we use geoDNS, which is a DNS service that allows domain names to be resolved to IP addresses based on the location of a user. In the event of any significant data center outage, we direct all traffic to a healthy data center.</p> <p>Important challenges to consider are:</p> <ul> <li>Traffic redirection: Effective tools are needed to direct traffic to the correct data center.</li> <li>Data synchronization: Users from different regions could use different local database or cache. In failover cases, traffic might be routed to a data center where data is unavailable.</li> </ul>"},{"location":"architecture/system-design/#cold-standby","title":"Cold standby","text":"<ul> <li>periodic backup</li> <li>restore backup on DR site</li> <li>reconnect front end server to new DB server</li> <li>Data gone after the database backup is lost</li> <li>Take time to get the new server up and running.</li> <li>RPO can be day (snapshot frequency) - RTO hours</li> </ul>"},{"location":"architecture/system-design/#warm-standby","title":"Warm Standby","text":"<ul> <li>Continuous replicated: the DB is ready to get connected</li> <li>Tiny window to get data loss</li> <li>Still using vertical scaling </li> </ul>"},{"location":"architecture/system-design/#hot-standby","title":"Hot Standby","text":"<ul> <li>Write to both servers simultanuously </li> <li>Can distribute the read</li> </ul>"},{"location":"architecture/system-design/#sharding-database","title":"Sharding database","text":"<ul> <li>Horizontal partition of the database</li> <li>Each shard has its own replicated backup</li> <li>Hashcode is used to allocate data to shard</li> <li>Combining data from shards is more complex. So need to minimize joins and complex SQL</li> <li>Organize data in key value, to easy the hashing. </li> <li>Value can be an object and let the client being able to interpret.</li> </ul> <p>For example MongoDB uses <code>mongos</code> on each app server to distribute the data among a replica set. Replica sets are managed by primary server and secondary servers manage shards. In case of primary server fails, the secondary servers will elect a new primary. Primary looks like a SPOF, but it recovers quickly via the secondary taking the lead. Need at least 3 servers to elect a primary. Traffic is partitioning according to a scheme, which is saved in a config servers. </p> <p>Cassandra uses node rings, a shard is replicated multiple times to other nodes, but each node is a primary of a shard. So data needs to be fully replicated, and eventually will be consistent. </p> <p>Resharding is a challenge for the database.</p> <p>NoSQL really means sharded database, as some DB can support most of SQL operations.  </p> <p>Need to address how to partition the raw data for best performance. For example organize the bucket or folder structure to simplify future queries: organize them by date, or entity key...</p>"},{"location":"architecture/system-design/#de-normalizing","title":"De-normalizing","text":"<p>We normalize the data to use less storage and updates in one place. But need more lookups.</p> <p>De-normalize duplicates data, use more storage, but uses one lookup to get the data, which leads to have  harder update.</p> <p>To assess what is a better fit, start with normalize, as we need to think about the customer experience, and consider different types of query.</p>"},{"location":"architecture/system-design/#data-lake","title":"Data lake","text":"<p>Throw data into text files (json, csv...) into big distributed storage system like S3. Which his named data lake. It is used in common problem like Big Data and unstructured data. </p> <p>We can also query those data by adding an intermediate components to create schema from the data and support queries. (Amazon Glue like a schema discovery) and Amazon Athena to support queries and Redshift to do distributed warehouse with spectrum to query on top of s3.</p> Product Description AWS S3 Service. data lake with s3. 11 9's% for durability. Pay as you go. Offers different level of backup with Glacier Google Cloud Storage introduction. Use hierarchy like: organization -&gt; project -&gt; bucket -&gt; object. Tutorial IBM Cloud Object Storage doc Azure Blob doc"},{"location":"architecture/system-design/#cap-acid","title":"CAP &amp; ACID","text":"<ul> <li>Atomicity: either the entire transaction succeeds or the entire thing fails</li> <li>Consistency: All database rules are enforced, or the entire tx is rolled back. Consistency outside of ACID is really to address how quickly we get the data eventually consistent after a write. </li> <li>Isolation: no tx is affected by any other tx that is still in progress.</li> <li>Durability: once a tx is committed, it stays, event if the system crashes.</li> </ul> <p>CAP theorem: We can have only 2 of the 3: Consistency, Availability and Partition tolerance. With enhanced progress, CAP is becoming weaker, but still applies. A is really looking at single point of failure when something going down. MongoDB for example may loose A for a few seconds maximum (find a new primary leader), which may be fine.</p> <ul> <li>AC: is supported by classical DBs like mySQL, postgresql</li> <li>AP: Cassandra: C is lost because of the time to replicate</li> <li>CP: Mongodb, HBASE, dynamoDB: strong consistent read request, it returns a response with the most up-to-date data that reflects updates by all prior related write operations to which DynamoDB returned a successful response, so network delay or outtage does no guaranty A.</li> </ul> <p>Single-master designs favor consistency and partition tolerance.</p>"},{"location":"architecture/system-design/#caching","title":"Caching","text":"<p>Goal: limit the access to disk to get data, or go over the network.</p> <p>Solution is to add a cache layer in front of the DB to keep the most asked data, or the most recent... Caching services can be used to be able to scale horizontally.</p> <p>Every cache server is managing a partition of the data, using hashing.</p> <p>Appropriate for applications with more reads than writes. Expiration policies dictate how long data stays in cache. Avoid data go stale.</p> <p>Hotspot may bring challenge for cache efficiency, need to cache also on load distribution and not just on hash. Finally starting the cache is also a challenge, as all requests will go to the DB.</p> <p>Inconsistency can happen because data-modifying operations on the data store and cache are not in a single transaction. When scaling across multiple regions, maintaining consistency between the data store and cache is challenging.</p> <p>Manage the cache size with different eviction policies:</p> <ul> <li>LRU: least recently used. HashMap for key and then doubly linked-list, head points to MRU and tail points to the LRU. Evicts data that hasn't been accessed in the longest amount of time once memory for the cache fills up.</li> <li>LFU: least frequently used. </li> <li>FIFO</li> </ul> <p>Redis, Memcached, ehcache. AWS Elasticache</p>"},{"location":"architecture/system-design/#content-delivery-networks","title":"Content Delivery Networks","text":"<p>when a user visits a website, a CDN server closest to the user will deliver static content.  CDN distributes read data geographically (css, images, js, html...), can even being used for ML model execution.</p> <p>Load balancers and caching technologies such as Redis can also be parts of low-latency designs, but are not specifically for the problem of global traffic.</p> <p>CDNs may run by third-party providers, and you are charged for data transfers in and out of the CDN.</p> <p>AWS cloudfront is a CDN.</p>"},{"location":"architecture/system-design/#resiliency","title":"Resiliency","text":"<p>Assess what could happen is a blade/ server, a rack, an AZ, a data center, a region goes down.  Mission critical applications should not loose data. Use 3 tiers app classification schemas.</p> <p>Use Geo-location load balancer then geo LB. </p> <p>Need to plan for capacity to be able to get traffic from failed region to 'backup region'.</p> <p>Secondary replicas should be spread to different servers, racks and then data centers. </p> <p>Balance budget over availability. Amazon with infinite money does over-provisioning. </p>"},{"location":"architecture/system-design/#hdfs","title":"HDFS","text":"<p>Files are broken into blocks. Blocks are replicated within the cluster. Replicas are rack aware. </p> <p>Clients try to read from nearest replica.</p> <p>The Name node coordinate the blocks placement operations. For HA the name nodes is a 3 nodes cluster, so a single point of failure for a very short time period.</p> <p>If a client app is running in the same server as HDFS the data it accesses may be moved to it.</p>"},{"location":"architecture/system-design/#sla","title":"SLA","text":"<ul> <li>Durability: % chance of losing data</li> <li>Latency to get the time for a service to return a response to a request. 99.9% response time is under 100ms</li> <li>99.9999% availability is 30 s down time. 99% is 3.45 days out</li> </ul>"},{"location":"architecture/system-design/#big-data","title":"Big Data","text":""},{"location":"architecture/system-design/#apache-spark","title":"Apache Spark","text":"<p>Goal: parallelize processing on big amount of data.</p> <p>On classical HADOOP 2 - architecture for big data: </p> <ul> <li>HDFS to distribute data</li> <li>Yarn (yet another resource negotiator) to manage access to the data in HDFS</li> <li>MapReduce processing (old google - map to extract data and reduce to combine for aggregation) or Spark.</li> </ul> <p>Spark is a replacement of MapReduce. It decides how to send processing to run it in parallel. Work with in memory caching. Compute aggregation on data at rest. You can use it for interactive queries with Spark SQL. </p> <p>The drive program (or SparkContext) is the one who define what are the input, output, and the processing to do. They are scheduled by Spark to run within a cluster. SparkContext sends app code to executors.</p>"},{"location":"architecture/system-design/#flink","title":"Flink","text":"<p>Scalable streaming platform for inbound and outbound data. See dedicated study</p>"},{"location":"architecture/system-design/#cloud-computing-services","title":"Cloud computing services","text":"AWS Google Azure Storage s3 cloud storage Disk, blob, data lake Compute EC2 Compute engine VM NoSQL DynamoDB BigTable CosmosDB / Table Storage Containers Kubernetes / ECR / ECS Kubernetes Kubernetes Data streams Kinesis DataFlow Stream Analytics Spark / Hadoop EMR Dataproc Databricks Data warehouse RedShift BigQuery Azure SQL / Database Caching ElastiCache (Redis) Memorystore (Redis or memcached) Redis"},{"location":"architecture/system-design/#mock-system-design-interviews","title":"Mock system design interviews","text":""},{"location":"architecture/system-design/#url-shortening-service","title":"URL shortening service","text":"<ul> <li>Talking about bit.ly: a service where anyone can shorten an existing URL and then the service is managing redirecting the traffic.</li> <li>what sort of scale? </li> <li>Any restriction on chars to be used?</li> <li>How short is short?</li> </ul> <p>The approach is to design the potential API, then the components, present a data model for persistence and then address the redirect.</p> Verb APIs POST New long url, user-id returns short url and status POST Propose vanity long url, vanity URL, user-id returns status GET mapping between long and short PATCH update:  short URL, long URL, user id, returns status DELETE short URL, user id return status GET short URL, redirect to long URL <p></p> <p>To perform redirect, the HTTP response status may be 301 or 302. 301 is a permanent redirect, so intermediate components will keep the mapping. But if you want to keep analytics (search engine) on your web site traffic, you want to know how many redirects are done per day, therefore 302 is a better choice. This is a temporary redirect.</p>"},{"location":"architecture/system-design/#a-restaurant-system-like-opentable","title":"A restaurant system like OpenTable","text":"<p>As customer I want to:</p> <ul> <li>specify the number of members for the dining party.</li> <li>select a time slot and specify a location.</li> <li>select a restaurant by name.</li> <li>select a type of food.</li> <li>book the table for the selected time slot, by specifying phone number and email address.</li> <li>specify how to be contacted when closer to the time (sms).</li> <li>register as a recurring user to get royalty points.</li> </ul> <p>Other customers of this application will be restaurant owners, who want to see their booking, but also get more information about the customers, the forecast, and the number of search not leading to their restaurant. May be expose their menu. Get statistic on customer navigation into menu items.</p> <p>NFR: thousands of restaurants, national, millions of users.</p> <p>Need to scale and be reliable. Cost is not an issue.</p> <p>Data model: </p> <ul> <li>search: location, time - date, party size, type of food.</li> <li>list of restaurants.</li> <li>list of time slots.</li> <li>reservation: restaurant, party size, time slot, email, phone number, type of contact.</li> <li>return booking status with reservation ID to be able to cancel it in the future.</li> </ul> <p>So we need: </p> <ul> <li>Restaurant to describe physical location, # tables.., how they can group tables for bigger party.</li> <li>Customer representing physical contact.</li> <li>A schedule per 15 mn time slots, table list per restaurant.</li> <li>Reservation to link customer to restaurant by using their primary keys.</li> <li>A Search: time slot, party, location.</li> <li>The reservation function need to take into account capability of the restaurant on how to organize tables.</li> </ul> <p>This is a normalized data model, as it is easier and really for reservation, there is one transaction on a simple object. Restaurant and customer are read models in  this context.</p> <p>Outside of the authentication, lost password APIs... we need few APIs to support the reservation:</p> Verb APIs GET search with query? location, date, party POST reservation DELETE reservation <p>Other APIs may be needed to do CRUD for each business entities: restaurants</p> <p>The servers are scaled horizontally, rack and AZ allocated, and even geo-routed.</p> <p></p> <p>DB system may be no sql based, and can be highly available and support horizontal scaling too.</p>"},{"location":"architecture/system-design/#web-crawler-for-a-search","title":"Web Crawler for a search","text":"<p>Problem: build a web crawler tool to prepare data for semantic search:</p> <p>Need to parse html, keep text only, may be pictures in the future, billions of web pages, and run every week. Need to update of existing page refs if they changed. </p> <p>HTML = text + ref to URL</p> <p>Singleton web crawler will put links into a LIFO pile. If we consider the web crawling as a graph navigation, we want to do BFS, so we need a pile as data structure.</p> <p>As we need to update crawled content every week, we can, in fact, run all the time with a single instance of the crawler. The solution uses batch, as no end-user uses the system. Except that, we still need to provide metrics to display coverage: pile size, total number of page processed, link occurrence...</p> <p>Need to avoid looping into pages, so need to keep the visited pages as well. If we need to run in parallel, then pile and visited page tree will be shared and distributed. So we need a partition key, may be a hash of the url.</p> <p>The solution may look like in the figure below:</p> <p></p> <ul> <li>Web Crawler using HTML parser and text extraction</li> <li>Save in a distributed cloud storage the source page, and pictures</li> <li>Extract the URLs and put them in a Pile data structure. Hash the URL. Filter out URLs already processed. Add the new URLs in the Queue</li> </ul>"},{"location":"architecture/system-design/#top-sellers-in-e-commerce","title":"Top sellers in e-commerce","text":"<p>Present the top sellers per category / sub-category. We can have thousand of request per second.</p> <ul> <li>top seller means trending: so # of sell over a time period.</li> <li>time period is variable for low sell item, so item sold few time per year, needs to be visible for a category. We can also use a weight algorithm to put less impact on older sells.</li> </ul> <p></p> <p>The update can be done few time a day for most active category.</p> <p>The new customer experience includes: after searching and looking at a category of items, the system returns a list of top seller under the product description.</p> <p>Sell Object is a product ID, category, date of sell.</p> <p>Batch processing computes the top sellers per category, and saves it in distributed cache. </p> <p>The amount of data is massive so querying per category and sort by date of sell will put stress to any SQL database. We can replicate the DB and work on the data warehouse, or adopt cloud object storage bucket with category being the bucket. For the job processing, we could use Flink or Spark, to compute the top-sell product. </p> <p>Job will run in parallel and flink will distribute data. It will compute for each product a score based in (t - purchase time) time, decaying older sells. We sort by this score.</p> <p>The top-seller data store does not need to support a big amount of data, may be keep the top 20 or 50 items per category, and may be hundred of thousand of category. Distributed document oriented Database can be used.</p> <p>To support scaling and low latency at the web page level, we need distributed caching, and scale web server horizontally. Here is a component view:</p> <p></p>"},{"location":"architecture/system-design/#video-sharing-service","title":"Video sharing service","text":"<p>Some thing like youtube.</p> <ul> <li>anybody in the work can upload video, and anybody can view it.</li> <li>massive scale.</li> <li>Users and videos in billions .</li> <li>Video upload and playing back those videos.</li> </ul> <p>Feel free to use existing services.</p> <p>Customer centric design:</p> <ul> <li> <p>Watching user: video search, video metadata and video player </p> <ul> <li>Need a web server returning video URL and metadata: name, url, length, author, thumbnail picture,</li> <li>Big table to keep metadata. Key value pair. Easy to replicate and cache in different geographies</li> <li>Video needs to be close to user, like a CDN, with transcoded videos. Need distributed storage, and able to scale at billion of file. File between 50 Mb to 4 Gb may be.</li> <li>Object Store can be used for the video persistence. The key for each video may include compression type, resolution...</li> <li>The video player will be able to switch between resolution, or advance in the video time.  </li> </ul> </li> <li> <p>To reduce the cost of this system as CDN and object storage are not cheap. </p> <ul> <li>can classify videos that will be in CDN versus one staying on the servers.</li> <li>Long tail meaning faming - popularity - </li> <li>predict a likelihood to get a video watch today so it can be pushed to the CDN</li> <li>CDN per region per language.</li> </ul> </li> <li> <p>for uploading video, users create metadata, and then upload the raw video in a distributed storage, then the video needs to be transcoded to the different format. We can use queueing approach to get the transcoders always feeded and also being able to run in parallel.</p> </li> <li>Metadata needs to keep the state of if the video transcoding is complete or not, to avoid publishing reference to video not yet transcoded.</li> </ul> <p></p>"},{"location":"architecture/system-design/#designing-search-engine","title":"Designing search engine","text":"<ul> <li>Like google, billions of people and billion of pages</li> <li>The problem is really how to get accurate results for a given query.</li> <li> <p>Start from a reporting database with URL of pages </p> </li> <li> <p>Need to avoid people adding thousand of the same keyword in a unique page to get the hit.</p> </li> <li>Accurate result means the search will return a list of the most likely page what the user's expects. So need to get metrics on page accuracy. </li> <li>we can compute how many times a user perform a search after page results were displayed within a specific time window.</li> </ul> <p>Elaborate an algorithm:</p> <ul> <li>TF/IDF : term frequency / document frequency to assess how a term is relevant across documents.  It works fine for a small document base. With internet scale the denominator is mostly impossible to compute.</li> <li>To address a better solution we need to thing about what to present to the user: a list of top 10 hits. So we need an inverted index: searched keyword -&gt; list of pages sorted by relevance.</li> <li>Page rank was developed for Google to assign a rank based on number of link to the page.</li> <li> <p>How to evaluate relevance of a term within a doc: terms in the documentation, the position, the title, the font size, heading, formatting, metadata attached to the document.</p> </li> <li> <p>Starting for the repository of web pages</p> </li> <li>Need to build a forward indexing to extract word count in document, then reverse it to build for word -&gt; documents list</li> <li>Then compute back links: the link reaching a given page</li> </ul> <p></p>"},{"location":"architecture/system-design/#bpel-to-microservices","title":"BPEL to microservices","text":"<p>Taking into source a BPEL flow like the one below, how do you migrate this application logic to a microservice architecture, may be event-driven?</p> <p></p> <ul> <li>Input can come from HTTP SOAP requests or messages in queue</li> <li>Green components are Service Component Architecture services</li> <li>Customers validation, and persistence can be retried multiple times</li> <li>With BPEL engine state transfer will be persisted to the process server database.</li> <li>The big white rectangle represents an orchestrator</li> <li>Acknowledgement service is to send the customer a message, so we do not want to send duplicate messages</li> <li>B2B call is idempotent</li> <li>Generate a unique id as part of the response: tx_id</li> <li>Exception is about business exception so will be defined inside of any business service.</li> <li>Exception management may trigger human activities to clean the data, gather more information.</li> <li>Compensation flow: if persistence fails, we want to roll back the exception message persistence and restarts from the point before the parallel processing. There are points in the process where we can restart a process, but they are points where the only solution is to cancel the full case.</li> </ul> <p>There is no mention that the flow is part of an external transaction, so to avoid loosing message as soon as the flow gets http soap request or message from queue, it needs to persist those messages and starts the process of customer validation, form validation... If this STP takes 5 seconds to run, it is possible to loose data.</p> <p>HTTP response needs to be returned, so is this flow needs to be terminated before sending the response? If not it means the process will take more time, and so there is a need to be able to contact the person / submitter that something went wrong.</p> <p>One of the question is is this white big rectangle could be part of a 'transaction' so needs to offer rolling back state controlled by other.</p> <p>Saga: is the solution for distributed long running transaction. In Saga, a compensating transaction must be idempotent and retryable.</p> <p>Saga as atomicity, consistency, durability but is not isolated. Kafka and event sourcing can be user to implement the SAGA. The event is the source of trust. Kafka is the data base for this saga transaction. What does it mean to be consistent with event sourcing? </p> <p>The Saga Execution Coordinator is the central component to implement a Saga flow. It contains a Saga log that captures the sequence of events of a distributed transaction. For any failure, the SEC component inspects the Saga log to identify the components impacted and the sequence in which the compensating transactions should execute.</p> <p>For any failure in the SEC component, it can read the Saga log once it\u2019s coming back up. It can then identify the transactions successfully rolled back, which ones are pending, and can take appropriate actions:</p> <p>In the Saga Choreography pattern, each microservices that is part of the transaction publishes an event that is processed by the next microservice. To use this pattern, one needs to make a decision whether the microservice will be part of the Saga. The Choreography pattern is suitable for greenfield microservice application development. </p> <p>With Camel SAGA the Saga EIP implementation based on the MicroProfile sandbox spec (see camel-lra) is indeed called LRA that stands for \"Long Running Action\".</p> <p>There is another solution that it is purely on top of Kafka: the Simple source is an open source project for event sourcing with Kafka and Kafka Streams which can be used for SAGA. </p> <p>Microprofile Long Running Action is the supported approach in Java to do SAGA. This project presents how to use LRA in the context of refrigerator contrainer shipping solution.</p> <p>While this project implements Saga choreography with Kafka.</p>"},{"location":"architecture/system-design/#music-tracking","title":"Music tracking","text":"<p>Problem: given an API to track a certain user listened to a certain song, build a system to track the top 10 most listened to songs and albums in the last week (hours, day, week, month, years aggregations are needed).</p> <p>Use cases: - user plays song. This API tracks the song played at a certain time by a certain users. - playing a song creates an event of start time and song played by who - The list of top 10 songs needs to be updated every hour</p> <p>Scale: - 400 registered users - 40% are active users - Average 2 hours listening every day. Will depend of the population age. - 70 millions of songs. 60 songs added every day - traffic around 200 query per second can grow to 1000qps</p> <ol> <li> <p>Components: Add queueing, pub/sub after the API gateway to push song start record with other information of the user. The goal is to decouple the downstream processing from the write API.</p> <p></p> <p>The 'start-song -event' includes user_id, song_id, timestamps, device_reference. There will be a stop-song-event, with the same data. Stop event may not be relevant for this use cases.</p> <p>The aggregation will be: count the number of event with the same song_id over 60 minutes time window, report this aggregation to a persisted row: date-hours - song_id - count table in Lake House. </p> </li> <li> <p>Application Flow: event to kafka partition can be allocated on the song_id. So all events avout a sond will be in the same partition. We can have hundred of partitions so scale the number of streaming processing. Once event arrived in kafka partition it has also a timestamp. We can add stateful processing to compute the aggregate per minutes or hours. As we also needs to compute larger time windows, we can do some push the aggregation to lake house persistence. Lake House hasa protocol to query data at rest, in cloud object storage format, parquet format. </p> </li> <li> <p>Scaling: adopting a messaging system will help scaling streaming processing. The cloud object storage will help to scale the number of object to persist. The current estimated size it quite low:</p> </li> <li> <p>Resiliency: multi-region deployment, load balancers, active-active capacity on both regions to support failing</p> </li> </ol>"},{"location":"architecture/tcpip/","title":"TCP/IP summary","text":"<p>Transmission Control Protocol (TCP) and the Internet Protocol (IP), TCP/IP provides end-to-end connectivity specifying how data should be formatted, addressed, transmitted, routed and received at the destination. This functionality has been organized into four abstraction layers which are used to sort all related protocols according to the scope of networking involved.</p> <ol> <li>application layer: user data and communicate this data to other applications on another or the same host. HTTP, SMTP, FTP, SSH are applications.</li> <li>transport layer: The transport layer constitutes the networking regime between two network processes, on either the same or different hosts and on either the local network or remote networks separated by routers. Processes are addressed via \"ports,\" and the transport layer header contains the port numbers. UDP is the basic transport layer protocol, providing communication between processes via port addresses in the header. While TCP provides flow-control, connection establishment, and reliable transmission of data</li> <li>internet layer: It has the task of exchanging datagrams across network boundaries. It provides a uniform networking interface that hides the actual topology of the underlying network connections. This layer defines the addressing and routing structures used for the TCP/IP protocol suite. IP addresses are defined in this layer. Its function in routing is to transport datagrams to the next IP router that has the connectivity to a network closer to the final data destination.</li> <li>link layer:  protocols used to describe the local network topology and the interfaces needed to effect transmission of Internet layer datagrams to next-neighbor hosts. Ethernet is part of the link layer. </li> </ol>"},{"location":"architecture/tcpip/#faq","title":"FAQ","text":""},{"location":"architecture/tcpip/#what-is-the-difference-between-tcp-and-udp-when-would-you-use-each-of-them","title":"What is the difference between TCP and UDP? When would you use each of them?","text":"<ul> <li>UDP  provides data integrity via a checksum but does not guarantee delivery.</li> <li>TCP provides both data integrity and delivery guarantee by retransmitting until the receiver acknowledges the reception of the packet.</li> </ul> <p>UDP is typically used for applications such as streaming media: less reliable but performance is important. Calls to DNS are done in UDP</p>"},{"location":"architecture/tcpip/#what-is-a-subnet","title":"what is a subnet?","text":"<p>A subnet is an identifiably separate part of an organization's network. Typically, a subnet may represent all the machines at one geographic location, in one building, or on the same local area network (LAN). Having an organization's network divided into subnets allows it to be connected to the Internet with a single shared network address.</p>"},{"location":"architecture/tcpip/#what-is-subnet-mask","title":"what is subnet mask?","text":"<p>hosts are uniquely identified by their address. IP address is divided into two logical parts, the network prefix and the host identifier field. The network prefix is determining by using the subnet mask and the ip address. So 9.48.85.99 AND 255.255.255.0 gives a network prefix of  9.48.85 and 254 potential hosts in this network (256 - 2).</p> <p>Routers constitute logical or physical borders between the subnets, and manage traffic between them. Each subnet is served by a designated default router.</p>"},{"location":"architecture/tcpip/#what-the-24-number-means-after-an-ip-address","title":"What the /24 number means after an ip address?","text":"<p>Instead of writing out a full subnet mask such as 255.255.255.0, we can shorten this to /24. To come to this number, we simply add the number of subnet bits (1's) in the subnet mask. /24 specifies the number of bits that do not change. So with 192.128.0.0/24 only the last 2^8 bit can be used to assign IP addresses. </p> <ul> <li>192.128.23.31/32 means one IP @.</li> <li>192.128.23.0/31 means we have 192.128.23.0 and 192.128.23.1 possible IP addresses.</li> <li>0.0.0.0/0 means all IP @.</li> </ul> <p>https://www.ipaddressguide.com/cidr</p>"},{"location":"architecture/tcpip/#what-is-cidr","title":"what is CIDR?","text":"<p>Classless inter-domain routing (CIDR) is a set of Internet protocol (IP) standards that is used to create unique identifiers for networks and individual devices.  CIDR uses variable length subnet masking (VLSM) to divide networks into subnets of various size: the first part of an IP address is a prefix, which identifies the network. The prefix is followed by the host identifier so that information packets can be sent to particular computers within the network. It helps to group blocks of addresses into a single routing network: 192.0.1.0/24, the prefix is 192.0.1.0, and the total number of bits in the address is 24 so it is possible to have 256 hosts. With a.b.0.0/16 we can have 65536 hosts.</p>"},{"location":"architecture/tcpip/#how-to-interpret-routing-table","title":"How to interpret routing table?","text":"<p>In linux <code>route -n</code> displays IP routing table: the host is on 172.16 network. The router/gateway is on 251.2 </p> <pre><code>Kernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n0.0.0.0            172.16.251.2    0.0.0.0             UG    100    0        0 ens33\n169.254.0.0     0.0.0.0            255.255.0.0     U     1000   0        0 ens33\n172.16.251.0   0.0.0.0            255.255.255.0   U     100    0        0 ens33\n172.17.0.0       0.0.0.0            255.255.0.0     U     0      0        0 docker0\n</code></pre> <p><code>netstat -nr</code> works the same. </p>"},{"location":"architecture/tcpip/#what-does-a-default-gateway-do","title":"What does a default gateway do?","text":"<ul> <li>serves as an access point to another network</li> <li>is a node that routes the traffic from a workstation to another network segment. </li> <li>it commonly connects the internal networks and the outside network (Internet).</li> <li>the gateway is also associated with both a router, which uses headers and forwarding tables to determine where packets are sent, and a switch, which provides the actual path for the packet in and out of the gateway.</li> </ul>"},{"location":"architecture/tcpip/#what-is-the-difference-between-broadcast-and-multicast","title":"What is the difference between broadcast and multicast?","text":"<p>With Broadcast a message is sent to all devices on a network segment or subnet. It is used for tasks like network discovery, address resolution, and distributing network-wide information.</p> <p>multicast is the delivery of a message or information to a group of destination computers simultaneously in a single transmission from the source. Source sends a packet only once, nodes in the network take care of replicating the packet to reach multiple receivers. It is used in video streaming, audio conferencing, and real-time data distribution.</p>"},{"location":"architecture/tcpip/#how-does-ping-work","title":"How does ping work?","text":"<p>used to test if a host can reach an ip address on the network. It sends ICMP, an echo request packet, to the target.  It measures the time from transmission to reception (round-trip time) and records any packet loss. </p> <p>IPV4 uses a 32 bits addressing schema.  IPV6 uses 128 bits</p>"},{"location":"architecture/tcpip/#what-is-nat-where-would-you-use-it-how-does-it-work-what-are-some-limitations","title":"What is NAT? Where would you use it? How does it work? What are some limitations?","text":"<p>Network address translation (NAT) is a methodology of modifying network address information in Internet Protocol (IP) datagram packet headers while they are in transit across a traffic routing device for the purpose of remapping one IP address space into another. It is used to hide private hosts from one network to another. </p>"},{"location":"architecture/tcpip/#what-is-dhcp","title":"What is DHCP?","text":"<p>Dynamic Host Configuration Protocol (DHCP) is a client/server protocol that automatically provides an Internet Protocol (IP) host with its IP address and other related configuration information such as the subnet mask and default gateway. Local router has an IP address assigned by the Internet Service Provider.</p> <p>Within a local network, a DHCP server assigns a local IP address to each device connected to the network. For example at home with AT&amp;T ISP router is on 192.168.1.254 all devices in the home network are under  192.168.1.xx the subnet mask 255.255.255.0. The public API address is 108.199.129.124</p>"},{"location":"architecture/tcpip/#how-does-web-browser-communicate-to-http-server","title":"How does web browser communicate to http server?","text":"<ul> <li>user enters a URL, the port number can be 80 or something else specified</li> <li>browser does a NS lookup to get IP address for the specified host. The DNS server returns IP addresses of the load balancers which will route to the target backend apps.</li> <li>browser initiates a TCP connection to the web server</li> <li>After the server completes acknowledgment of its side of the TCP connection, the browser sends HTTP requests to the server to retrieve the content</li> <li>To avoid re-initializing the HTTP connection, HTTP offers the keep-alive protocol. HTTP 1.1 supports persistent connections by default. </li> <li>the browser may also save temporary information about its connections to local files on the client computer called cookies</li> </ul>"},{"location":"architecture/tcpip/#what-is-http-pipelining","title":"What is HTTP pipelining?","text":"<p>Because HTTP 1.1 relies on persistent connections, you can use it to send multiple queries in a row and expect responses in the same order. This is called HTTP pipelining.</p> <p>However, the drawback is that requests must queue up on the connection until the requests in front of them complete. This is called head-of-line blocking.</p>"},{"location":"architecture/tcpip/#http2","title":"HTTP/2","text":"<p>HTTP/2 is a redesign of HTTP to support low latency with full request and response multiplexing. Connection header is now forbidden and all clients and servers must persist connections. </p> <p>A client can send multiple requests in parallel on the same connection, which is called multiplexing.</p> <p>It supports the notion of a stream, which allows a bidirectional flow of bytes to be exchanged over a persistent TCP connection. Each stream can carry one or more messages. A message is a complete sequence of frames that map to a logical request or response.</p>"},{"location":"architecture/tcpip/#how-to-setup-static-ip-address-with-a-guest-ubuntu-running-on-mac-using-vmfusion","title":"How to setup static IP address with a Guest ubuntu running on Mac using VMfusion?","text":"<p>By default, a new VMware Fusion guest will have a single NIC (Network Interface Card) configured to use Internet Sharing (e.g., \u201cShare with my Mac\u201d) This allows the guest to communicate with the outside world\u200a\u2014\u200aresolve DNS records, download operating system updates, access arbitrary Internet resources\u200a\u2014\u200ausing network address translation (NAT). Through an internal DHCP service, the guest is assigned a private, RFC1918 address, which VMware translates to the host\u2019s IP address before forwarding along to the appropriate gateway (typically, the \u201cdefault gateway\u201d, i.e., the upstream router).</p> <p>When the guest serves a set of services, then it does not work well, and you need a static address for clients to connect.</p> <p>The solution is to add a second NIC to your VMware guest, configure it to use a static IP address, and configure your Mac to resolve all hostnames that match a particular pattern (e.g., \u201c.dev\u201d) to that IP address</p> <ul> <li> <p>Add a device in the VM settings, select \u2018Private to my mac\u2019 and generate a MAC address.</p> <p></p> </li> <li> <p>keep the mac address 00:50:56:3D:3D:B4</p> </li> <li>Once done do a <code>ifconfig</code> on the MAC and we can see that VM fusion use vmnet1 for \u201chost-only\u201d networking and use vmnet8 for NAT/shared connections</li> </ul> <pre><code>vmnet1: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\nether 00:50:56:c0:00:01 \ninet 192.168.65.1 netmask 0xffffff00 broadcast 192.168.65.255\nvmnet8: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500\nether 00:50:56:c0:00:08 \ninet 172.16.251.1 netmask 0xffffff00 broadcast 172.16.251.255\n</code></pre> <p>the vmnet1 network is using the 192.168.65.0/255 network - Check which IPs may be automatically given out by the VMware DHCP server, so that we can avoid an overlapping</p> <pre><code>grep range /Library/Preferences/VMware\\ Fusion/vmnet1/dhcpd.conf\nrange 192.168.65.128 192.168.65.254;\n</code></pre> <p>You should choose an IP outside this range, but not .1 as that\u2019s assigned to the vmnet1 NIC itself; for this example, I\u2019ve chosen 192.168.65.100 for my \u201cubuntu\u201d VMware guest.</p> <ul> <li>Edit /Library/Preferences/VMware Fusion/vmnet1/dhcpd.conf to add the host static address.</li> </ul> <pre><code>host ubuntu {\n        hardware ethernet 00:50:56:3D:3D:B4\n        fixed-address 192.168.65.100;\n        option domain-name-servers 0.0.0.0;\n        option domain-name \"\";\n}\n</code></pre> <ul> <li>Configure the DHCP server of the vmware</li> </ul> <pre><code>sudo /Applications/VMware\\ Fusion.app/Contents/Library/vmnet-cli \u2014configure\n\nsudo /Applications/VMware\\ Fusion.app/Contents/Library/vmnet-cli \u2014stop\n\nsudo /Applications/VMware\\ Fusion.app/Contents/Library/vmnet-cli --start\n</code></pre> <ul> <li>Boot the vm and verify the IP address</li> </ul>"},{"location":"architecture/tcpip/#what-is-qos-how-does-it-work-where-would-you-apply-it-when-is-it-enforced","title":"What is QoS? How does it work? Where would you apply it? When is it enforced?","text":"<p>Quality of service comprises requirements on all the aspects of a connection, In packet switching, QOS is the ability to provide different priority to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow.</p> <p>To quantitatively measure quality of service, several related aspects of the network service are often considered, such as error rates, bandwidth, throughput, transmission delay, availability, jitter...</p> <p>A best-effort network or service does not support quality of service. An alternative to complex QoS control mechanisms is to provide high quality communication over a best-effort network by over-provisioning the capacity so that it is sufficient for the expected peak traffic load.</p>"},{"location":"astronomy/","title":"Astronomy Introduction","text":""},{"location":"astronomy/#position","title":"Position","text":"<p>There are 88 constellations to map stars in our sky.  The night sky changes with time and season, some constellations could be seen only during a certain period of the year. The relative positions of the stars do not change,</p>"},{"location":"astronomy/#celestial-sphere","title":"Celestial Sphere","text":"<p>Celestial Sphere  is an imaginary sphere of infinitely large radius enclosing the visible universe so that all celestial bodies appear to be projected onto its surface. The two points projected onto the celestial sphere by the extension of the Earth's <code>axis of rotation</code> are called the celestial poles (North Celestial Pole and South Celestial Pole). It is easy to find the North Celestial Pole in the sky, as the relatively bright Pole Star is located very close to this place.</p> <p>Meridian: is a great circle passing through the two poles of the celestial sphere and the zenith of a given observer.</p> <p>Position of the stars are fixed in the celestial sphere from the Earth point of view. Celestial Sphere rotates daily around the axis through the poles from East to West. Inside the celestial sphere the Earth rotates from west to east. The stars (apparently) rotate about the NSP counter-clockwise.</p> <p></p> <p>See this detailed explanations for NSP, horizon, zenith...</p> <p>On Earth : latitude is the angle from the equator to the observed position. Longitude is the angle from meridian 0, Greenwich, to that position. </p> <p></p> <p>The position on Earth determines the visible sky: the horizon is perpendicular to the zenith line. Extending the axis of Earth, we can find the axis of the relative rotation of the celestial sphere.</p> <p>If we project the Earth equator to the celestial sphere, we can have the Celestial equator, so it is possible to measure the height of a star in the sky from its angle to this equator. This is the Declination or celestial latitude. Declination 0 is the celestial equator, 90 is the north celestial pole, and -90 the south celestial pole.</p> <p>The right ascension, RA, is the celestial longitude measured in hours instead of degrees. The full circle being 24h. 1 h = 15 degrees. The hour makes sense, as if we let the Earth turns, it takes 2 hour to see a star at a longitude that was 30 degrees East earlier.</p> <p>The zero longitude meets the equator in pisces: it represents the celestial meridian= the 0 hour of right ascension. </p>"},{"location":"astronomy/#local-view","title":"Local view","text":"<p>Any celestial body can be identified by the two coordinates altitude h and azimuth alpha (horizontal coordinates). The Altitude of a star is its angle from the horizon of the observer on Earth.</p> <p>The zenith is the point above our head perpendicular to our horizon line.</p> <p>The zenith angle: 90 - altitude. </p> <p>Azimuth: angle between the north vector and the perpendicular projection of the star down onto the horizon. To an observer on Earth sky appears to rotate around celestial pole.</p> <p>Stars near north celestial pole never set: an example is Big Dipper. Stars near celestial equator rise, move from East to West and set. Orion is such constellation.</p> <p>See details in this reference: http://www.jgiesen.de/elevaz/basics/index.htm</p> <p>Here is a simple schema to present the relation between zenith, Latitude star declination and the Right Ascension:</p> <p></p> <p>When the star is at zenith, the latitude of the observer on Earth = the star declination. </p> <p>The star position simulation applet.</p> <p>Knowing two star declinations and right ascensions when one star is at zenith for an observer, where on Earth another observer will see the other star at zenith?</p> <p>Consider two stars, star 1 and star 2, with right ascensions RA1 and RA2 where RA2 &gt; RA1. Let say star 1 is on your meridian right now, star 2 will cross your meridian in (RA2 - RA1) hours later.  1h of right ascension is 15\u00b0 To compute latitude and longitude of the second observer; the latitude is the declination of the second star (hypothesis star at zenith), and the difference of longitudes between the two points on Earth is the RA2-RA1 which should be in minutes brought back to degree using the conversion 60 minutes = 1 degree (longitude point 1 - this delta) = longitude difference point 2 =&gt; still need to see where it is from 0 meridian.</p> <p>If Aldebaran is crossing our local meridian, how long would we need to wait to see Regulus cross the meridian?</p> <p>Convert the RA in minutes and then do the difference between aldebaran and regulus RA, convert back in hour: 5h 32 mn</p> <p>Altitude and Azimuth of a Star at a location?</p> <p>If Aldebaran (d=16d 31\u2019) is crossing the local meridian in Saint Petersburg (l=59d 56\u2019). <code>altitude= declination + 90 - latitude</code> -&gt; a=16 + 90 - 59 = 46 degres  -&gt; transform each value in minutes then back to degree for azimuth.</p> <p>A star's azimuth is only either north (0deg) or south (180deg) if it's crossing the observer's meridian. If it's not on the meridian, the azimuth could be another angle.</p> <p>At Sao Paulo, the latitude is -23 so the zenith is -23. The closest star is at -23 - (altitude) -&gt; Regulus Zenith angle = |Declination - Latitude|. To derive what star is the closed to the zenith of sao paulo</p>"},{"location":"astronomy/#sidereal-time","title":"Sidereal Time","text":"<p>Sidereal time is the celestial meridian coinciding with local meridien. 24 sidereal time hours is one full rotation of Earth. It changes with longitude: 1 h = 15 degrees.</p> <p>1 sidereal day is one rotation with respect to the stars = 23h 56 minutes</p> <p>One solar day is 24h and equal to one rotation/sun.</p> <p>Which of the stars would be visible in the early evening sky (around sunset) on September 21 in Sao Paulo, Brazil?</p> <p>On September 21, Sidereal Time = Local Time. The position of 0RA is defined as one of the points at which the celestial equator intersects the ecliptic. This happens twice a year, in the Spring and in the Fall. The Vernal Equinox is when this occurs in the Spring, and the Sun overhead on the Celestial Equator.  That is defined as 0RA, and occurs at noon local time. Therefore 12:00 local on the Vernal Equinox is 0RA. So Sidereal Time is LT + 12 on the Vernal Equinox. Six months later, on the Autumnal Equinox, the Sun is 12 hours opposite the position at the Vernal Equinox. And at noon, 12:00, the Sun is at 12RA. Therefore LT = ST, on the Autumnal Equinox.</p> <p>In the early evening, LT = 18.00 = ST. LT \u201clies\u201d on the zero meridian. At the eastern horizon (at azimuth 90\u2070), we can see the right ascension 18.00+06.00 = 24.00, and at the western horizon (at azimuth - 90\u2070), we can see the right ascension 18.00-06.00 = 12.00.</p> <p>An observer will see locally stars whose RA lies between 12.00 and 24.00 + the stars always visible, as always set.</p>"},{"location":"astronomy/#tilt-and-seasons","title":"Tilt and seasons","text":"<p>Earth is tilted by 23.5 degrees from its ecliptic plane: the plane on which earth orbits the sun. From an earth centric view, the sun is following a circle not on the celestial equator, but a circle 23.5d tilted. Ecliptic meets equator at vernal (3/21) &amp; autumnal (9/21) equinox by convention 0h and 12h RA. At that time the earth tilt is perpendicular to the direction of the sun rays. The line separating the night and the day pass by the pole. all location on earth has the same amount of time on day and night.</p> <p>The sun declination changes from 23.5 max (6/21) to - 23.5 (12/21)</p> <p>See this Season simulator.</p>"},{"location":"astronomy/#moon","title":"Moon","text":"<p>Moon moves around the celestial sphere as, it orbits Earth, from west to east, in 27.32 days. RA increases by 52 min per day. As the spin is locked to orbit we see the same side of the moon.</p> <p>The moon rises 48 mn later than it does the day before.</p> <p>The synodic month is 29.52 day. It is a full moon rotation to the sun. The moon position controls the phases and the rise/set times. Full moon when moon rises 12 h after the sun.</p> <p>The moon orbits in the plane of the ecliptic but with a tilt of 5 degrees. The intersections between the orbit circle of the moon and the ecliptic is called the node. When both the moon and the sun are on the nodes then there is an eclipse: twice a year of 346.6 days (roughly every 173.3 days) it is the eclipse season: new moon -&gt; solar eclipse, full moon -&gt; moon eclipse.</p> <p>At new moon, it is zero and the Moon is said to be in conjonction. At full moon, the elongation is 180\u00b0 and it is said to be in opposition. </p> <p>The lunar equator is not in the plane of its orbit, but is inclined to it by a constant value of 6.688\u00b0.</p> <p>Moon has almost same angular size as sun. When the alignment is perfect the moon completely obscures Sun on a 250 km shadow projected on earth.</p> <p>When the moon is little bit farther this is an annular eclipse.</p> <p>The reddish color of the moon during a moon eclipse is due to the earth atmosphere. At the beginning of moon croissant it is possible to see the part that is in shadow. This is due to the reflection of the light from earth.</p> <p>Moon animation</p>"},{"location":"astronomy/#mathematical-recall","title":"Mathematical recall","text":"<ul> <li>arc second = 1/3600 degree</li> <li>arc minute = 1/ 60 degree</li> <li>tan = sin / cos = opposed / adjacent</li> <li>radian to degree: 180 d = pi radian</li> <li>arcsecond to radian = n arcseconds =&gt; n/ 206265 radians.  </li> </ul>"},{"location":"astronomy/physic/","title":"Physics summary","text":""},{"location":"astronomy/physic/#kinematics","title":"Kinematics","text":"<p>Kinematics describes the way objects move with their velocity and their acceleration. Their positions are relative to a reference frame.</p> <p>A displacement is the change in position of an object = Xf - Xo. Displacement is defined by both direction and magnitude: it is a vector quantity. Distance is the magnitude or size of displacement between two positions. It has no sign.</p> <p>The distance traveled can be greater than the magnitude of the displacement. </p> <p>A scalar is any quantity that has a magnitude, but no direction. Speed is a scalar quantity. It does not change at all with direction changes. </p> <p>Time is related to motion by looking at elapsed time for a particular motion:</p> <p></p> <p>The average velocity is the displacement (change in position) divided by the time of travel:</p> <p></p> <p>When arriving at the same position as the beginning: </p> <p></p> <p>then average velocity is zero.</p> <ul> <li>Instantaneous velocity v is the average velocity at a specific instant in time or an infinitesimally small time interval.</li> <li>Instantaneous speed is the magnitude of instantaneous velocity</li> <li>Average speed is the distance traveled divided by elapsed time.</li> <li>Average Acceleration is the rate at which velocity changes, </li> </ul> <p></p> <ul> <li>Acceleration is a vector in the same direction as the change in velocity, \u0394v.</li> </ul> <p></p> <p>which gives insight into the relationship between displacement, average velocity, and time.</p> <p> gives us insight into the relationships among velocity, acceleration, and time</p> <p></p>"},{"location":"astronomy/physic/#physic-problem-solving","title":"Physic problem solving","text":"<ul> <li>Examine the situation to determine which physical principles are involved. Uses sketch to clarify the elements.</li> <li>Make a list of what is given or can be inferred from the problem as stated (identify the knowns).</li> <li>Identify exactly what needs to be determined in the problem (identify the unknowns).</li> <li>Find an equation or set of equations that can help you solve the problem.</li> <li>Substitute the knowns along with their units into the appropriate equation, and obtain numerical solutions complete with units.</li> <li>Check the answer to see if it is reasonable: Does it make sense? </li> </ul>"},{"location":"astronomy/physic/#gravity","title":"Gravity","text":"<ul> <li> <p>Falling objects, if air resistance and friction are negligible, fall toward the center of Earth with the same constant acceleration, independent of their mass. </p> <p></p> </li> <li> <p>Air resistance opposes the motion of an object through the air, while friction opposes the motion between objects.</p> </li> <li>Acceleration due to gravity   varies from 9.78 to 9.83, depending on latitude, altitude, underlying geological formations, and local topography.</li> </ul>"},{"location":"astronomy/physic/#projectile-motion","title":"Projectile motion","text":"<p>Projectile motion is the motion of an object thrown or projected into the air, subject to only the acceleration of gravity.</p> <p>Motions along perpendicular axes are independent and thus can be analyzed separately.</p> <p>The total displacement of a soccer ball at a point along its path. The vector <code>s</code> has components <code>x</code> and <code>y</code> along the horizontal and vertical axes. Its magnitude is <code>s</code> , and it makes an angle <code>\u03b8</code> with the horizontal.</p> <p></p> <p></p>"},{"location":"astronomy/physic/#newtons-laws-of-motion","title":"Newton's laws of motion","text":"<p>Dynamics considers the forces that affect the motion of moving objects and systems. Newton\u2019s laws of motion are the foundation of dynamics.</p> <ol> <li>Objects and systems have properties such as mass and charge. Systems may have internal structure.</li> <li>Fields existing in space can be used to explain interactions.</li> <li>The interactions of an object with other objects can be described by forces.</li> <li>Interactions between systems can result in changes in those systems.</li> <li>Changes that occur as a result of interactions are constrained by conservation laws. </li> </ol> <p>Force is a push or pull that can be defined in terms of various standards, and it is a vector having both magnitude and direction. External forces are any outside forces that act on a body. A free-body diagram is a drawing of all external forces acting on a body. </p> <p>Newton\u2019s first law of motion states that in an inertial frame of reference a body at rest remains at rest, or, if in motion, remains in motion at a constant velocity unless acted on by a net external force. This is also known as the law of inertia. We can think of this law as preserving the status quo of motion. </p> <p>Inertia is the tendency of an object to remain at rest or remain in motion. Inertia is related to an object\u2019s mass. The inertia of an object is measured by its mass. Mass is the quantity of matter in a substance. Unlike weight, mass does not vary with location, it is the same on Earth, in orbit, or on the surface of the Moon.</p> <p>Change in motion is equivalent to a change in velocity -&gt; A change in velocity means, by definition, that there is an acceleration. </p> <ul> <li>The net external force is the vector sum of  all external forces</li> <li>Acceleration is directly proportional to the net external force</li> <li>Acceleration should be inversely proportional to the mass of the system : the larger the mass (the inertia), the smaller the acceleration produced by a given force:</li> <li> <p>Newton\u2019s second law of motion is:  </p> <p>  or </p> </li> </ul> <p>The unit is the Newton:  is the force needed to accelerate a 1-kg system at the rate of 1m/s/s.</p> <p>The equation for an object weight is </p> <p>it is the gravitational force applied to the object from the nearest large body.</p> <ul> <li>The third law stipulates: Whenever one body exerts a force on a second body, the first body experiences a force that is equal in magnitude and opposite in direction to the force that it exerts.  </li> </ul> <p>Normal force is perpendicular to the surface of the contact and is supporting the  load of  the weight applied to this surface. A tension is a force along the length of a medium, especially a force carried by a flexible medium, such as a rope or cable.</p>"},{"location":"astronomy/physic/#problem-solving","title":"Problem solving","text":"<ul> <li>Draw a sketch of the problem.</li> <li>Identify known and unknown quantities, and identify the system of interest.</li> <li>Write Newton\u2019s second law in the horizontal and vertical directions and add the forces acting on the object. If the object does not accelerate in a particular direction (for example, the x -direction) then Fnet x = 0 . If the object does accelerate in that direction, Fnet x = ma</li> <li> <p>Verify result is reasonable.</p> </li> <li> <p>Only four distinct forces account for all known phenomena: the gravitational force, the electromagnetic force, the weak nuclear force, and the strong nuclear force.</p> </li> <li> <p>What is it that carries forces between objects? </p> <ul> <li>Imagine that a force field surrounds whatever object creates the force.</li> <li>The field itself is the \u201cthing\u201d that carries the force from one object to another.</li> <li>These fields may be either scalar or vector fields. Gravity and electromagnetism are examples of vector fields.</li> </ul> </li> <li> <p>Gravitational waves are created during the collision of massive stars, in black holes, or in supernova explosions, like shock wave. Waves move at the speed of light.</p> <ul> <li>The Laser Interferometer Gravitational-Wave Observatory (LIGO) is the instrument to try to catch gravitational waves.</li> <li>It uses optical lasers to examine any slight shift in the relative positions of two masses due to the effect of gravity waves.</li> <li>Laser Interferometer Space Antenna will be the antenna in space. The system will measure the relative positions of each satellite to detect passing gravitational waves. Accuracy to within 10% of the size of an atom will be needed to detect any waves.</li> </ul> </li> </ul>"},{"location":"astronomy/physic/#friction","title":"Friction","text":"<p>Friction is a force that opposes relative motion between systems in contact. When objects move then the friction between them is called kinetic friction. If they do not move, friction = static friction = fs</p> <p></p> <p>Static friction is a responsive force that increases to be equal and opposite to whatever force is exerted, up to its maximum limit. Once the applied force exceeds fs(max), the object will move. </p> <p>Then the friction is:</p> <p></p> <p>The motion of the skier and friction are parallel to the slope and so it is most convenient to project all forces onto a coordinate system where one axis is parallel to the slope and the other is perpendicular.</p> <p></p> <p></p> <p>An object will slide down an inclined plane at a constant velocity if the net force on the object is zero.</p>"},{"location":"astronomy/physic/#rotation-angle-and-angular-velocity","title":"Rotation Angle and Angular Velocity","text":"<p>Rotation angle \u0394\u03b8 to be the ratio of the arc length to the radius of curvature: </p> <p></p> <p>The arc length \u0394s is the distance traveled along a circular path.</p> <p>Angular velocity \u03c9 as the rate of change of an angle. In symbols, this is \u03c9 = \u0394\u03b8/ \u0394t. The greater the  rotation angle in a given amount of time, the greater the angular velocity. The units for angular velocity are radians per second (rad/s).  Angular velocity \u03c9 is analogous to linear velocity v. To get the precise relationship between angular and linear velocity:</p> <p></p>"},{"location":"astronomy/physic/#centripetal-acceleration","title":"Centripetal Acceleration","text":"<p>Centripetal acceleration is the acceleration of an object moving in uniform circular motion. Acceleration is in the direction of the change in velocity, which points directly toward the center of rotation.</p> <p>The magnitude is:</p> <p></p> <p>Always convert in rad/s</p>"},{"location":"astronomy/physic/#centripetal-force","title":"Centripetal Force","text":"<p>Any net force causing uniform circular motion is called a centripetal force. The direction of a centripetal force is toward the center of curvature.</p> <p></p>"},{"location":"astronomy/physic/#work-and-kinetic-energy","title":"Work and Kinetic Energy","text":"<p>We can loosely define energy as the ability to do work. An important aspect of energy is that the total amount of energy in the universe is constant. Energy can change forms, but it cannot appear from nothing or disappear without a trace.</p> <p>Energy may be transferred into or out of the system, and the change must be equal to the amount transferred. Kinetic energy is introduced as an energy of motion that can be changed by the amount of work done by an external force.</p> <p>Potential energy can only exist when objects interact with each other via conservative forces according to classical physics. It works for a constant force: </p> <p> </p> <p>d represents the displacement of the system, the Force applied at angle \u03b8. Work and energy are measured in newton-meters  or Joule. One calorie (1 cal) of heat is the amount required to warm 1 g of water by 1 C degree, and is equivalent to 4.184 Joules.</p> <p>The net work on a system equals the change in the kinetic energy for transactional motion. </p> <p></p> <p></p> <p></p> <ul> <li> <p>What is the Work to accelerate a 30 kg package by pushing at constant force = 120 N through a distance of 0.8 m (friction force is 5 N):</p> <ul> <li>The force of gravity and the normal force acting on the package are perpendicular to the displacement and do no work.</li> <li>The net force arises solely from the horizontal applied force Fapp and the horizontal friction force Fc </li> <li>Net force is // to the displacement so \u03b8 = 0o and cos(\u03b8) = 1 </li> <li>Fnet = 120 - 5 = 115 N</li> <li>The effect of Fnet is to accelerate the package from V0 to V.</li> <li>The kinetic energy increases, the net work done on the system is positive</li> <li>Wnet = Fnet .d = m.a.d   (from newton 2<sup>nd</sup> law Fnet = m.a)  Wnet = 115 * 0.8 = 92</li> <li>V^2 = Vo^2 + 2.a.d  =&gt; a = (V^2  - Vo^2) / 2d</li> <li>Wnet = m/2 . (V^2  - Vo^2)</li> <li>By looking at the work only: W total = Wapp + Wgr + Wn + Wfr   =&gt; W total = 115 . cos(0) . 0.8 + 0 + 0 + 5 . cos(180) . 0.8 = 96 - 4 = 92</li> </ul> </li> </ul>"},{"location":"astronomy/physic/#gravitational-potential-energy","title":"gravitational potential energy","text":"<p>The gravitational potential energy:</p> <p></p> <p>This energy is associated with the state of separation between two objects that attract each other by the gravitational force. Because gravitational potential energy depends on relative position, we need a reference level (earth surface) at which to set the potential energy equal to 0. What is important is the difference in gravitational potential energy, because this difference is what relates to the work done.</p> <p>If we release the mass, gravitational force will do an amount of work equal to <code>mgh</code> on it, thereby increasing its kinetic energy by that same amount:</p> <ul> <li> <p>\u0394PEg = mgh applies for any path that has a change in height of h  </p> </li> <li> <p>At the start of a roller coaster its PE is max and it will lose it as it goes downhill. The net work on the roller is the gravity one so:  </p> </li> </ul> <p></p> <p></p> <p></p> <p>So </p> <p></p> <p>When friction is negligible, the speed of a falling body depends only on its initial speed and height, and not on its mass or the path taken.</p>"},{"location":"astronomy/physic/#conservative-forces-and-potential-energy","title":"Conservative Forces and Potential Energy","text":"<p>The potential energy is the energy a system has due to position, shape, or configuration. It is stored energy that is completely recoverable. We can define a potential energy (PE) for any conservative force. The work done against a conservative force to reach a final configuration depends on the configuration, not the path followed, and is the potential energy added. Potential energy comes from the interaction between the bowling ball and the ground.</p> <ul> <li>For a spring the Hooke's law says:</li> </ul> <p></p> <p>the amount of deformation produced by a force F , k is the spring\u2019s force constant. </p> <p>The average force is:</p> <p></p> <p>From the  Kinetic Energy and the Work-Energy Theorem, we note the area under a graph of F vs. \u0394L is the work done by the force so:</p> <p></p> <p>x is \u0394L =  distance that the spring is stretched.</p> <p>The conservation of mechanical energy :  KE_initial + PE_initial = KE_final + PE_final   for conservative force only.</p>"},{"location":"astronomy/physic/#non-conservative-forces","title":"Non conservative forces","text":"<p>A non-conservative force is one for which work depends on the path taken. The work done by a non-conservative force adds or removes mechanical energy from a system. Friction creates Thermal energy.</p> <p>The net work is the sum of the work by non-conservative forces plus the work by conservative forces.</p> <p>The work and energy theorem works as:</p> <p>KE_initial + PE_initial + Wnc = KE_final + PE_final </p> <p>the amount of non-conservative work equals the change in mechanical energy.</p>"},{"location":"astronomy/physic/#conservation-of-energy","title":"Conservation of Energy","text":"<p>The total energy is constant in any process. It may change in form or be transferred from one system to another, but the total  remains the same. </p> <p>KEi + PEi + Wnc + OEi = KEf + PEf + OEf</p> <p>OE is for other energies.</p> <ul> <li>Electrical energy is a common form that is converted to many other forms.</li> <li>Fuels, such as gasoline and food, carry chemical energy that can be transferred to a system through oxidation.</li> <li>Chemical fuel can also produce electrical energy, such as in batteries. Batteries can in turn produce light, which is a very pure form of energy.</li> <li>Nuclear energy comes from processes that convert measurable amounts of mass into energy. Nuclear energy is transformed into the energy of sunlight, into electrical energy in power plants, and into the energy of the heat transfer and blast in weapons.</li> <li>Atoms and molecules inside all objects are in random motion. This internal mechanical energy from the random motions is called thermal energy, because it is related to the temperature of the object. These and all other forms of energy can be converted into one another and can do work.</li> <li>A car is not a closed system. You add energy in the form of more gas in the tank (or charging the batteries), and energy is lost due to air resistance and friction.</li> <li>The efficiency Eff of an energy conversion process is defined as work output / total energy input</li> </ul>"},{"location":"astronomy/physic/#power","title":"Power","text":"<ul> <li>P = W / t   1 Watt = 1 Joule / s</li> <li>Because work is energy transfer, power is also the rate at which energy is expended. </li> <li>Sunlight reaching Earth\u2019s surface carries a maximum power of about 1.3 kilowatts per square meter  </li> <li>Energy consumed E = P . t</li> </ul>"},{"location":"astronomy/physic/#energy-conversion-in-human","title":"Energy Conversion in Human","text":"<p>Our own bodies, like all living organisms, are energy conversion machines. Conservation of energy implies that the chemical energy stored in food is converted into work, thermal energy, and/or stored as chemical energy in fatty tissue. The rate at which the body uses food energy to sustain life and to do different activities is called the metabolic rate.</p> <p>Energy consumption is directly proportional to oxygen consumption because the digestive process is basically one of oxidizing food.</p> <p>Approximately 20 kJ of energy are produced for each liter of oxygen consumed.</p>"},{"location":"blogs/","title":"Blogs","text":""},{"location":"blogs/#feb-2024","title":"Feb 2024","text":"<ul> <li>Medium - Unveiling the Future with Insight Storming: Enhancing Event Driven Architecture (EDA) Solutions</li> </ul>"},{"location":"blogs/#aug-2023","title":"Aug 2023","text":"<ul> <li>Linkedin- Why Event-Driven Architecture is important in 2020s Multiple articles on IBM developers site, and BPM journal.</li> </ul>"},{"location":"blogs/#march-2022","title":"March 2022","text":"<ul> <li>Medium - Developer\u2019s experience with an event-driven solution implementation - 03/2022</li> </ul>"},{"location":"blogs/#dev-2021","title":"Dev 2021","text":"<p>A developer journey to develop an event-driven microservice from DDD to devops.</p> <p>&gt;&gt; read more</p>"},{"location":"blogs/#sept-2021","title":"Sept 2021","text":"<p>Develop a decision service with IBM Automation Decision Service, predictive scoring with Watson Studio and Quarkus App as client.</p> <p>&gt;&gt; read more</p>"},{"location":"blogs/#aug-2021","title":"Aug 2021","text":"<p>Support new requirements and EDA governance construct in a new reference architecture</p> <ul> <li>Medium - Updated EDA reference architecture - 08/2021</li> <li>Medium - Event-driven solution is still a hot topic</li> </ul> <p>&gt;&gt; local blog</p>"},{"location":"blogs/#may-2021-a-new-interesting-reference-implementation","title":"May 2021 A new interesting reference implementation","text":"<p>The Covid-19 vaccine shipment is a real challenge and last year we had to develop a proof od technology to address how and event based solution with linear programming could help address the vaccine order and optimize the vaccine lots shipment. In this blog I'm presenting different components of the order optimization and reference all the material to  run it your self on OpenShift.</p> <p>&gt;&gt; Read more</p>"},{"location":"blogs/#april-2021-event-driven-architecture-still-a-hot-topic","title":"April 2021 - Event Driven Architecture still a hot topic","text":"<p>All the major 500 companies are adopting loosely coupled, event-driven microservice architecture,  new data pipeline for their big data processing and deploying event backbone technology like Apache Kafka or Pulsar.  In this blog, I present the use cases, methodology and technology pointers...</p> <p>&gt;&gt; Read more</p>"},{"location":"blogs/#jan-2021-outbox-pattern-with-debezium-quarkus","title":"Jan 2021 - Outbox Pattern with Debezium , Quarkus","text":"<p>The Transactional outbox pattern helps to save within the same database transaction, and the events you want to publish to the message brokers, like Kafka. In this blog I present the labs and content I developed with Postgresql, Quarkus and Debezium.</p> <p>&gt;&gt; Read more</p>"},{"location":"blogs/#oct-2020-take-advantage-of-the-best-integration-approaches","title":"Oct 2020 - Take advantage of the best integration approaches","text":"<p>IT environments are now fundamentally hybrid and multicloud in nature. Devices, systems, and people are spread across the globe and at the same time are virtualized. Achieving integration across this ever-changing environment, providing it at the pace of modern digital initiatives, and still being able to manage the landscape is a significant challenge</p> <p>&gt;&gt; Read more</p>"},{"location":"blogs/#to-do","title":"TO DO","text":"<ul> <li>Blog on kc container solution</li> <li>Blog on vaccine demo</li> <li>Blog on real time inventory</li> <li>Blog on EDA governance</li> <li>Blog on fit for purpose</li> </ul>"},{"location":"blogs/02-27-2021/","title":"EDA at the core of automation","text":"<p>With the digitalization movement, a lot of business optimizationd and IT optimizationd are linked to the increase of automating human work. </p>"},{"location":"blogs/consultant-job/","title":"The job of Solution Architect","text":"<p>Solution Architects are consultants. The IT Consultant is defined as \"knowledgeable professional who helps businesses develop, integrate, and maximize the value of IT systems\". I do not want to re-invent all the extended information about IT consultant job, but for me there are two main group of activities:</p> <ol> <li>Manage day to day tasks like any other white collar workers do: meetings with customer, issues management, project management, dependencies management, management reporting, internal administration tasks.</li> <li>Knowledge management: As Stephen Covey in \"the 7 habits of highly effective people\" wrote: we need to sharpen the saw, to be a better person. This is where the value of a consultant resides. SAs share his knowledge to get customer\u2019s trust, so customers feel comfortable to invest in your technology and in your company because they feel the company's technical human resources will help them to remove roadblocks. Therefore Solution Architect needs to continuously work on his skill set for IT, Cloud and different technology to support customer's requests and build trust.I think as of now a SA should spend around from 40% to 60% on knowledge acquisition and maintenance.</li> </ol> <p>In 2023 I think the role of SA needs to change dramatically bad adding generative AI as part of the job.</p> <p>To simplify the management of the knowledge acquisition and measurement process and get better results per consultant, genAI will help dramatically as it will offers an integrated and standard way to share knowledge. It will help scaling SA business too by improving the quality of the responses and increasing the number of accounts per SA.</p> <p>We should be able to get rid of those useless certifications, that do no proof anything in term of consultant value: it just demonstrate that the person was able to memorize a lot of questions and answers to succeed to an exam at a given point of time. 80% of this knowledge is lost after 2 months.</p> <p>After 2 weeks we tend to remember 10% of we read, 20% of what we hear, 30% of what we see, 50% of what we hear and see, 70% of what we say, and 90% of what we say and do. Those classical management numbers  will stay valid for a long time, and demonstrate we should enforce show and tell as a model for knowledge acquisition.</p> <p>The big idea will be to build on top of existing LLM the following corpus for SA:</p> <ul> <li>Service product documentation</li> <li>Blog, forum, re:post, Q&amp;A, FAQ, articles</li> <li>Labs, code sample</li> <li>Architecture patterns</li> <li>Slack channel content</li> </ul> <p>The consultant will leverage this chat to get answers to most of the customer requests. The skill will then evolve as understanding customer problems and questions, assess what is the context of those questions, query the bot and select the best answer, then validate the answer with hands-on work. As to apply show and tell, we should focus on getting SAs the most efficient tools possible to be able to demonstrate and build PoC very quickly and the same way cross SAs.</p>"},{"location":"blogs/freelance/","title":"Freelance Consultant","text":"<p>Notes from Udemy course + articles.</p>"},{"location":"blogs/freelance/#introduction","title":"Introduction","text":"<p>As consultant, we find solution to problems and we are hired to solve them. As outside contractor, we are more objective than employees, not influenced by the same constraints than employees. We read between the lines, think out of the boxes, and find solutions.</p> <p>Why becoming consultant: Jobs are not secured, more freedom, and there are taxation benefits (expenses). </p>"},{"location":"blogs/freelance/#is-there-a-market","title":"Is there a Market?","text":"<p>Yes people uses consultants. 40% of US workforce is tuning freelance in 2020. IT is always in high demand.</p>"},{"location":"blogs/freelance/#how-to-do-the-job","title":"How to do the job","text":"<ul> <li>Understand the problem to solve</li> <li>Where are we at the moment?</li> <li>Where do we want to reach? By when at what cost?</li> <li>Once you have determined the desired outcome, deliver it, on time, on budget, on quality.</li> </ul>"},{"location":"blogs/freelance/#find-your-niche","title":"Find your niche","text":"<ul> <li>What are your talents? </li> <li>Experience.</li> <li>What are the things people ask your help for?</li> <li>What falls within your comfort zone?</li> <li>What has worked and what has not worked.</li> <li>Be relevant and always up to date on the last news.</li> </ul>"},{"location":"blogs/freelance/#project-management-time","title":"Project management - Time","text":"<ul> <li>Always defined a permissible variation of time. (around 10% and 15% )</li> <li>Use management PM tool, like trello.com, Asana.com</li> </ul>"},{"location":"blogs/freelance/#project-management-cost","title":"Project management - Cost","text":"<ul> <li>Fee and costs are different. Fee how much you charge for the work, hours, week,... costs are for expenses, or hire more people to do certain job.</li> <li>Keep them clearly separate line items in contract. Set permissible variation in %.</li> <li>Cost can be hourly payment at the task</li> <li>But could be cost upfront from you and get paid later</li> <li>Make impact sheet: money customer spent with my service vs how much the customer will have paid for internal resources. This is a deliverable. </li> </ul>"},{"location":"blogs/freelance/#project-management-quality","title":"Project management - Quality","text":"<ul> <li>Always keep the client in the know.</li> <li>This is what we agree with the client.</li> <li>Use sharing platforms such as Dropbox, Google Drive.</li> <li>Share prototype, always see intermediate steps.</li> <li>Get every step approved. </li> </ul>"},{"location":"blogs/freelance/#tricks-of-the-trade","title":"Tricks of the trade","text":""},{"location":"blogs/freelance/#dealing-with-the-client","title":"Dealing with the client","text":"<ul> <li>Don't overkill (using academic words, buzz words...) or undercook (seat back lazily). Meet the client at the level they are at!. </li> <li>Be sure to involve them in the problem solving exercise, do not let the client left out.</li> <li>Always do a client profiling: let them speak, do not be disruptive. Open up to ask them if they do not understand something you are saying.</li> </ul>"},{"location":"blogs/freelance/#contracts","title":"Contracts","text":"<ul> <li>Always get a written contract. This is a performance based industry.</li> <li>At least deliver what you agreed and try to over deliver, as this is the best thing you can do to the client.</li> <li>Start from Law depot to get contract template.</li> <li>Fees are not define alone. Understand what the Market wants. </li> <li>A rule of estimation: the annual salary in the company / annual working hours * 3.</li> <li>Charge by Hour, week, project based.</li> </ul> Type Pros Cons Hours Better with uncertainty. Problem with dependencies. Spend more time that was planned due to client's circumstances Link to time. Have a minimum amount per hours (15 mn?) Daily Same hours. Longer project. What constitute a day. Define a minimum chunk of hours per day (4h) Fixed You know what needs to be done. Delivered, get paid. Good when reuse. Can be part of the reputation too. Complicated. Fault management. Change management."},{"location":"blogs/freelance/#resource-goldmine","title":"Resource Goldmine","text":"<ul> <li>Personal network</li> <li>People per hour</li> <li>Fiverr</li> <li>[wix.com]</li> </ul>"},{"location":"blogs/freelance/#managing-work","title":"Managing Work","text":"<ul> <li>Zoho to manage customer. Free for up to 10 users.</li> </ul>"},{"location":"blogs/freelance/#strategy","title":"Strategy","text":"<ul> <li>Personal brand (niche market, limited amount of people) or company.</li> <li>Business plan: Consider and plan: taxation, time, and current job.</li> <li>One-page business plan: bplans</li> <li>Not use personal email. </li> <li>What's App has business service, for B2B.</li> <li>Web Site: Wordpress. Need to be responsive, to adapt to mobile devices. Do not spend too much money. Think get a web site.</li> <li>Logo, for branding and attractive. </li> <li>Always get an inward information gathering mechanism: Use competitive intelligence, or news aggregator like feedly.com</li> </ul>"},{"location":"blogs/freelance/#marketing-side","title":"Marketing side","text":"<ul> <li>Make yourself being known. </li> <li>Target oriented marketing. Match to the niche we are in. </li> <li>Sustained marketing: Be online- offline. Persevere. Except if there is a huge niche and we are the only to address it.</li> <li>Personal network. Your net worth = your network. Do not neglect your network.</li> <li>WebSite for marketing: project yourself in first person. </li> <li>Separate mobile phone for business. For business we may need a landline. </li> <li>If Tweet, be relevant and content with quality. Brand awareness. </li> <li>Blogs: at least one blog per week to show you are relevant. Show what you know about. Consider rewriting some news from news.google.com, techcrunch, consultancy. Platform Medium, Linkedin</li> <li>Live streaming, and youtube: one video per week. Embedded in blog. </li> <li>Paid marketing: google ads, Facebook. But $$$.</li> <li>Get thing done for free as a marketing method: on your own field. May be in educational sector.</li> <li>Get good reviews.= to build a reputation. </li> </ul>"},{"location":"blogs/freelance/#content-curation","title":"Content curation","text":"<ul> <li>The goal is to save time, money and effort for the readers, clients. Take daily news, organize it and present to readers. Combined content. Get credit for the people as source.</li> </ul>"},{"location":"blogs/freelance/#leveraging-social-medias","title":"Leveraging social medias","text":""},{"location":"blogs/freelance/#personal-web-page-get-insight-of-traffic","title":"Personal web page, get insight of traffic","text":"<ul> <li>wix.com</li> <li>wordpress</li> </ul> <p>ex: https://www.ce2innovate.com/</p>"},{"location":"blogs/freelance/#core-services","title":"Core services","text":"<p>Hybrid Cloud architecture, application modernization consulting Jumpstart your Event-driven architecture and solution implementation Event and data streaming for modern pipelines Implement real-life RAG and business application using LLM architecture.</p>"},{"location":"blogs/freelance/#what-offers","title":"What offers","text":"<p>See someone doing this: https://www.lydtechconsulting.com/news.html</p> <p>ai-innovation.com</p> <ul> <li>Event Storming workshop</li> <li>Assess technology choice</li> <li>Conduct the first iteration of an event-driven solution</li> <li>Discovery assessment</li> <li>Courses</li> <li>Case Studies</li> <li>News section</li> <li>Book a call type of service</li> </ul>"},{"location":"blogs/freelance/#linkedin","title":"Linkedin","text":""},{"location":"blogs/freelance/#youtube","title":"Youtube","text":""},{"location":"blogs/freelance/#medium","title":"Medium","text":""},{"location":"blogs/01-10-21/","title":"01/10/2021 - Transactional Outbox pattern with Debezium, Quarkus and Postgresql","text":"<p>The Transactional outbox pattern helps to save within the same database transaction, the event you want to publish to the messaging middleware, like Kafka. I just finalized an example of Order management microservice in the context of the Vaccine at scale demo.</p> <p>The implementation uses Quarkus with Reactive Messaging, OpenAPI, JAXRS and theDebezium outbox pattern with Debezium change data capture  for Postgresl to Kafka.</p> <p>The main classes in this project is the OrderService.java class which use the following code approach. Get the Order entity persist it in the same transaction as 'emitting the event', which is not really emitting an event, but save to the <code>orderevents</code> table.</p> <pre><code> @Inject\n    Event&lt;ExportedEvent&lt;?, ?&gt;&gt; event;\n\n@Transactional\npublic VaccineOrderEntity saveNewOrder(VaccineOrderEntity orderEntity) {\n    orderEntity.status = OrderStatus.OPEN;\n    orderEntity.creationDate = simpleDateFormat.format(new Date());\n    orderRepository.persist(orderEntity);\n    event.fire(OrderCreatedEvent.of(orderEntity));\n    return orderEntity;\n}\n</code></pre> <p>So the trick is coming from the OrderCreateEvent which is a io.debezium.outbox.quarkus.ExportedEvent. </p> <pre><code>@Entity\npublic class OrderCreatedEvent implements ExportedEvent&lt;String, JsonNode&gt; {\n    ....\n}\n</code></pre> <p>The application is configured to specify what table to use, and what will be the key and value of the future Kafka records.</p> <pre><code>quarkus.debezium-outbox.id.name=aggregateid\nquarkus.debezium-outbox.id.column-definition=\"DECIMAL NOT NULL\"\nquarkus.index-dependency.outbox.group-id=io.debezium\nquarkus.debezium-outbox.aggregate-id.name=aggregateid\nquarkus.debezium-outbox.aggregate-id.column-definition=\"DECIMAL NOT NULL\"\nquarkus.index-dependency.outbox.artifact-id=debezium-quarkus-outbox\nquarkus.debezium-outbox.table-name=orderevents\n</code></pre> <p>The deployment of this service to Kubernetes is explained in this section. And for the Debezium Change data Capture component, as it is a Kafka Connector, I use the Strimzi source to image approach, as described here.</p> <p>A new added Order </p> <p></p> <p>is now visible in the entity table </p> <p>and the orderevents table.</p>"},{"location":"blogs/04-15-21/","title":"Event Driven Architecture still a hot subject","text":"<p>Info</p> <p>This blog was published to Medium, here and to blogspot.</p> <p>Recently I have to justify why Event-Driven Architecture is still a hot topic on the current market: as I work with a lot of companies in financial, healthcare or retailer industries, I see strong adoption for loosely coupled, event-driven microservice solutions, with new data pipeline used to inject data to modern data lakes, and the adoption of event backbone technology like Apache Kafka, or Apache Pulsar.</p> <p>With my team, we are working on reference architectures, reference implementations, and first-of-a-kind solutions based on event-driven implementations. Over the last year, this space is evolving a lot with the adoption of serverless, knative eventing, cloud-native technologies, microprofile messaging...</p> <p>In this blog, I want to introduce our EDA work and references to some assets you may be able to leverage for your own project. I will continue future blogs to present in more detail some of those assets and best practices we have to develop for customer engagements.</p> <p>Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. It supports asynchronous communication between components and most of the time a pub/sub programming model. The adoption of microservices brings some interesting challenges like data consistency, contract coupling, and scalability that EDA helps to address.</p> <p>From the business value point of view, adopting this architecture helps to scale business applications according to workload and supports easy extension by adding new components over time that are ready to produce or consume events that are already present in the overall system. New real-time data streaming applications can be developed which we were not able to do before.</p>"},{"location":"blogs/04-15-21/#the-main-use-cases","title":"The main use cases","text":"<p>At the high level, the main business motivation to use event-based solutions is to respond to data creation or modification in real-time, as they happen, which means assessing what those changes are, delivering responsive customer experiences, or assessing business risks and opportunities.</p> <p>The adoption of AI and predictive scoring can also be integrated into the real-time data pipeline to build more intelligent applications.</p> <p>At the technical level we can see three adoptions of event-driven solutions:</p> <ol> <li>Modern data pipeline to move the classical batch processing of extract, transform and load job to real-time ingestion, where data are continuously visible in a central messaging backbone. The data sources can be databases, queues, or specific producer applications, while the consumers can be applications, streaming flow, long storage bucket, queues, databases\u2026</li> <li>Adopt asynchronously, publish-subscribe communication between cloud-native microservices to help to scale and decoupling: the adoption of microservices for developing business applications, has helped to address maintenance and scalability, but pure RESTful or SOAP based solutions have brought integration and coupling challenges that inhibited the agility promised by microservice architecture. Pub/sub helps to improve decoupling, but design good practices are very important. (I will elaborate on that in future blogs)</li> <li>Real time analytics: this embraces pure analytic computations like aggregate on the data streams but also complex event processing, time window-bbased reasoning, or AI scoring integration on the data streams. </li> </ol>"},{"location":"blogs/04-15-21/#reference-architecture","title":"Reference architecture","text":"<p>With those main drivers in place, we have defined reference architectures to assess what are the common components involved in such event-driven solutions. At IBM, we have developed two architecture diagrams, but I am using the extended one in most of my customer's engagements as it includes how to leverage data in the event backbone to develop AI model, which is, I think, an important pattern to adopt.</p> <p></p> <p>I encourage you to read more about those components and capabilities from this website.</p>"},{"location":"blogs/04-15-21/#event-driven-patterns","title":"Event-driven patterns","text":"<p>Coming with the adoption of event-driven solutions, developers and architects need to assess the different design patterns we have in our toolbox, like Event Sourcing to persist the state of a business entity as a sequence of state-changing events, Command Query Responsibility Segregation (CQRS)  to segregate the write from the read model and APIs,  SAGA to support long-running transactions that span multiple microservices with compensation process, Transactional Outbox to get data from data base table and to send messages to topic or queue,  and other patterns.</p> <p>Those patterns help to better design event-driven microservices, but common sense prevails for selecting such pattern when really needed. We have done different implementations of those patterns to get some starting code and practice for your own implementation:</p> <ul> <li>CQRS for an order management service for shipping goods overseas.</li> <li>Saga to support shipping order, with boat allocation, and refrigerator container assignment</li> <li>Transactional outbox on another order management service, this one on vaccine order. It uses Quarkus Debezium plugin, Postgresql  and Kafka Connector</li> </ul>"},{"location":"blogs/04-15-21/#how-to-get-started","title":"How to get started","text":"<p>From the methodology point of view, the event storming method was introduced and publicized by Alberto Brandolini in \"Introducing event storming book\u201d  for rapid capture of a solution design and improved team understanding of the domain. We extended the workshop to assess events relationship and insights derived from the data stream: this will help to design intelligent agents that continuously process event streams.</p> <p>As we use Lean startup and agile development practices, event storming and domain driven design, help us to start on good foundations for our event-driven solution. This is not simple exercise and it is easy to go wrong. But it is important to be able to pivot, refactor and adapt the boundary of the microservice when we discover major implementation issues, like coupling by the service and the event schemas, ...</p> <p>We go into details on how to conduct an event storming workshop in this article  with a quick summary of the domain driven design concepts and constructs.</p> <p>From a developer's point of view, I want to provide some simple sample starting code based on the same microservice scope: an order management service, but in different languages, and by using domain-driven design and elements like Asynch API, Avro schemas\u2026.</p> <p>The eda-quickstarts github repository includes Quarkus 2.x Kafka producer and consumer, and Spring cloud projects: this github repository is still under work and I welcome contributor.</p>"},{"location":"blogs/04-15-21/#technology-trends","title":"Technology trends","text":"<p>Apache Kafka is the current choice to support the event-backbone component of our reference architectures, we have extensive studies to summarize the key concepts and best practices to implement producer and consumer applications, complemented with a lot of hands-on labs.</p> <p>Deploying and managing Kafka is greatly facilitated if it runs on Kubernetes, and for the best deployment approach, use the Strimzi Operator which can be used from OpenShift Operator Hub. IBM Event Streams is also based on this operator.</p> <p>As a nice competitor to Kafka, Apache Pulsar addresses all the expected event-backbone features but add more interesting feature.</p> <p>A new player in the Kafka field is \"Redpanda\" from Vectorized which delivers a lot of very nice features to overcome some of the Kafka's issues: Kafka is a 10+ years old technology designed with hardware constraints that are now legacy.</p> <p>When you want to look at implementing streaming logic, Kafka uses Kafka Streams API or kSQL to support it. They are strongly integrated with Kafka and even if they address most of the stream processing requirements, there are alternates. In 2021, I think adopting Apache Flink as your tool to support data streaming may be a better solution and investment as it is really an universal tool, well designed, able to scale independently of Kafka, and can be used with different data sources. Flink has a lot of features that will help you to address data streaming use cases, but also support SQL, and complex event processing. Combining event backbone and data streaming with AI is the way to go and we need to ramp up our skillset on all of that.</p>"},{"location":"blogs/04-15-21/#important-eda-links","title":"Important EDA links","text":"<ul> <li>IBM event-driven reference architecture</li> <li>EDA field guide</li> <li>Event-driven architecture body of knowledge (Always work in progress)</li> <li>Event storming methodology</li> <li>Kafka fundamentals with some other best practices</li> </ul> <p>This blog was also published to Medium</p>"},{"location":"blogs/05-29-21/","title":"Index","text":"<p>The optimization problem can be summarized as </p>"},{"location":"blogs/08-15-21/","title":"Updated EDA reference architecture","text":"<p>This blog is now on medium</p>"},{"location":"blogs/09-30-21/","title":"Develop a credit risk scoring solution with ADS and Watson Studio","text":"<p>In this article I am covering how to develop a simple decision service with the new IBM Automation Decision Service product using Decision Model Notation, a Quarkus app to call the deployed service and a Watson ML predictive scoring model.  This article is inspired from the work done by Pierre Berlandier on that subject.</p> <p>At the highest level the solution looks like in the following system context diagram:</p> <p></p> <p>But we are more interested by the components involved in this demo:</p> <p></p> <p>To get started with ADS product, I recommend to follow the ADS product documentation Getting Started tutorial. In this article, I am detailing some sections of this tutorial and add other development practices like Quarkus integration, adopting event processing and predictive scoring.</p> <p>The business use case is quite simple as it is based on a person asking for a credit to his/her bank. Each person is classified as good or bad and then rules defined combined risk score from other business rules  and a predictive score.</p> <p>The credit scoring predictive model is based on the Kaggle German credit data set. </p>"},{"location":"blogs/09-30-21/#pre-requisites","title":"Pre-requisites","text":"<p>You need to get the following products / tool available to your environment</p> <ul> <li>Maven</li> <li>Quarkus CLI</li> <li>Access to an OpenShift Cluster. For example using Red Hat OpenShift on IBM Cloud.</li> </ul> <p></p> <p>Open the OpenShift Console and get access to the login CLI</p> <pre><code>oc login --token=..... --server=https://...\n</code></pre> <ul> <li>Get Cloud Pak for Automation deployed with ADS configured. </li> <li>Get one Waston Studio instance on IBM Cloud</li> </ul>"},{"location":"blogs/09-30-21/#too-long-to-read","title":"Too long to read","text":"<p>If you just want to see the code, go to this repository and browse the solution, or import it in your own Automation Studio: to do so perform the following steps:</p> <ol> <li>Clone this git repository <code>git clone https://github.com/ibm-cloud-architecture/ads-assess-credit-risk</code></li> <li>Connect to Automation Decision Designer: If you need details on how to connect, see this section</li> <li>Add Decision Automation Service project, see this section) for details but come back here once done.</li> <li>Import the decision service</li> <li>Navigate into the decision model using the Designer.</li> </ol>"},{"location":"blogs/09-30-21/#connect-to-automation-studio","title":"Connect to Automation Studio","text":"<p>To get visibility of the different access points, and user credentials of the Cloud Pak for Automation components use the <code>icp4adeploy-cp4ba-access-info</code> config map:</p> <pre><code># under the project where the common services are deployed\noc describe cm icp4adeploy-cp4ba-access-info \n</code></pre> <p>Look at the <code>bastudio-access-info</code>. The address may start with <code>https://cpd-&lt;projectname&gt;-&lt;clustername&gt;</code></p> <ul> <li>Once logged in, the IBM Automation, go to the Navigation bar.</li> </ul> <p></p> <ul> <li>Select Business Automation Studio app, on the left menu:</li> </ul> <p></p>"},{"location":"blogs/09-30-21/#play-with-existing-solution","title":"Play with existing solution","text":"<p>Warning</p> <p>If you want to develop the solution continue on the next section. If not import and review the content of the decision service.</p>"},{"location":"blogs/09-30-21/#import-the-demo","title":"Import the demo","text":"<ul> <li>Use <code>Import</code> button from main Business Automation home page, select the <code>assess-credit-risk.zip</code> file from the <code>ads-assess-credit-risk</code> cloned repository.</li> </ul> <ul> <li>Select <code>ads-assess-credit-risk</code> tile from the <code>Decision Services</code> view, you should see the assess risk decision and a risk prediction.</li> </ul>"},{"location":"blogs/09-30-21/#navigate-into-the-content","title":"Navigate into the content:","text":"<p>The model designer is giving us the current model implementation. </p> <ul> <li>Select the <code>Assess Risk</code> decision model, which lead to the decision model designer view:</li> </ul> <p></p> <p>The diagram shows us the combined risk assessment is defined from the loan data, a risk prediction and a relationship assessment.</p> <ul> <li>From there you can review  the <code>Loan</code> entity, and using the <code>Go to Data model</code> button, you can  look at the data model definition</li> </ul> <p></p> <ul> <li>Selecting the <code>relationship assessment</code> node in the DMN, and then the <code>Logic</code> button, we can see a decision table which is setting a risk value according to the number of owned accounts, and the number of years as  bank customer.</li> </ul> <p></p> <p>You can continue to navigate into this simple decision model</p>"},{"location":"blogs/09-30-21/#test-it","title":"Test it","text":"<p>Selecting Run will help us to test the model. Running with no data will return a <code>null</code> output.</p> <p></p> <pre><code>{\n  \"loan\": {\n    \"borrower\": {\n      \"accountses\": [\n        {\n          \"type\": \"Primary\",\n          \"balance\": 2000\n        }\n      ],\n      \"years with bank\": 5,\n      \"years employed\": \"\",\n      \"years at current address\": 6,\n      \"age\": 30\n    }\n  }\n}\n</code></pre>"},{"location":"blogs/09-30-21/#deploy-it","title":"Deploy it","text":""},{"location":"blogs/09-30-21/#work-on-the-automation-decision-service","title":"Work on the Automation Decision Service","text":""},{"location":"blogs/09-30-21/#create-a-new-decision-automation","title":"Create a new decision automation","text":"<ul> <li>Select the <code>Create</code> button and the <code>Decision Automation</code> choice, </li> </ul> <p>Define a name according to the business intent: </p> <p></p> <ul> <li>From the home page for the decision automation, we need  to add decision service, define decisions, model...</li> </ul> <p></p>"},{"location":"blogs/09-30-21/#create-decision-services","title":"Create Decision Services","text":"<p>Create a decision service to start creating all the artifacts you need to capture your decision.</p> <p></p> <p>In the Decision Service page, start by adding model:</p> <p></p>"},{"location":"blogs/09-30-21/#define-decision-model","title":"Define Decision Model","text":"<p>This is where the analysis work done before by the business analysts will help define the elements of the decision model.</p> <ul> <li>First we create the DMN model:</li> </ul> <p></p> <p>Which leads us to the DMN designer:</p> <p></p> <p>With decision model, we do not want to define complex data model, but focus only on the relevant attributes needed to take business decisions.</p> <p>Once we renamed the decision (to <code>AssessRisk</code>) and select the <code>Output type</code> to change the type to be a Risk instead of a String. </p> <p></p> <p>When we first open the data modeler, we need to specify a model name <code>LoanModel</code>.</p> <p></p>"},{"location":"blogs/09-30-21/#define-the-decision-output","title":"Define the decision output","text":"<ul> <li>The we can define enumeration type (Like a <code>RiskLevel</code> with <code>low, medium, high</code> values):</li> </ul> <ul> <li>And define the <code>Risk</code> composite type:</li> </ul> <p>The Business Analysts and load underwriter defined the following requirements:</p> <ul> <li>The Risk aggregates a set of sub risk assessments and scoring: predictive, relationship to derive a combined value.</li> </ul> <p></p> <ul> <li>Once the Risk and Risk Level are defined we can change the output of the decision to be a Risk, using the <code>Output Type</code> combo-list.</li> </ul>"},{"location":"blogs/09-30-21/#some-business-decision-analysis","title":"Some business decision analysis","text":"<p>What kind of input do we need to use to compute a risk?. In this example we will make it simply by looking at the following requirements:</p> <ul> <li>A loan application has borrower and co-borrowers. </li> <li>A borrower has Accounts, each with a balance and a type. This will help to compute current assets.</li> <li>A borrower has classical human characteristics but also the information about the number of years with employers, type of employment, and the number of years with the same bank.</li> <li>Finally the loan application, the expected duration, type, amount to borrow, and property value.</li> <li>The loan to value ratio can be computed if not set by the decision service client.</li> </ul>"},{"location":"blogs/09-30-21/#extend-the-input-model","title":"Extend the input model","text":"<p>So we need Borrower, LoanApplication, Account as Composite types. We do not need to detail all attributes for each entities. Later as we will connect to a git repository, the model will be persisted in different model files. Those files can be seen here and you can get inspiration from those class definitions to develop your model. </p> <p></p>"},{"location":"blogs/09-30-21/#defining-the-business-logic","title":"Defining the business logic","text":"<p>We want to use two sud-decisions and then combined them to build a combined risk. So in the Editor, we select the Assess Risk and add two decisions. We defined the Risk Level to be the output of both decisions, and then link the Loan Application input to those two decisions. The final model looks like:</p> <p></p>"},{"location":"blogs/09-30-21/#assess-loan-amount-risk","title":"Assess loan amount risk","text":"<p>Select the <code>Assess loan amount</code> node and add a decision table using the <code>logic</code> button:</p> <p></p>"},{"location":"blogs/09-30-21/#assess-relationship-to-the-bank-risk","title":"Assess relationship to the bank risk","text":"<p>To add some of the business rules or tables, we select the decision node and then the <code>logic</code> choice, and then select 'Decision table`:</p> <p>You select the years with current bank attribute and populate min, max values for three rows.</p> <p></p>"},{"location":"blogs/09-30-21/#combine-the-two-risks","title":"Combine the two risks","text":"<p>To combine the two risk, we build another decision table:</p> <p></p> <p>Add a default rule</p> <p></p>"},{"location":"blogs/09-30-21/#change-the-terms-so-they-can-be-more-readable-and-reflect-ubiquitous-language","title":"Change the terms so they can be more readable and reflect ubiquitous language","text":"<p>The default rules is showing use that the current 'verbalization' is not perfect, so we need to change some working on the decision names:</p> <ul> <li>Rename <code>Assess relationship</code> decision node to <code>Assess bank relationship risk decision</code>.</li> <li>Rename <code>Assess loan amount</code> node to <code>Assess loan amount risk decision</code></li> </ul>"},{"location":"blogs/09-30-21/#test-locally","title":"Test locally","text":"<p>Using the <code>Run</code> tab we reach a panel where we can enter some test data and then run:</p> <p></p> <p>Most likely it will fail.  Need to look into the data and the rules. The <code>Run history</code> provides some hints on what went wrong.</p> <p>In the previous screen, I forgot to add a default rule on the final decision so at least when conditions are not met something is returned. </p> <p>Normally we need to do more testing, and also extract to json input documents to be able to use them with deployed code.</p>"},{"location":"blogs/09-30-21/#deploy","title":"Deploy","text":"<p>The next step is to deploy to an ADS run time. To deploy a decision service, we must define an operation that is used to call the service.</p> <ul> <li>Define an operation based on the decision model define</li> </ul> <p></p> <p>Then use <code>Deploy</code> tab</p> <p>./images/deploy-ds-0.png)</p> <ul> <li>define a version number (<code>v0.0.1</code>), then select the service and the deploy command.</li> </ul> <p></p> <p>The deployment is using Jenkins pipeline, and will build from the github repository, so ensure to always commit your last changes.  Looking at the trace the build process should be successful, it should build a jar file like <code>.m2/repository/decisions/assessloadrisk/assess_loan_risk/assess-Loan-RiskDecisionService/v0.0.1/assess-Loan-RiskDecisionService-v0.0.1.jar</code></p> <pre><code>[INFO] Reactor Summary:\n[INFO] \n[INFO] assess-Loan-Risk v0.0.1 ............................ SUCCESS [  1.930 s]\n[INFO] loanModel LATEST-SNAPSHOT .......................... SUCCESS [ 10.267 s]\n[INFO] assessLoanRisk LATEST-SNAPSHOT ..................... SUCCESS [ 12.905 s]\n[INFO] assess-Loan-RiskDecisionService v0.0.1 ............. SUCCESS [ 37.442 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n</code></pre>"},{"location":"blogs/09-30-21/#share-with-git","title":"Share with Git","text":"<p>In gitHub, create a public repository, for example <code>assess-loan-application-ds</code>. The suffix <code>-ds</code> for decision service. Be sure to have defined a git access Token on your github account to let the Automation Studio being able to push code to this new repository.</p> <p>Select the Share change tab </p> <p></p> <p>Then select the Repository <code>Connect</code> button to define the git connection parameters (use the access token as API Key):</p> <p></p> <p>Once connected you should get the following output:</p> <p></p> <p>The next step is to go to the <code>Share changes</code> tab and share the changes proposed, which should update the Git repository content. It is important to have no content in the newly created git repository for the first sharing operation. </p> <p></p> <p>Which includes folder for the data model and the decision model:</p> <p></p>"},{"location":"blogs/09-30-21/#predictive-scoring-development","title":"Predictive scoring development","text":"<p>This section is inspired by the work done by Pierre Berlandier on the Kaggle Risk Data.</p>"},{"location":"blogs/09-30-21/#credit-application-client-development","title":"Credit application client development","text":""},{"location":"blogs/09-30-21/#future-readings","title":"Future Readings","text":"<ul> <li>German Credit Risk - Credit Classification - Kaggle project</li> </ul>"},{"location":"blogs/10-11-20/","title":"10-11/2020 - Take advantage of the best integration approaches","text":"<p>Digital transformation, microservices adoption, and cloud deployment include integration  modernization. When you design solutions, you need to think end-to-end and not just  cover microservices boundaries. Integration defines reusable and managed APIs, develops modern  integration flows that can be deployed inside microservice, or addresses the data pipeline at  scale with a high-throughput, low-latency messaging platform. You can continue transactional  support with IBM\u00ae MQ while you adopt event sourcing for resiliency, auditing, or to apply  AI models on real time flows. With high-volume data transfer, you can inject unstructured  data to your data lake for developing new AI models. Integration stays at the core of the IT  infrastructure with a new containerized deployment model to integrate your modern development  practices. This is an exciting time to think about and adopt the next integration reference  architecture</p> <p>The IBM integration reference architecture introduction outlines how to transform your integration platform to a more modern architecture, with fine grained deployment, decentralized integration ownership and cloud native infrastructure.  To start the journey you have to assess where the integration modernization will make more impact, and you can see three major perspectives:</p> <p></p> <ol> <li>Application integration</li> <li>API exposure: Consumer-centric, channel optimized, adopting distributed gateways, with self-service discovery, developer portals to subscribe to API, and manage versioning. APIs can be internal and external</li> <li>Messaging, and as part of the messaging the event driven architecture with technology like Kafka, Pulsar, NATS, or RedPanda.</li> </ol> <p>Read more from the IBM integration reference architecture and listen to Kim Clark's video on breaking the ESB starting at 3 minutes.</p> <p>The Booklet referenced in this video, is in this page.</p>"},{"location":"blogs/12-27-21/","title":"Developer's experience with an event-driven solution implementation","text":"<p>Event-driven solutions are complex to implement, a lot of parts need to be considered, and I did  not find any articles that present an end-to-end discussion on how to do things with the last technologies available  to us ('last' meaning March 2022). </p> <p>I want to propose a set of articles to address this end-to-end developer's experience, not  fully in the perfect order of developer's activities,  as normally  we should start by doing an event-storming workshop with the business subject matter experts  and apply Domain Driven Design approach to design the different event-driven microservices. I assume this work is already done and bounded contexts have defined the microservice's scope.</p> <p>At the minimum level, an event-driven solution will have producer applications, event brokers, consumer applications for event processing and sinks to keep data for the longer term or do other functions such as indexing and querying. As we want governance to understand how to consume data and who is doing what in this distributed solution, we need to add schema registry,  OpenAPI and AsyncAPI management, complemented with application metadata management. </p> <p>So we have a lot to cover, but three parts should be enough to swallow.</p> <ul> <li>Part 1: is about reviewing the components to consider and getting started with GitOps to deploy all those components</li> <li>Part 2: addresses producer code development with OpenAPI and AsyncAPI</li> <li>Part 3: covers consumer code, starting from AsyncAPI and working with streaming. </li> </ul>"},{"location":"blogs/12-27-21/#part-1-event-driven-solution-components-and-getting-started","title":"Part 1: Event-driven solution components and getting started","text":"<p>I will take a real-time inventory use case, and use a simulator to generate TLOG transactions coming from cashing machines. From the software components that we will install, we use IBM Event Streams, IBM MQ and API Connect, the API management product, all running on Red Hat OpenShift as our Kubernetes platform.</p> <p>The figure below represents all the components deployed:</p> <p></p> <p>Figure 1: the product view of a simple solution</p> <p>The blue components are product operators and operands and the green components will be solution-oriented. To support the deployment of such components, I will use a GitOps approach where Day 1 operations are defined in one repository to bootstrap the deployment, and Day 2 operations will be controlled with the Git operating model, which developers know very well, based on Pull Request, commit, release... The GitOps repository for this real-time inventory is here and the readme guides you on how to deploy the solution on your OpenShift Cluster.</p> <p>The GitOps approach is using the catalog repository to keep product-specific operator subscription definitions, where product instance definitions are part of the real-time inventory solution GitOps repository. This corresponds to the yellow rectangles in the figure below:</p> <p></p> <p>Figure 2: GitOps approach</p>"},{"location":"blogs/12-27-21/#high-level-developers-activities","title":"High-level developer's activities","text":"<p>At the high level, developers need to address the following tasks:</p> <ul> <li>Use domain-driven design and event storming to discover the business process to support  and discover the different bounded contexts which will be mapped to microservices.</li> <li>Use a code template as a base for the event-driven microservice: to avoid reinventing configuration definitions for the different messaging used (MQ or Kafka or other). Those templates use the DDD Onion architecture, and are based on Quarkus, Nodejs, Python, or Spring cloud..  The code templates also assume the services are containerized and deployed to Kubernetes or OpenShift.</li> <li>Create a GitOps repository with KAM, deploy the pipelines and GitOps to manage the solution and the different deployment environments (<code>dev</code>, <code>staging</code>, <code>prod</code>). Define specific pipelines tasks and pipeline flows to build the code and the docker images. Connect git repository  via webhook to the pipeline tool (e.g. Tekton)</li> <li>Define message structure using AVRO or JSON schemas, generate Java Beans from the event definitions using maven or other tools (Quarkus Apicurio plugin supports bean autogeneration).</li> <li>Connect and upload schemas to the schema registry.</li> <li>Define REST endpoints and OpenAPI documents, then upload them to API management tool.</li> <li>Apply test-driven development for the business logic, assess integration tests scope,  and tune development environment accordingly. </li> <li>Ensure continuous deployment with ArgoCD</li> <li>Create the consumer application, get AsyncAPI document from API management portal. (To learn more from AsyncAPI see the videos from Dale Lane here)</li> </ul> <p>In this series, I propose to reuse our reactive messaging Quarkus producer and consumer code templates from the eda-quickstarts repository. This is not a step-by-step tutorial, but more a see journey of developing and deploying a basic solution.</p> <p>The following figure illustrates part of the above task, related to the components used in any event-driven solution:</p> <p></p> <p>Figure 3: a component view of a simple solution</p> <ul> <li>On the left, the first green box is a microservice that exposes REST APIs to be used by a single page webapp or a mobile app. The service produces records to Kafka topic to represent facts about the main business entity (Order Entity): OrderCreated, OrderUpdated, OrderDelivered, OrderCancelled\u2026</li> <li>A Kafka Broker cluster, in this series, I will use IBM Event Streams which is an open-source Kafka packaging with enhanced features.</li> <li>A schema registry, based on the apicur.io Red Hat led open-source project which is also included in IBM Event Streams.</li> <li>One or more consumers (green boxes on the right), can consume order events, using streaming processing. For the real-time inventory, I will do a simple Kafka Stream app as a consumer, and a stateful streaming processing with Apache Flink.</li> <li>Integrated API management (IBM API connect) to manage both REST endpoints and asyncAPI definitions. The AsyncAPI is defined for Kafka binding by connecting to the Event Stream Kafka Topic. It will work for any Kafka brokers.</li> <li>Event end-point gateway to control the communication between consumers and Kafka brokers, and an API gateway is controlling REST APIs traffic. I will not deploy OpenAPI gateway as there is already a lot of content available on this subject.</li> <li>S3 sink Kafka connector to move events to a long-term storage capability like S3 buckets. We will use IBM Cloud Object Storage in this demonstration.</li> <li>Elastic Search sink Kafka connector to move events to indices in Elastic Search.</li> <li>The open-source project, Apache Atlas, is used to keep metadata about brokers, topics, producers, streaming, and consumer apps.</li> </ul> <p>The numbers highlight some top-level developer's activities:</p> <ol> <li>Step 1. The producer application is defined with JAXRS, reactive messaging, and the OpenAPI document is created from JAXRS annotations, and then pushed to the API management tool. </li> <li>Step 2. When the application starts to be deployed, it produces events to the <code>orders</code> topic, the schemas defined for those messages are pushed to a schema registry at mostly the same time as the message is published. For the real-time inventory with we use items.</li> <li>Step 3.  To make this topic and schema governed and known, it is possible to connect IBM API Connect to the Kafka  topic to build the AsyncAPI documentation, and then add any metadata to it.</li> <li>Step 4. Now the developer of the consumer applications will get the list of topics and their metadata inside the developer Portal of the API management tool. He downloads the asyncAPI and gets the contract with Avro schemas. </li> <li>Step 5. When the application starts, it will consume from a topic, get the schema identifier from the record header, and will download the schema definition from the schema registry.</li> </ol> <p>The application is using Kafka API / reactive messaging to access Kafka brokers. But in fact, the URL is proxied by the event gateway service. This event gateway can enforce traffic and access policies.</p>"},{"location":"blogs/12-27-21/#from-domain-driven-design","title":"From Domain-Driven Design...","text":"<p>In real life project, the journey starts from an event storming workshop where architects, Subject Matter Experts, analysts and developers work together to discover the business process in scope with an event focus. They apply domain-driven design practices, to identify bounded contexts and context maps. As part of the solution design, a challenging architecture decision is to map bounded contexts to microservices. This is not a one-to-one mapping, but the classical approach is to manage a DDD aggregate in its own microservices. We have documented a set of DDD principles in this DDD article.</p> <p>So if we take the traditional order processing domain, we will discover events about the  Order business entity's life cycle and the Order entity information with its attached value objects.  The figure below presents some basic DDD elements: Commands are in blue rectangles,  Entity-aggregate in dark green, value-objects in light green, and events in orange.</p> <p></p> <p>Figure 4: from DDD to microservice</p> <p>The right side of the diagram presents a DDD approach to the application architecture, described in layers. We could have used the onion architecture, but the important development practice is to apply clear separation of concern principles and isolate the layers so for example code to produce messages are not in the business logic layer.</p> <p>Commands will help to define APIs and REST resources and maybe the service layer.  Root aggregate defines what will be persisted in the repository, but also what will be exposed via the APIs. In fact, it is  important to enforce avoiding designing a data model with a canonical model approach, as it will expose a complex data model at the API level, where we may need to have APIs designed for the service and the client contexts.  The old Data Transfer Object pattern should be used  to present a view of the complex data model.</p> <p>Events are defined with Avro schemas that are used in the messaging layer, schema registry, and AsyncAPI definition. </p>"},{"location":"blogs/12-27-21/#to-code-repositories-and-gitops","title":"... To code repositories and GitOps","text":"<p>Developer starts to create a code repository in its preferred Software Configuration Manager, for the real-time solution we use public gitHub.</p> <ul> <li>The store simulator application is a Quarkus app, which generates item sales (a simplified version of a TLOG transaction) to different possible messaging middlewares ( MQ or Kafka). If you want to browse the code, the main readme of this project includes how to package and run this app with docker compose, and explains how the code works.  The docker image is quay.io/ibmcase/eda-store-simulator/</li> <li>The item inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in the refarch-eda-item-inventory project.  Consider this more as a black box in the context of the scenario, it consumes items events, aggregates them, exposes APIs on top of Kafka Streams interactive queries, and publishes inventory events on <code>item.inventory</code> topic. </li> <li>The store inventory aggregator is a Kafka Stream Quarkus application, done with Kafka Stream API. The source code is in the refarch-eda-store-inventory project the output is in <code>store.inventory</code> topic. </li> </ul> <p>To support a GitOps approach for development and deployment, Red Hat has delivered two operators: </p> <ul> <li>Tekton / OpenShift Pipelines for continuous integration</li> <li>ArgoCD / OpenShift GitOps for continuous deployment. </li> </ul> <p>As part of the OpenShift GitOps, there is also the KAM CLI tool  to help developers to start on the good track for structuring the different deployment   configurations and ArgoCD app configurations. The Order GitOps repository was  created with kam. See the repository main readme for more details.</p> <p>The core idea of GitOps is to have a Git repository that always contains declarative descriptions  of the infrastructure currently desired in the production environment and an automated process  to make the production environment match the described state in the repository.</p> <p>To get the basic GitOps knowledge related to this article, I recommend reading the following documentations:</p> <ul> <li>Understand GitOps</li> <li>KAM presentation</li> </ul>"},{"location":"blogs/12-27-21/#part-2-create-the-producer-app","title":"Part 2: Create the producer app","text":"<p>As we use Quarkus for our implementation of event-driven, reactive microservice we can use the <code>quarkus cli</code> to create the application. It will be too long to go over everything in this blog and  the subject is well covered in Quarkus guides. The order management microservice readme file explains how the service was created.</p> <p>You can fork and clone this repo to try running the producer code locally with <code>quarkus dev</code>.</p> <p>The application created with quarkus CLI may be deployed to OpenShift, and Tekton pipeline can be defined to manage the continuous integration, as well as ArgoCD application can also be defined to deploy the application. </p> <p>The Order GitOps repository includes such elements:</p> <ul> <li>ArgoCD for continuous deployment of the application (dev-app-eda-demo-order-ms-app.yaml): config/argocd/ folder</li> <li>application deployment: `` folder</li> <li>service deployment</li> <li>Pipeline </li> </ul> <p>We will go over how to use and build those elements in next sections.</p>"},{"location":"blogs/12-27-21/#environment-setup","title":"Environment Setup","text":"<p>From the GitOps, developers have to define the different target environment and how to build and deploy each applications. First let define Event Streams Cluster and API Connect end point management.</p> <p>We assume, you have access to an OpenShift 4.7 or newer version cluster, we used cluster version 4.8. Login to your cluster.</p>"},{"location":"blogs/12-27-21/#install-pre-requisites","title":"Install pre-requisites","text":"<p>The first thing to do, is to install the different services / middleware operators, and then create one or more instance of those 'services'. I will combine Open Source and IBM products for this solution.  The products I'm using for the order microservices are:</p> <ul> <li>IBM Event Streams on OpenShift for Kafka</li> <li>IBM API Connect to manage API definitions</li> </ul> <p>We have defined a public github repository to define operator subscriptions with some examples of  cluster instances (operandes) for Event Streams, MQ and other IBM Automation products.  See the eda-gitops-catalog repository readme to get more information on the catalog.</p> <p>The GitOps repository for the solution will reuse the catalog.</p> <ul> <li> <p>Clone the Order GitOps repository</p> <pre><code>git clone https://github.com/jbcodeforce/eda-order-gitops\ncd eda-order-gitops\n</code></pre> <p>Then bootstrap GitOps and Pipelines, and the different operators needed for the solution. See the <code>how to use</code> explanations in the repository Readme if you want to run it.</p> </li> <li> <p>If not already done, install Quarkus CLI</p> <pre><code>curl -Ls https://sh.jbang.dev | bash -s - app install --fresh --force quarkus@quarkusio\n</code></pre> </li> <li> <p>Install KAM and put the  downloaded binary into a folder of your <code>$PATH</code></p> </li> <li> <p>If you are using an external image repository, get its secret to authenticate the <code>pipeline</code> service account  to push image on successful pipeline execution. For Quay.io see this note on how to create a Robot Account with Write permission. Below is an example of robot account for quay.io:</p> </li> </ul> <p></p> <p>Download the Kubernetes Secret definition to be defined in your cicd project.</p> <p></p>"},{"location":"blogs/12-27-21/#environment-up-and-running","title":"Environment up and running","text":"<p>The GitOps repository define two namespaces to run the solution, one for development, one for staging.  The ArgoCD project includes the following applications:</p>"},{"location":"blogs/12-27-21/#defining-the-api-from-jaxrs-resource-and-openapi-annotation","title":"Defining the API from JAXRS Resource and OpenAPI annotation","text":"<p>API can be defined top down with a Swagger UI or API Connect. Applying test driven development, we develop the business entities while implementing the requirements, and expose data transfer objects during integration tests. This leads to have the OpenAPI definition evolving over time before a specific synchronization point with the first consumers of the API. Contract testing is used too.  So having a bottom-up definition of the OpenAPI document makes a lot of sense in this context. Which means that we may export the OpenAPI document from code execution and then keep a first version in the project git repository under <code>src/main/apis</code> folder.</p>"},{"location":"blogs/12-27-21/#push-the-openapi-document-to-api-management","title":"Push the OpenAPI document to API management","text":""},{"location":"blogs/12-27-21/#manage-asyncapi-from-kafka-topic","title":"Manage AsyncAPI from Kafka Topic","text":""},{"location":"blogs/12-27-21/#part-3-develop-a-consumer-app","title":"Part 3: Develop a consumer app","text":""},{"location":"blogs/12-27-21/#clean-your-gitops-environment","title":"Clean your gitops environment","text":"<ul> <li>Delete the pipeline custom resources: see this note</li> </ul>"},{"location":"blogs/12-27-21/#related-product-documentation","title":"Related product documentation","text":"<ul> <li>Cloud Native PostgreSQL Operator</li> </ul>"},{"location":"blogs/12-27-21/steps-to-dev-ms/","title":"Steps to develop an event driven microservices with Quarkus","text":"<ul> <li>Add all needed extensions</li> </ul> <pre><code>quarkus ext add reactive-messaging-kafka,reactive-mq,hibernate-reactive-panache,reactive-pg-client,rest-client-jackson\n</code></pre> <ul> <li>Define a docker compose with mq, redpanda, postgresql</li> </ul> <pre><code>version: '3.1'\nservices:\n  postgresql:\n    container_name: postgres\n    hostname: postgres\n    image:  docker.io/bitnami/postgresql:13.2.0-debian-10-r11\n    environment:\n      POSTGRESQL_USERNAME: postgres\n      POSTGRESQL_PASSWORD: pgpwd\n      POSTGRESQL_DATABASE: loandb\n      BITNAMI_DEBUG: \"false\"\n      ALLOW_EMPTY_PASSWORD: \"yes\"\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - ./data:/bitnami/postgresql\n  pgadmin:\n    image: dpage/pgadmin4\n    hostname: pgadmin\n    container_name: pgadmin\n    ports:\n      - 8082:80\n    environment:\n      PGADMIN_DEFAULT_EMAIL: admin@domain.com\n      PGADMIN_DEFAULT_PASSWORD: alongpassw0rd\n  ibmmq:\n    image: ibmcom/mq\n    ports:\n        - '1414:1414'\n        - '9443:9443'\n        - '9157:9157'\n    volumes:\n        - qm1data:/mnt/mqm\n    stdin_open: true\n    tty: true\n    restart: always\n    environment:\n        LICENSE: accept\n        MQ_QMGR_NAME: QM1\n        MQ_APP_PASSWORD: passw0rd\n        MQ_ENABLE_METRICS: \"true\"\n  redpanda:\n    command:\n    - redpanda\n    - start\n    - --smp\n    - '1'\n    - --reserve-memory\n    - 0M\n    - --overprovisioned\n    - --node-id\n    - '0'\n    - --kafka-addr\n    - PLAINTEXT://0.0.0.0:29092,OUTSIDE://0.0.0.0:9092\n    - --advertise-kafka-addr\n    - PLAINTEXT://redpanda:29092,OUTSIDE://localhost:9092\n    # NOTE: Please use the latest version here!\n    image: docker.vectorized.io/vectorized/redpanda:v21.7.6\n    container_name: redpanda-1\n    ports:\n    - 9092:9092\n    - 29092:29092\nvolumes:\n  qm1data:\n</code></pre> <ul> <li>Start the compose: <code>docker compose up</code></li> <li>Open Postgresql Admin console from URL: http://localhost:8082/browser/</li> <li> <p>Define a new server with the name <code>local-dev-Server</code> and the connection property matching the environment settings in the  compose file. The hostname = postgres and port 5432 and not localhost, as the console runs in the docker network</p> <pre><code>POSTGRESQL_USERNAME: postgres\nPOSTGRESQL_PASSWORD: pgpwd\nPOSTGRESQL_DATABASE: loandb\n</code></pre> </li> <li> <p>Add a business entity like an Order or a LoanApplication to be a PanacheEntity. See this guide to refresh memory.</p> <pre><code>@Entity\npublic class LoanApplication extends PanacheEntity{\n    public String loanApplicationId;\n    public String primaryApplicantName;\n    public String secondaryApplicantName;\n    public double loanAmount;\n    public String propertyType;\n    public String loanPurposeType;\n}\n</code></pre> </li> <li> <p>add <code>import.sql</code> in the <code>src/main/resources</code> to add one record to the table</p> </li> <li>restart the application <code>quarkus dev</code></li> <li>Within the PGAdmin console, verify the record is created in database using the option <code>scripts &gt; SELECT scripts</code> at the table level, and then run it.</li> <li>Add one resource class and define basic APIs</li> </ul> <pre><code>@Path(\"/api/v1/loans\")\n@ApplicationScoped\n@Produces(MediaType.APPLICATION_JSON)\n@Consumes(MediaType.APPLICATION_JSON)\npublic class LoanApplicationResource {\n\n    @ConfigProperty(name=\"app.version\")\n    public String version;\n\n    @GET\n    @Path(\"/version\")\n    public String getVersion(){\n        return \"{ \\\"version\\\": \\\"\" + version + \"\\\"}\";\n    }\n\n    @GET\n    public Uni&lt;LoanApplication&gt;&gt; getAll(){\n        return LoanApplication.listAll());\n    }\n}\n</code></pre> <ul> <li>Install OpenShift Pipeline and OpenShift Gitops operators, can be done via the OpenShift Console, Operator Hub or with CLI.</li> </ul> <pre><code> # for OpenShift 4.7+\n # GitOps for solution deployment\n oc apply -k ./openshift-gitops/operators/overlays/stable\n # and Pipeline for building solution\n oc apply -k ./openshift-pipelines-operator/overlays/stable\n</code></pre> <ul> <li>Create a gitops repo</li> </ul> <pre><code>kam bootstrap \\\n    --service-repo-url https://github.com/jbcodeforce/loan-origin-cmd-ms \\\n    --gitops-repo-url  https://github.com/jbcodeforce/loan-origin-gitops \\\n    --image-repo quay.io/jbcodforce/loan-origin-cmd-ms \\\n    --git-host-access-token &lt;your-github-token&gt; \\\n    --prefix los --push-to-git=true\n</code></pre> <ul> <li>In the -gitops project, add a <code>bootstrap</code> folder and add an <code>ArgoProject</code> definition so we will not use  the <code>default</code> project. See example in this project.</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: rt-inventory\n</code></pre> <p><code>oc apply -f argo-project.yaml</code></p> <ul> <li>Get Argo CD Console URL and admin user password</li> </ul> <pre><code>oc get route openshift-gitops-server -n openshift-gitops\n# Get password\noc extract secret/openshift-gitops-cluster -n openshift-gitops --to=-\n</code></pre> <ul> <li>Bootstrap continuous deployment with ArgoCD</li> </ul> <pre><code>oc apply -k config/argocd/\n</code></pre> <p>At least 5 ArgoCD applications will be created</p> <pre><code>application.argoproj.io/rt-argo-app created\napplication.argoproj.io/rt-cicd-app created\napplication.argoproj.io/rt-inventory-dev-app-refarch-eda-store-simulator created\napplication.argoproj.io/rt-inventory-dev-env created\napplication.argoproj.io/rt-inventory-dev-services created\n</code></pre> <ul> <li>create sealed secret</li> <li>Define secrets</li> <li>create deployment</li> <li>Tune deployment for the application</li> <li> <p>add privileged scc to pipeline sa</p> </li> <li> <p>add webhook in each microservice git repo - use the secret </p> </li> <li>create openapi doc</li> <li>create asyncAPI doc</li> </ul>"},{"location":"coding/agile/","title":"Embracing Agile","text":"<p>quick summary of agile practices</p>"},{"location":"coding/agile/#definitions","title":"Definitions","text":""},{"location":"coding/agile/#product-backlog","title":"Product backlog","text":"<ul> <li>Features we want to implement but not yet prioritized for the release</li> <li>Often decided by the product manager/offering manager</li> </ul>"},{"location":"coding/agile/#release-backlog","title":"Release backlog","text":"<ul> <li>Features that we want to implement for a particular release</li> <li>Offering management along with engineering team collaborates to set the scope of a release</li> </ul>"},{"location":"coding/agile/#sprint-backlog","title":"Sprint backlog","text":"<ul> <li>User stories, specific tasks that need to be completed during a specific period of time</li> <li>Decided by the development team depending on the velocity and capacity</li> </ul>"},{"location":"coding/agile/#epic","title":"Epic","text":"<ul> <li>Large body of work that can be broken down to a number of smaller stories</li> <li>Spanning across multiple sprints</li> <li>Commitment tracked over a release</li> <li>Evolves based on customer feedback and development progress</li> <li>No Pull Requests are made against an Epic</li> </ul>"},{"location":"coding/agile/#user-stories","title":"User Stories","text":"<ul> <li>Informal, general explanation of a software feature/end goal written from the perspective of the end user or customer</li> <li>Spans across multiple days</li> <li>Commitment tracked over a sprint</li> <li>Does not change over a sprint</li> </ul>"},{"location":"coding/agile/#sub-tasks","title":"Sub tasks","text":"<ul> <li>A set of specific outcome focussed activities that contribute to a Story's progress</li> <li>Spans across a few hours to a couple of days</li> <li>Commitment tracked over days</li> <li>Does not change over a sprint </li> </ul>"},{"location":"coding/agile/#definition-of-ready","title":"Definition of ready","text":"<p>The criteria to move from new-issue to backlog:</p> <ul> <li>requirement is well defined in the user story following INVEST criteria</li> <li>Dev lead and dev team must have talked about the story at least once</li> <li>A least one acceptance criterio is explicitly defined</li> <li>Story is sized and prioritized</li> <li>Story must be brokent down enough to fit single sprint</li> </ul>"},{"location":"coding/agile/#definition-of-done","title":"Definition of Done","text":"<p>Describes the requirements that must be met in order for a story to move from In Progress/Review/QA to Done/Closed</p> <p>Criteria</p> <ul> <li>Code is refactored and reviewed</li> <li>Code is integrated with master/main branch</li> <li>Automated tests (UT+FVT) are written and all tests are green</li> <li>Documentation changes are incorporated</li> </ul>"},{"location":"coding/agile/#ceremonies","title":"Ceremonies","text":""},{"location":"coding/agile/#zenhub-practices","title":"Zenhub practices","text":"Objectives Best Practice Backlog Grooming * New Issues are created/reviewed based on OM/Customer inputs * 'Move' the issues under 'New Issues' to 'Backlog' observing DoR * 'Move' the issues under 'Backlog' to 'Sprint Backlog' upon prioritization * Appropriate 'tags' are added for easy classification/tracking Backlog' Actions * Issues are visually ordered according to the priority (Highest priority always at the top) * Prioritized issues are tagged to the targetted 'Release' Sprint Backlog' Actions * Only those issues are present that are planned for the current sprint * Team commits the Sprint Backlog according to the capacity * Each issue is tagged to the associated 'Milestone' (Sprint 1, Sprint 2 etc.) * Each issue has potentially identified an owner(s) In Progress' Actions * 'Move' the issues from 'Sprint Backlog' to 'In Progress' once the team start working on the particular story * Only those issues are present that are actively worked upon by an individual (and vice versa) * Has at least one owner * Regular updates on the progress made in 'Comments' section of the issue QA/Review' Actions * 'Move' the issues from 'In Progress' to 'QA/Review' to indicate the development completion * DoD is observed before marking as 'Done'/'Closed' * Regular updates on the progress made in 'Comments' section of the issue Reporting Defects * The defect description follows the defect template * Severity/Priority is rightly identified and updated in the github issue * Targetted release is identified and updated * Ensure appropriate engineering priority is observed for addressing the defect Daily Scrum * Issues \"In Progress\" are iterated through * Respective owner of the issue updates the progress, potential blockers, need for help explicitily * Blockers, Need for Help etc. which require extensive discussion are parked and taken up after the scrum"},{"location":"coding/argocd/","title":"ArgoCD tutorial","text":"<p>To implement GitOps workflow, we used Argo CD, the GitOps continuous delivery tool for Kubernetes which also names OpenShift Gitops on OpenShift.  Argo CD models a collection of applications as a project and uses a Git repository to store the application's desired state (a gitops repo).  Argo CD compares the actual state of the application in the cluster with the desired state defined in Git and determines if they are out of sync. When it detects the environment is out of sync, Argo CD can be configured to either send out a notification to kick off a separate reconciliation process or Argo CD  can automatically synchronize the environments to ensure they match.</p> <p>ArgoCD is deployed with OpenShift GitOps operator. </p> <p>If you go to the Developer's Perspective, you can see a topology like:</p> <p></p> <p>Clicking the Argo Server node that contains the URL takes you to the Argo login page.</p> <p>See this getting started tutorial  and the core concepts.</p>"},{"location":"coding/argocd/#installation","title":"Installation","text":"<p>Installing ArgoCD will include CRD, service account, RBAC policies, config maps, secret and  will deploy <code>Redis</code> and <code>argocd server</code>. It makes sense to have one ArgoCD operator deployed per cluster.  It can manage projects and within project, applications.</p> <p>The better approach for installation is to use the Red Hat OpenShift GitOps operator. </p> <pre><code># Example of operator installation from https://github.com/jbcodeforce/eda-gitops-catalog\noc apply -k openshift-gitops/operator/overlays/stable\n</code></pre> <p>After installing the OpenShift GitOps operator, an instance of Argo CD Operator is installed  in the <code>openshift-gitops</code> namespace which has all needed privileges  to manage cluster configurations and application deployments. The following pods run:</p> <pre><code>argocd-application-controller-0                                   1/1     Running     0          20h\nargocd-dex-server-9dc558f5-4dw4q                                  1/1     Running     2          20h\nargocd-redis-759b6bc7f4-g2jbj                                     1/1     Running     0          20h\nargocd-repo-server-5fbf484547-6x4rj                               1/1     Running     0          20h\nargocd-server-6d4678f7f6-vqs64  \n</code></pre>"},{"location":"coding/argocd/#argo-cd-console","title":"Argo CD console","text":"<p>To get the <code>admin</code> user's password, get it from the secret</p> <pre><code>oc extract secret/openshift-gitops-cluster -n openshift-gitops --to=-\n</code></pre> <p>A route is created to the ArgoCD server:</p> <pre><code>oc get routes -n openshift-gitops\n</code></pre> <p>Within the console we can create application, referencing git repositories, and to kubernetes clusters.</p>"},{"location":"coding/argocd/#prepare-an-application","title":"Prepare an application","text":"<p>In Argo CD terms, each deployable component is an application and applications are grouped  into projects. Projects are not required for ArgoCD to be able to deploy applications,  but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project</p> <p>One ArgoCD server can deploy applications to multiple k8s clusters.</p>"},{"location":"coding/argocd/#deploy-from-the-ui","title":"Deploy from the UI","text":"<p>Follow the Getting started tutorial, but  basically the idea is to define an application, that gets information from git repository to deploy something to k8s as manifests. The configuration specifies what to monitor from the Git and where to deploy the \"thing\".</p> <p>The thing will be a <code>kustomization</code> to specify an application deployment.yaml, services, configmap...  (It can also use an Helm chart and values files).</p>"},{"location":"coding/argocd/#deploy-using-argocd-cli","title":"Deploy using argocd CLI","text":"<ul> <li>Install argocd CLI</li> </ul> <p>On MAC: <code>brew install argocd</code></p> <ul> <li>If ArgoCD was installed manually, expose the API server: by default, the Argo CD API server is not exposed with an external IP.  Update the service to use load balancer: </li> </ul> <pre><code>kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'\n</code></pre> <p>Get the IP address of the argocd server: <code>oc get svc</code> then look at the LoadBalancer service (argocd-server) external-IP.</p> <p><code>argocd login SERVERIP</code>  then use admin user and password.  You can also access the ArgoCD UI using the load balancer IP address, admin user and password. </p> <ul> <li>Define application: this is the deployable unit, which is map to a k8s deployment that references the image built during the CI pipeline. Each component, microservice or app is defined inits own folder and can use Helm or Kustomize to define the deployment, service, configmap... </li> </ul> <p>You use one ArgoCD application per target environment.</p>"},{"location":"coding/argocd/#deploy-from-yaml","title":"Deploy from Yaml","text":"<p>Here is an example of argoCD app descriptor:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: app-q-order-ms\n  annotations:\n    argocd.argoproj.io/sync-wave: \"300\"\n  labels:\n    gitops.tier.layer: applications\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  destination:\n    namespace: openshift-gitops\n    server: https://kubernetes.default.svc\n  project: applications\n  source:\n    path: quarkus-order-ms/config/dev\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>On the application tile, we can use the 'SYNC' to deploy the application or use <code>argocd app sync &lt;appname&gt;</code>.</p>"},{"location":"coding/argocd/#argocd-app-of-app","title":"ArgoCD app of app","text":"<p>The concept is to use a bootstrap application that will start other apps from gitops repository. (See video presentation here)</p>"},{"location":"coding/argocd/#connect-pipeline-to-deployment","title":"Connect pipeline to deployment","text":"<p>Once the manifests for a given app are defined in the gitops repository, we need to have the pipeline being able to update the deployment descriptor with the image reference and tag defined as part of the build pipeline.</p> <p>The pipeline needs to have the git credentials to be able to write to the gitops repository. Credentials are saved in a secret and a configmap define github host, user.</p>"},{"location":"coding/argocd/#more-reading","title":"More reading","text":"<ul> <li>ArgoCD documentation</li> </ul>"},{"location":"coding/finch/","title":"Open source Finch for container development","text":"<p>Finch is a an open source client for container development with nice integration with AWS.</p> <p>Architecture:</p> <p></p> <p>It does not share the local image registry of docker. And as <code>sam cli</code> has dependencies on docker v7.0 API, sam does not work yet with Finch.</p>"},{"location":"coding/finch/#value-propositions","title":"Value propositions","text":"<ul> <li>Use containerd (with nerdctl CLI), buildKit to support OCI image builds, and run within a VM managed by Lima.</li> <li>Replace the <code>docker cli</code>.</li> <li>Address that local environment does not match production architecture, like AWS Graviton. </li> <li>Support docker compose.</li> <li>Can emulate different linux architecture within the same vm.</li> </ul>"},{"location":"coding/finch/#quick-command-summary","title":"Quick command summary","text":"<ul> <li>once install, set up the underlying system</li> </ul> <pre><code>finch vm init \n</code></pre> <ul> <li>Reboot or restart the vm</li> </ul> <pre><code>finch vm start \n</code></pre> <ul> <li>Run an image</li> </ul> <pre><code>finch run \n</code></pre> <ul> <li>Select the architecture of the vm: (The --platform option can be used for builds as well)</li> </ul> <pre><code>finch run --platform linux/amd64 ubuntu /bin/bash\n</code></pre> <ul> <li>build an image</li> </ul> <pre><code>finch build -t jbcodeforce/python .\n</code></pre> <ul> <li></li> </ul>"},{"location":"coding/finch/#resources","title":"Resources","text":"<ul> <li>AWS Blog on Finch</li> </ul>"},{"location":"coding/git/","title":"Git summary","text":"<p>Git can convert any local system folder into a Git repository. Each machine or location is called a remote, in Git's terminology, each remote may have one or more branches.</p> <pre><code># git authentication specify user and email: jbcodeforce or I22Scode\ngit config --global user.name \"Your Name Here\"\n# change email address\ngit config --global user.email \"MY_NAME@example.com\"\n</code></pre> <p>When you clone a project, a complete copy of the original remote repository is created locally on your system. Your local copy of the repository contains the entire history of the project files, not just the latest version of project files.</p> <p>Files in your working directory can be in one of two states: tracked or untracked. Tracked files are files that were in the last snapshot; they can be unmodified, modified, or staged. Untracked files are everything else.</p> <p>In order to begin tracking a new file, you use the command <code>git add &lt;filename&gt;</code>. If you want to remove a file: <code>git rm &lt;filename&gt;</code></p> <p>To track file changes in Git, you create a series of project snapshots or commits.</p> <p>Git version control features a branching model to track code changes. A branch is a named reference to a particular sequence of commits.</p> <p>When doing a <code>git status</code>, git compares with the stage: files staged are under the \u201cChanges to be committed\u201d heading. File not staged are under Changes not stated for commit. Any changes done on a file after it was added to the stage, enforce doing a second add operation. </p> <p>All Git repositories have a base branch named main.</p> <p>By convention, the main branch in a Git repository contains the latest, stable version of the application source code. To implement a new feature or functionality, create a new branch from the main branch. This new branch, called a feature branch, contains commits corresponding to code changes for the new feature. </p> <p>When you use a branch for feature development, you can commit and share your code frequently without impacting the stability of code in the main branch. After ensuring the code in the feature branch is completed, tested, and reviewed, you are ready to merge the branch into another branch, such as the main branch. Merging is the process of combining the commit histories from two separate branches into a single branch.</p>"},{"location":"coding/git/#different-workflows","title":"Different workflows","text":"<p>A centralized Git workflow uses a central Git repository as the single source of record for application code. Developers push code changes directly to the main branch, and do not push commits in other branches to the central repository. Because the workflow results in commits to a single branch only, team members are prone to merge conflicts. It open doors to commit partial or incomplete code changes. Code is always on the most updated features so may be unstable.</p> <p>A feature branch workflow implements safety mechanisms to protect the stability of code on the main branch. The aim of this workflow is to always have deployable and stable code for every commit on the main branch, but still allow team members to develop and contribute new features to the project. We can add a second main trunk, <code>development</code> or <code>dev</code> to support coding for the next software release. And feature branches can be done from this development branch. </p> <p>In a feature branch workflow, each new feature is implemented in a dedicated branch. </p> <p>The Forked repository workflow is often used with large open source projects. With a large number of contributors, managing feature branches in a central repository is difficult. Additionally, the project owner may not want to allow contributors to create branches in the code repository. In this scenario, branch creation on the central repository is limited to a small number of team members. Once forked, clone your repo and add the upstream repository (the one you forked from).  <code>git remote add upstream &lt;url of the git repository source of your fork&gt;</code>. It's good practice to regularly sync your work with the upstream repository. To do this, you'll need to use Git on the command line:</p> <pre><code>git fetch upstream\nor\ngit merge upstream/main\n</code></pre> <p>In case the upstream get other Pull Requests, you can rebase to the latest upstream changes, resolving any conflicts. Commits to <code>main</code> will be stored in a local branch, <code>upstream/main</code>. Merge the changes from <code>upstream/main</code> into your local main branch. This brings your fork's main branch into sync with the upstream repository, without losing your local changes.</p> <pre><code># Be sure to be on your own main branch\ngit branch\ngit branch --all\n# if needed checkout your main \ngit checkout main\n# merge the changes\ngit merge upstream/main\n# \n</code></pre> <p>Create pull request against the integration branch of the upstream repository.</p>"},{"location":"coding/git/#commands-summary","title":"Commands summary","text":"<ul> <li>git config: Short for \u201cconfigure,\u201d this is most useful when you\u2019re setting up Git for the first time</li> </ul> <pre><code>git config --global user.name \"Your Name Here\"\n# change email address\ngit config --global user.email \"MY_NAME@example.com\"\n</code></pre> <ul> <li> <p>git init: Initializes a new Git repository. Until you run this command inside a repository or directory, it\u2019s just a regular folder. Only after you input this does it accept further Git commands.</p> </li> <li> <p>git add: This does not add new files to your repository. Instead, it brings new files to Git\u2019s attention. After you add files, they\u2019re included in Git\u2019s \u201csnapshots\u201d of the repository</p> </li> <li>git rm: remove from local working directory. If you want to keep the local file use the <code>git rm --cached</code> command</li> <li> <p>git status: Check the status of your repository. See which files are inside it, which changes still need to be committed, and which branch of the repository you\u2019re currently working on</p> </li> <li> <p>git commit: After you make any sort of change, you input this in order to take a \u201csnapshot\u201d of the repository. Usually it goes git commit -m \u201cMessage here.\u201d The -m indicates that the following section of the command should be read as a message. changing the last commit: If you want to try that commit again, you can run commit with the --amend option</p> </li> </ul> <pre><code>git commit -m 'initial commit'\ngit add forgotten_file\ngit commit --amend\n</code></pre> <p>This command takes your staging area and uses it for the commit. If you\u2019ve made no changes since your last commit (for instance, you run this command immediately after your previous commit), then your snapshot will look exactly the same, and all you\u2019ll change is your commit message. </p> <ul> <li>git branch: Working with multiple collaborators and want to make changes on your own? This command will let you build a new branch, or timeline of commits, of changes and file additions that are completely your own.</li> </ul> <pre><code># create a local branch from remote branch. So this one will be local branch with tracking\ngit checkout -b &lt;my-feature-dev&gt; integration\n# list the local branch\ngit branch   \n# Remote tracking branches can be viewed with \ngit branch -r\n# create a local tracking branch\ngit branch --track hello-kitty origin/hello-kitty\n# delete a branch on a remote machine:\ngit branch -rd origin/registration\n</code></pre> <p>Commits are done on the current checked out branch. If by any bad chance you have committed changes to the wrong branch, create the new branch, reset the master branch back to before these commits, then switch to the new branch and then commit:</p> <pre><code>git reset abc5b0de1 --hard\n</code></pre> <pre><code>MA --- MB --- MC --- FA --- FB --- FC &lt;- main\n\ngit checkout -b feature\n\nMA --- MB --- MC --- FA --- FB --- FC &lt;- feature\n                                    ^\n                                    |\n                                  main\n\ngit branch -f master MC\n\nMA --- MB --- MC --- FA --- FB --- FC &lt;- feature\n               ^\n               |\n             main\n</code></pre> <ul> <li>git checkout: Literally allows you to \u201ccheck out\u201d a repository that you are not currently inside. This is a navigational command that lets you move to the repository you want to check. You can use this command as <code>git checkout main</code> to look at the main branch. Un-modify a modified file by doing <code>git checkout -- &lt;filename&gt;</code></li> <li>git merge: When you\u2019re done working on a branch, you can merge your changes back to the main branch, which is visible to all collaborators.</li> <li>git diff to compare what is in your working directory with what is in your staging area</li> <li>git push: <code>git push  [remote-name] [branch-name]</code>  If you\u2019re working on your local host, and want your commits to be visible online on GitHub as well, you \u201cpush\u201d the changes up to GitHub with this command. <code>git push -u origin main</code>. If you and someone else clone at the same time and they push upstream and then you push up- stream, your push will rightly be rejected. You\u2019ll have to fetch their work first and incorporate it into yours before you\u2019ll be allowed to push.</li> <li> <p>git pull: If you\u2019re working on your local computer and want the most up-to-date version of your repository to work with, you \u201cpull\u201d the changes down from GitHub with this command. It is a shorthand for <code>git fetch</code> followed by <code>git merge FETCH_HEAD</code>.</p> </li> <li> <p>git fetch : <code>git fetch origin</code>, get update from a remote server branch and download to local repository. Not doing a merge. This will fetch any work that has been pushed, and download it to local repository without merging with your local work.</p> </li> <li>git remote: to work on the repositories: <code>git remote -v</code>: to get the list of remote repository/server part of the configuration. <code>get remote show origin</code> to get detail of the origin repo URL.</li> </ul> <pre><code># Change the url of repository\ngit remote set-url origin &lt;url&gt;\n</code></pre> <ul> <li><code>git log -10 --stat</code>: To see the last 10 commits done on main</li> <li>disable using ssl: <code>git config --global http.sslverify false</code></li> <li>In case the push did not return and hangs, it may be a problem of buffer size, then use <code>git config http.postBuffer 524288000</code>.</li> </ul>"},{"location":"coding/git/#practices","title":"Practices","text":"<ul> <li>Link a commit to an issue to close it. The commit message need to just include one of the following:</li> </ul> <pre><code>fix #xxx\nfixes #xxx\nfixed #xxx\nclose #xxx\ncloses #xxx\nclosed #xxx\nresolve #xxx\nresolves #xxx\nresolved #xxx\n</code></pre>"},{"location":"coding/git/#branching","title":"Branching","text":"<p>Branching means you diverge from the main line of development and continue to do work without messing with that main line. Git encourages a workflow that branches and merges often, even multiple times in a day.</p> <p>To create a branch and switch to it at the same time, you can run the git checkout command with the -b switch:</p> <pre><code># git branch creates a pointer to the last commit\ngit branch issue2\n# checkout move the HEAD to the branch\ngit checkout issue2\n#can be done with one command:\ngit checkout -b issue2\n</code></pre> <p>Any new commit will move the branch forward, because you have it checked out (that is, your HEAD is pointing to it). It\u2019s important to note that when you switch branches in Git, files in your working directory will change. If you switch to an older branch, your working directory will be reverted to look like it did the last time you committed on that branch. If Git cannot do it cleanly, it will not let you switch at all.</p> <p>Because a branch in Git is in actuality a simple file that contains the 40 character SHA-1 checksum of the commit it points to, branches are cheap to create and destroy.</p> <p>If you need to apply fix to existing main branch, do a <code>git checkout main</code> and the local working directory is exactly the way it was before you started working on the branch. Then add a new branch as hotfix, work on the code, and proceed to merge it back to the main branch using git merge.</p> <p>When you try to merge one commit with a commit that can be reached by following the first commit\u2019s history, Git simplifies things by moving the pointer forward because there is no divergent work to merge together \u2014 this is called a \"fast forward\".</p> <p>When the main branch and the fix branch are at the same level, you should delete the fix branch: <code>git branch -d hotfix</code></p> <p>Once the work on the branch is done, checkout to the target main branch:</p> <pre><code>git checkout main\ngit merge issue2\ngit push\n</code></pre> <p>When development history has diverged from some older point. Because the commit on the branch you\u2019re on isn\u2019t a direct ancestor of the branch you\u2019re merging in, Git has to do some work. In this case, Git does a simple three-way merge, using the two snapshots pointed to by the branch tips and the common ancestor of the two. Git creates a new snapshot that results from this three-way merge and automatically creates a new commit that points to it: this is a merge-commit and it has two parents. Git determines the best common ancestor to use for its merge base.</p> <p>When code change applies to the same source code, conflict may happen and the merge will not be automatic. <code>git status</code> helps to assess where the merge-commit stopped. Anything that has merge conflicts and hasn\u2019t been resolved is listed as <code>unmerged</code>.</p> <p>After you\u2019ve resolved each of these sections in each conflicted file, run git add on each file to mark it as resolved</p> <p>Stashing takes the dirty state of your working directory \u2014 that is, your modified tracked files and staged changes \u2014 and saves it on a stack of unfinished changes that you can reapply at any time.</p>"},{"location":"coding/git/#get-ssh-key-for-github-account","title":"Get SSH key for github account","text":"<p>Use OpenSSH client, which comes pre-installed on GNU/Linux, macOS, and Windows 10 to define public key. Existing ssh keys are under <code>.ssh/</code> folder. The file <code>id_rsa.pub</code> is the public key for RSA encrypted key.</p> <p>If needed generate key with:</p> <pre><code>ssh-keygen -t rsa -b 2048 -C \"&lt;comment&gt;\"\n</code></pre> <p>To debug the ssh authentication</p> <pre><code>ssh -vvvv git@ssh.gitlab.aws.dev\n</code></pre>"},{"location":"coding/git/#webhook","title":"Webhook","text":"<p>Webhooks help to get applications to subscribe to events on GitHub. When one of those events is triggered, github'll send a HTTP POST payload  to the webhook's configured URL.</p> <p>Webhooks can be installed on an organization, a specific repository. To set up a webhook, go to the settings page  of your repository or organization. From there, click Webhooks, then Add webhook.</p>"},{"location":"coding/git/#vs-code-git-plugin","title":"VS Code git plugin","text":"<p>Palette includes: git: clone, git... </p> <p>VS Code can initialize a folder as local git repository.</p> <p>VS Code handles the pull and push Git operations when you synchronize your local repository to the remote repository.</p> <p>The Source Control view compares your local repository with the corresponding remote repository. If there are commits to download from the remote repository, then the number of commits displays with a download arrow icon</p>"},{"location":"coding/git/#tags","title":"Tags","text":"<p>If we want to mark a specific point in our history as a particular version, that\u2019s what tags are for.</p> <p>Tag helps to create code release. In the github pages, tags are visible in the release folder.</p> <pre><code># create a local tag\ngit tag &lt;tagname&gt;\n# When pushing to your remote repo, tags are NOT included by default. You will need to explicitly say that you want to push your tags to your remote repo:\ngit push origin \u2014tags\n# or\ngit push origin &lt;tagname&gt;\n</code></pre>"},{"location":"coding/git/#removing-sensitive-data","title":"Removing sensitive data","text":"<p>Removing sensitive data from repo by using PGP and the filter approach from here removing-sensitive-data-from-a-repository</p> <pre><code>bgp -D mm2.properties\ngit reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive\n</code></pre> <p>Can also remove all the commit history using:</p> <pre><code>git checkout --orphan TEMP_BRANCH\ngit add -A\ngit  commit -am \"New initial commit\"\n# delete main\ngit branch -D main\n# rename temp branch to main\ngit branch -m main\ngit push -f origin main\n</code></pre>"},{"location":"coding/git/#more-reading","title":"More reading","text":"<ul> <li>Getting started</li> <li>Git Branching Rebasing</li> <li>Gitlab</li> <li>Eclipse EGit</li> </ul>"},{"location":"coding/gitops/","title":"GitOps","text":"<p>Gitops is a way of implementing Continuous Deployment for containerized applications.</p> <p>The core concept of GitOps is to maintain a single Git repository that consistently holds declarative descriptions of the desired infrastructure in the production environment. An automated process ensures that the production environment aligns with the state described in the repository.</p> <p>From an infrastructure perspective, we aim to govern the infrastructure effectively and ensure that what is expected aligns with what actually occurs.</p>"},{"location":"coding/gitops/#requirements","title":"Requirements","text":"<p>Developer and operation teams want to:</p> <ul> <li>Audit all changes made to pipelines, infrastructure, and application configuration.</li> <li>Roll forward/backward to desired state in case of issues.</li> <li>Consistently configure all deployment environments.</li> <li>Reduce manual effort by automating application and environment setup and remediation.</li> <li>Have an easy way to manage application and infrastructure state across clusters/environments</li> </ul> <p>GitOps is a natural evolution of DevOps and Infrastructure-as-Code.</p>"},{"location":"coding/gitops/#core-principles","title":"Core principles","text":"<ul> <li>Git is the source of truth for both application code, application configuration, dependant service/product deployments, infrastructure config and deployment.</li> <li>The management operations are splitted into Day 1 and Day 2 operations. Day 1 to install GipOps pipeline, Day2 to add more applications.</li> <li>Separate application source code (Java/Go) from deployment manifests i.e the application source code and the GitOps configuration reside in separate git repositories.</li> <li>Deployment manifests are standard Kubernetes (k8s) manifests i.e Kubernetes manifests in the GitOps repository can be simply applied with nothing more than a <code>oc apply</code> or <code>kubectl apply</code>.</li> <li>Kustomize.io is used as template engine for defining the differences between environments i.e reusable parameters with extra resources described using <code>kustomization.yaml</code>.</li> <li>Minimize yaml duplication - no copy/paste</li> <li>Support two axis of configuration: clusters and environments: prod, test, dev. (Accept separating production repo if organization is willing to do so).</li> <li>Prefer a multi-folder and/or multi-repo structure over multi-branch. Avoid dev or test configuration in different branches.  In a microservices world, a one branch per environment will quickly lead to an explosion of branches  which again becomes difficult and cumbersome to maintain.</li> <li>Minimize specific gitops tool dependencies: (Try to converge to Tekton and ArgoCD)</li> <li> <p>Do not put independent applications or applications managed by different teams in the same repo. </p> </li> <li> <p>Day 1 Operations are actions that users take to bootstrap a GitOps  configuration and on how to set up GitOps and Sealed Secrets.</p> </li> <li>Day 2 Operations are actions that users take to change a GitOps system.</li> </ul> <p>Using continuous delivery approach and tool like ArgoCD, the deployment of all the components is controlled  by the tool from the Gitops repositories.</p>"},{"location":"coding/gitops/#supporting-tools","title":"Supporting tools","text":"<ul> <li>KAM CLI from the Red Hat gitops team </li> <li>Tekton for continuous integration and even deployment</li> <li>ArgoCD | Openshift Gitops for continuous deployment</li> <li>Kustomize.io to templatize the deployment and support overlay mechanisms.</li> <li>git Action for continuous execution of development workflows right in your repository</li> </ul>"},{"location":"coding/gitops/#git-action","title":"Git Action","text":"<p>Git Action</p> <p>Configure github SECRETS for each project:</p> <ul> <li>DOCKER_REGISTRY  quay.io</li> <li>DOCKER_REPOSITORY jbcodeforce</li> <li>DOCKER_USERNAME  jbcodeforce</li> <li>DOCKER_PASSWORD</li> <li>DOCKER_IMAGE_NAME  the name of the application</li> </ul> <p>Define a <code>.github/workflow/dockerbuild.yaml</code> to declare how to build the app.</p>"},{"location":"coding/gitops/#openshift-gitops","title":"OpenShift GitOps","text":"<p>OpenShift GitOps is the operator and configuration to run ArgoCD, as a controller inside OpenShift.</p> <p>The environment configuration repository defines the desired state of the application/solution.</p> <p>Using OpenShift console &gt; Operator Hub, search for GitOps and select Red Hat OpenShift GitOps operator:</p> <p></p> <p>Then click install and use the default parameters.</p> <p>To do it via oc CLI, use the eda-gitops-catalog gitops repository and the command:</p> <pre><code>oc apply -k openshift-gitops/operator/overlays/\n</code></pre> <p>It can take some minutes to get it installed.</p> <pre><code>oc describe operator openshift-gitops-operator\n</code></pre>"},{"location":"coding/gitops/#proposed-git-repository-structure","title":"Proposed git repository structure","text":"<p>There are different ways to organize projects: The KAM tool proposes to create one gitops repository to control the configuration and deployment of each services and apps of the solution.</p> <p>Another solution is to use a three repositories structure, that will match team structure and persona: dev, SRE.</p> <ul> <li>application: deployment.yaml, config map... for each application. Developers lead this repository.</li> <li>shared, reusable services like Kafka, Database, LDAP,... as reusable services between environments: Dev and operations ownership</li> <li>Cluster and infrastructure: network, cluster, storage, policies... owned by operation</li> </ul> <p>With this three level structure, each \"solution\" will have 3 separate repositories:</p> <ul> <li>solution_name-gitops-apps</li> <li>solution_name-gitops-services</li> <li>solution_name-gitops-infra</li> </ul> <p>Now the different deployment environments can be using different k8s clusters or the same cluster with different namespaces.</p> <p>With the adoption of ArgoCD, developer can have one bootstrap app that starts other apps and monitor each of those layers.</p> <p>See detail in this separate note.</p>"},{"location":"coding/gitops/#openshift-projects","title":"OpenShift Projects","text":"<p>For each solution we may use a number of different projects: </p> <ul> <li>one solutionname-cicd for Tekton pipeline run... </li> <li>one solutionname-dev for the application instances in a development mode.</li> <li>one solutionname-staging for pre-production tests</li> <li>one solutionname-prod for production deployment. This can be in the same OCP cluster or in different one.</li> </ul> <p>Within the <code>-dev</code> project we can isolate the dependant applications, like Kafka cluster, or postgresql, where with Staging and production we can use multi-tenant deployment of such applications.</p> <p>The KAM tool supports this approach.</p>"},{"location":"coding/gitops/#services","title":"Services","text":"<p>For the Service level, try to adopt catalog repositories to hold common elements that will be  re-used across teams and across other repositories.  See example in Red Hat Canada Catalog git repo and our Business automation or EDA catalogs. </p> <p>Use Kustomization to reference remote repositories as a base and then patch it  as needed to meet your specific requirements.</p> <pre><code>bases:\n - github.com/ibm-cloud-architecture/eda-gitops-catalog/kafka-strimzi/operator/overlay/stable?ref=main\n - ../base\n</code></pre> <p>Reference the common repository via a tag or commit ID.</p>"},{"location":"coding/gitops/#application","title":"Application","text":"<p>For Application repositories, align your repositories along application and team boundaries: one gitops repo per application, different folders for microservices and deployable components. </p> <p>Use bootstrap folder to load the application components into the cluster: this will be an ArdoCD app.</p> <p>Use Overlay for the different app configuration depending of the target environment.</p> <p>Dedicated a folder (tekton) for pipelines, tasks.</p> <p>If you find you are duplicating argoCD manifests in the bootstrap folder, then create a separate high  level <code>argocd</code> folder.</p> <ul> <li>Cluster Configuration. This repo shows how Gerald Nunn configures OpenShift clusters using GitOps with ArgoCD.</li> <li>Product Catalog example of a 3 tier app with ArgoCD, tekton and kustomize. (From Gerald Nunn )</li> <li>real time inventory demo gitops</li> <li>ADS GitOps example for a risk scoring app</li> </ul>"},{"location":"coding/gitops/#process","title":"Process","text":"<p>A key question in any gitops scenario is, how to manage promotion of changes in manifests between different environments and clusters. </p> <p>Some principles to observe:</p> <ul> <li>Every change in the manifests needs to flow through the environments in hierarchical order, i.e. (dev &gt; test &gt; prod)</li> <li>We do not want a change to the base flowing to all environments simultaneously</li> <li>Tie specific environments to specific revisions so that changes in the repo can be promoted in a controlled manner We can manage the revision in the gitops tools or in kustomize or both.</li> </ul>"},{"location":"coding/gitops/#the-pre-and-post-synch-processing","title":"The pre and post synch processing","text":"<p>When Argo CD starts a sync, it orders the resources in the following precedence:</p> <ul> <li>The phase (See resource hook doc)</li> <li> <p>The wave they are in (lower values first). Defined as annotations in the different resources </p> <pre><code>metadata:\n  annotations:\n    argocd.argoproj.io/sync-wave: \"5\"\n</code></pre> </li> <li> <p>By kind (e.g. namespaces first)</p> </li> <li>By name</li> </ul> <p>It then determines the number of the next wave to apply. This is the first number where any resource is out-of-sync or unhealthy.</p>"},{"location":"coding/gitops/#openshift-pipelines","title":"OpenShift Pipelines","text":"<p>OpenShift Pipelines is a Continuous Integration / Continuous Delivery (CI/CD) solution based on the open source Tekton project.  The key objective of Tekton is to enable development teams to quickly create pipelines of activity from simple, repeatable steps. A unique characteristic of Tekton that differentiates it from previous CI/CD solutions is that Tekton steps execute within  a container that is specifically created just for that task.</p> <p>Users can interact with OpenShift Pipelines using the web user interface, command line interface, and via a Visual Studio Code editor plugin.  The command line access is a mixture of the OpenShift <code>oc</code> command line utility  and the <code>tkn</code> command line for specific Tekton commands.  The <code>tkn</code> and <code>oc</code> command line utilities can be downloaded from the OpenShift console web user interface.  To do this, simply press the white circle containing a black question mark near your name on the top right  corner and then select Command Line Tools:</p> <p></p> <p>See more detail in this Tekton summary</p>"},{"location":"coding/gitops/#kustomize-and-gitops","title":"Kustomize and gitops","text":"<p>There are two ways to implement the deployment strategy for GitOps: </p> <ul> <li>Push-based: use CI/CD tools like Jenkins, Tekton, Travis... to define a pipeline, triggered when application  code source is updated. Pipelines build the container image and deploy the modified yaml files to the  GitOps repo. Changes to the environment configuration repository trigger the deployment pipeline.   It has to be used for running an automated provisioning of cloud infrastructure. See also this k8s engine 'cloud build' tutorial.</li> <li>Pull-based: An operator takes over the role of the pipeline by continuously comparing the  desired state in the environment repository with the actual state in the deployed infrastructure.</li> </ul> <p>A CI/CD based on git action will build the image and edit Kustomize patch to bump the expected container  tag using the new docker image tag, then will commit this changes to the gitops repo.</p> <p>Kustomize is used to simplify the configuration of application and environment. Kustomize traverses a Kubernetes manifest to add, remove or update configuration options without forking.  It is available both as a standalone binary and as a native feature of Kubectl.  A lot of Kustomize examples here..</p> <p>The simple way to organize the configuration is to use one <code>kustomize</code> folder, then one folder  per component, then one overlay folder in which environment folder include <code>kustomization.yaml</code>  file with patches.</p> <pre><code>\u2514\u2500\u2500 postgres\n    \u251c\u2500\u2500 base\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 configmap.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 pvc.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 secret.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 service-account.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 statefulset.yaml\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 svc-headless.yaml\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 svc.yaml\n    \u251c\u2500\u2500 kustomization.yaml\n    \u2514\u2500\u2500 overlays\n        \u2514\u2500\u2500 dev\n            \u251c\u2500\u2500 kustomization.yaml\n            \u2514\u2500\u2500 sealedsecret.yaml\n</code></pre> <p>Here is an example of <code>kustomization.yaml</code>:</p> <pre><code>bases:\n  - ../../base\npatchesStrategicMerge:\n  - ./secret.yaml\n</code></pre> <p>The <code>patchesStrategicMerge</code> lists the resource configuration YAML files that you want to merge  to the base kustomization. You must also add these files to the same repo as the kustomization file,  such as <code>overlay/dev</code>. These resource configuration files can contain small changes that are merged  to the base configuration files of the same name as a patch.</p> <p>In GitOps, the pipeline does not finish with something like <code>oc apply..</code>. but it\u2019s an external tool  (Argo CD or Flux) that detects the drift in the Git repository and will run these commands.</p>"},{"location":"coding/gitops/#future-readings","title":"Future readings","text":"<ul> <li>GitOps - Operations by Pull Request</li> </ul>"},{"location":"coding/gitops/#kam-gitops-application-manager-cli","title":"KAM - Gitops Application Manager CLI","text":"<p>KAM's goal is to help creating a GitOps project for an existing application as  day 1 operations and then add more services as part of <code>day 2 operation</code>.</p> <p>GitOps approach is to manage deployments into one or more environments (a namespace in a Kubernetes).  One or more applications can be deployed into a given environment. An application is an aggregation of one or more services. The source code for each service (or microservice) is contained within a single Git repository</p> <ul> <li>To install download the last release from github: https://github.com/redhat-developer/kam/releases/latest.  Rename the download file, and move it to <code>/usr/local/bin</code>.</li> </ul>"},{"location":"coding/gitops/#pre-requisites","title":"Pre-requisites","text":"<p>Have the OpenShift GitOps and OpenShift Pipelines operators installed.</p> <p>Be connected to the cluster.</p>"},{"location":"coding/gitops/#creating-a-gitops-project-for-a-given-solution","title":"Creating a GitOps project for a given solution","text":"<p>Use the following <code>kam</code> command to create a gitops repository with multiple ArgoCD applications. To run successfully, we need to be connected to OpenShift cluster with Sealed Secret, GitOps and Pipelines operators deployed, get SSH key from github if repositories are used:</p> <pre><code># list the  available options\nkam bootstrap -- \n# create a gitops repo by specifying argument\nkam bootstrap \\\n--service-repo-url https://github.com/jbcodeforce/refarch-eda-store-inventory \\\n--gitops-repo-url  https://github.com/jbcodeforce/rt-inventory-gitops \\\n--image-repo image-registry.openshift-image-registry.svc:5000/ibmcase/store-aggregator \\\n--output rt-inventory-gitops \\\n--git-host-access-token &lt;a-github-token&gt; \\\n--prefix rt-inventory --push-to-git=true\n</code></pre> <p>To retrieve the github access token see this note.</p> <p>Kam bug: the application name is matching the git repo name, and there will be an issue while creating binding with the limit of  the number of characters. kam bootstrap needs a --service-name argument.</p> <p>This will create a GitOps project, push to github.com as private repo. The repo includes two main folders and a  <code>pipelines.yaml</code> describing your first application, and the configuration for a complete CI pipeline and deployments from Argo CD.</p> <p>At the same level of the folder hierarchy, there is a secrets folder to keep secrets to do not commit to Git. See secrets section below.</p>"},{"location":"coding/gitops/#what-is-inside","title":"What is inside","text":""},{"location":"coding/gitops/#config","title":"Config","text":"<p>The <code>config</code> folder defines <code>argocd</code> and <code>cicd</code> Argo applications to support deployment and pipeline definitions:</p> <pre><code>  \u251c\u2500\u2500 config\n  \u2502   \u251c\u2500\u2500 argocd\n  \u2502   \u2502   \u251c\u2500\u2500 argo-app.yaml\n  \u2502   \u2502   \u251c\u2500\u2500 cicd-app.yaml\n  \u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml\n  \u2502   \u2502   \u251c\u2500\u2500 rt-inventory-dev-app-refarch-eda-store-inventory-app.yaml\n  \u2502   \u2502   \u251c\u2500\u2500 rt-inventory-dev-env-app.yaml\n  \u2502   \u2502   \u2514\u2500\u2500 rt-inventory-stage-env-app.yaml\n  \u2502   \u2514\u2500\u2500 rt-inventory-cicd\n  \u2502       \u251c\u2500\u2500 base\n  \u2502       \u2502   \u251c\u2500\u2500 01-namespaces\n  \u2502       \u2502   \u2502   \u2514\u2500\u2500 cicd-environment.yaml\n  \u2502       \u2502   \u251c\u2500\u2500 02-rolebindings\n  \u2502       \u2502   \u2502   \u251c\u2500\u2500 argocd-admin.yaml\n  \u2502       \u2502   \u2502   \u251c\u2500\u2500 pipeline-service-account.yaml\n  \u2502       \u2502   \u2502   \u251c\u2500\u2500 pipeline-service-role.yaml\n  \u2502       \u2502   \u2502   \u2514\u2500\u2500 pipeline-service-rolebinding.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 03-tasks\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 deploy-from-source-task.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 set-commit-status-task.yaml\n  \u2502 \u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 04-pipelines\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 app-ci-pipeline.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ci-dryrun-from-push-pipeline.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 05-bindings\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 github-push-binding.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 rt-inventory-dev-app-refarch-eda-store-simulator\n                  \u2514\u2500\u2500 refarch-eda-store-simulator-binding.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 06-templates\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 app-ci-build-from-push-template.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ci-dryrun-from-push-template.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 07-eventlisteners\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 cicd-event-listener.yaml\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 08-routes\n  \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 gitops-webhook-event-listener.yaml\n  \u2502       \u2502   \u2514\u2500\u2500 kustomization.yaml\n  \u2502       \u2514\u2500\u2500 overlays\n  \u2502           \u2514\u2500\u2500 kustomization.yaml\n</code></pre> <p>The major elements of this configuration are:</p> <ul> <li>argo-app: root application to manage the other applications: It points to the kustomization.yaml under <code>config/argocd</code> so when adding new microservice to the solution using <code>kam cli</code> then this kustomize will be updated triggering a new argo app to be created  <ul> <li>argo-app.yaml</li> <li>cicd-app.yaml</li> <li>rt-inventory-dev-app-refarch-eda-store-simulator-app.yaml</li> <li>rt-inventory-dev-env-app.yaml</li> <li>rt-inventory-stage-env-app.yaml</li> </ul> </li> <li>cicd-app: Argo app to monitor <code>config/rt-inventory-cicd/overlays</code> which defines <code>-cicd</code> namespace, <code>pipeline</code> service account, then role bindings.  And it also defines Tekton tasks, pipelines, triggers... This is where developers can add new tasks and pipelines for their applications.</li> <li>rt-inventory-dev-env-app: Argo app to monitor <code>environments/rt-inventory-dev/env/overlays</code> which defines the NameSpace, Service Account, RoleBindings, for a <code>dev</code> environment. </li> <li>rt-inventory-dev-app-refarch-eda-store-simulator: Argo app to monitor <code>environments/rt-inventory-dev/apps/app-refarch-eda-store-simulator/overlays</code> to deploy a specific app</li> </ul>"},{"location":"coding/gitops/#environment","title":"Environment","text":"<p><code>environment</code> includes two environment definitions for <code>dev</code> and <code>stage</code>. For each environment the <code>apps</code> folder includes the deployment of all the apps part of the solution and then a <code>env</code> folder to define the dev namespace, role bindings... </p> <pre><code>    environments\n  \u2502   \u251c\u2500\u2500 rt-inventory-dev\n  \u2502   \u2502   \u251c\u2500\u2500 apps\n  \u2502   \u2502   \u2502   \u2514\u2500\u2500 app-refarch-eda-store-inventory\n                ..... SEE BELOW\n  \u2502   \u2502   \u2514\u2500\u2500 env\n  \u2502   \u2502       \u251c\u2500\u2500 base\n  \u2502   \u2502       \u2502   \u251c\u2500\u2500 argocd-admin.yaml\n  \u2502   \u2502       \u2502   \u251c\u2500\u2500 kustomization.yaml\n  \u2502   \u2502       \u2502   \u251c\u2500\u2500 my-inventory-dev-environment.yaml\n  \u2502   \u2502       \u2502   \u2514\u2500\u2500 my-inventory-dev-rolebinding.yaml\n  \u2502   \u2502       \u2514\u2500\u2500 overlays\n  \u2502   \u2502           \u2514\u2500\u2500 kustomization.yaml\n  \u2502   \u2514\u2500\u2500 rt-inventory-stage\n  \u2502       \u2514\u2500\u2500 env\n  \u2502           \u251c\u2500\u2500 base\n  \u2502           \u2502   \u251c\u2500\u2500 argocd-admin.yaml\n  \u2502           \u2502   \u251c\u2500\u2500 kustomization.yaml\n  \u2502           \u2502   \u251c\u2500\u2500 rt-inventory-stage-environment.yaml\n  \u2502           \u2502   \n  \u2502           \u2514\u2500\u2500 overlays\n  \u2502               \u2514\u2500\u2500 kustomization.yaml\n</code></pre> <p>In an <code>apps</code> folder the <code>services/${app-name-here}/base/config</code> defines the manifests to configure the application. This is where we can put you app specific configuration.</p> <pre><code>\u2502\u00a0\u00a0 \u251c\u2500\u2500 rt-inventory-dev\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 apps\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 app-refarch-eda-store-simulator\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 base\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 overlays\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 services\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 refarch-eda-store-simulator\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u251c\u2500\u2500 base\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 100-deployment.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 200-service.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 300-route.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0             \u2514\u2500\u2500 overlays\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0                 \u2514\u2500\u2500 kustomization.yaml\n</code></pre> <p>Attention! if a service is used between multiple applications, ( for exmaple Kafka),  then we need to add under the <code>environments/....-dev/</code> folder  a <code>services</code> folder to declare such services. To see an example of this approach go to this repo.</p>"},{"location":"coding/gitops/#secrets","title":"Secrets","text":"<p>At the same level as the GitOps folder created by <code>kam</code>, there is a <code>secrets</code> folder  to be used to manage secrets without getting them into git.</p> <p>The approach is to use Bitmani Sealed Secret operators to  deploy a server on OpenShift to decrypt in the final format, encoded sealed secrets to a kubernetes secret into target namespace.</p> <p>Sealed Secrets are a \"one-way\" encrypted Secret that can be created by anyone, but can only be decrypted by the controller running in the target cluster.</p> <p>It needs kubeseal CLI on the client side to do encryption that only the controller can decrypt.</p> <ul> <li>Install the Sealed Secret Operator in <code>sealed-secrets</code> project</li> </ul> <pre><code># Under one of the catalog repo: eda or dba\noc apply -k sealed-secrets-operator/overlays/default\n</code></pre> <ul> <li>Create a Sealed secret controller: for example <code>sealed-secrets-controller</code>. The previous command will create it.</li> </ul> <pre><code>oc get pods -n sealed-secrets\nNAME                                       READY   STATUS    RESTARTS   AGE\nsealed-secrets-controller-8dd96b4d-4zq2b   1/1     Running   0          4m39s\n</code></pre> <ul> <li>Then for each secrets in <code>secrets</code> folder do something like:</li> </ul> <pre><code>cat gitops-webhook-secret.yaml| kubeseal --controller-namespace sealed-secrets \\\n--controller-name sealed-secrets-controller --format yaml &gt;gitops-webhook-sealedsecret.yaml\n#\ncat git-host-access-token.yaml | kubeseal --controller-namespace sealed-secrets \\\n--controller-name sealed-secrets-controller --format yaml &gt; git-host-access-token-sealedsecret.yaml\n# \ncat docker-config.yaml | kubeseal --controller-namespace sealed-secrets \\\n--controller-name sealed-secrets-controller --format yaml &gt; docker-config-sealedsecret.yaml \n# \ncat git-host-basic-auth-token.yaml | kubeseal --controller-namespace sealed-secrets \\\n--controller-name sealed-secrets-controller --format yaml &gt; git-host-basic-auth-token-sealed.yaml\n# then continue with any microservice webhook secret \n</code></pre> <p>Each sealed secret can be in uploaded to github, and applied to different k8s namespace.</p> <pre><code># mv all the sealed.yaml to config/...-cicd/base/09-secrets folder\n# in secrets folder\nmv *-sealed.yaml ../*gitops/config/\n\noc apply -f gitops-webhook-sealedsecret.yaml\n# this should create a k8s secret\n</code></pre> <p>ERROR: giving up: no key could decrypt secret (.dockerconfigjson)</p>"},{"location":"coding/gitops/#what-to-do-from-there","title":"What to do from there","text":"<ol> <li>Assess if you need the Pipeline or not (for example when defining a demo environment with public docker images we do not need Pipeline).  If you do not use Pipeline, comment in the <code>config/argocd/kustomization.yaml</code> the call to <code>cicd-app.yaml</code>. </li> <li>Assess if you need <code>stage</code> environment. For demo one environment is needed.</li> <li>Change the <code>environments/&lt;&gt;-dev/apps/app-&lt;appname&gt;/services/&lt;appname&gt;/base/config</code> file with the kustomization files of your application.</li> <li> <p>Add any cross microservice/application dependent services under a new structure per environment. Ex</p> <pre><code>environments/&lt;&gt;-dev/services/kafka-strimzi/\n</code></pre> </li> <li> <p>Create a <code>bootstrap</code> folder and add the declaration for the ArgoCD project to avoid using <code>default</code> project. See example in this repo.</p> </li> <li> <p>Bring up the Argocd app of app by doing: <code>oc apply -k config/argocd/</code>, which will create:</p> <ul> <li>three Argo applications to monitor CI/CD, Dev and Staging environments: rt-inventory-cicd, rt-inventory-dev, rt-inventory-stage</li> <li>One Argo Application per service to deploy: the following figure has only one of such service. </li> <li>Three OpensShift projects: one for cicd, one per target 'environments' (dev, stage)</li> </ul> <p>depending of the OCP version, there may be issue regarding <code>controller user</code> being able to create resource.  To enable full cluster admin access on OpenShift, run the following command: </p> <pre><code>oc adm policy add-cluster-role-to-user cluster-admin -z argocd-application-co \n</code></pre> </li> <li> <p>Part of the configuration bootstraps a simple <code>OpenShift Pipelines</code> pipeline for building code when a pull-request is opened.</p> </li> <li> <p>Add new microservices using command like:</p> <pre><code>kam service add --git-repo-url https://github.com/jbcodefoce/new-service.git --app-name newservicename  --env-name stage  --image-repo quay.io/jbcodefoce/new-service --service-name new-service\n</code></pre> <p>Attention there is an issue with KAM as it will overwrite any update in previously generated file, so for example  bringing the project name back to default. We need to tunes the pipelines.yaml file</p> <p>For each service, an unencrypted secret will be generated into the secrets folder. Make sure to apply these secrets to the cluster.</p> </li> <li> <p>Add a webhook to each microservice repository to connect to the Pipeline event listener.</p> <pre><code># Get the event listener route end point\noc get route --namespace rt-inventory-cicd\n</code></pre> <p>Then in the application github repository do the following:</p> <ul> <li>Add a secret with the same password as the created by kam: something like <code>webhook-secret-rt-inventory-dev-refarch-eda-store-simulator</code>. </li> </ul> <p><pre><code> oc get secret  webhook-secret-rt-inventory-dev-refarch-eda-store-simulator -o jsonpath='{.data.webhook-secret-key}' | base64\n</code></pre> * use, &gt; Settings &gt; Webhook &gt; add new webhook, copy this address with <code>http://</code> prefix.  Try to do a new push to see if the event is sent to the listener. If so a new pipeline is started visible in the <code>Pipelines</code> menu on OpenShift console.</p> </li> </ol>"},{"location":"coding/gitops/#future-readings_1","title":"Future readings","text":"<ul> <li>Creating CI/CD solutions for applications using OpenShift Pipelines (4.8)</li> <li>OpenShift pipeline tutorial</li> <li>Github create webhooks</li> </ul>"},{"location":"coding/knative/","title":"Knative","text":"<p>Knative is Kubernetes based platform to develop serverless. Major value proposition is a simplified  deployment syntax with automated scale-to-zero and scale-out based on HTTP load.  Other interesting characteristics are:</p> <ul> <li>adjust the traffic distribution amongst the service revisions, to support blue/green deployment and canary release</li> <li>able to scale to zero: After a defined time of idleness (the so called stable-window) a revision is considered inactive. Now, all routes pointing to the now inactive revision will be pointed to the so-called activator. By default the scale-to-zero-grace-period is 30s, and the stable-window is 60s.</li> <li>auto-scaling: By default Knative Serving allows 100 concurrent requests into a pod. This is defined by the  <code>container-concurrency-target-default</code> setting in the configmap <code>config-autoscaler</code> in the knative-serving namespace.</li> </ul> <p>Knative consists of the following components:</p> <ul> <li>Serving - Request-driven compute that can scale to zero</li> <li>Eventing - Management and delivery of events</li> </ul> <p>With OpenShift it is called OpenShift Serverless, and offers operators for functions, serving and eventing.  Integrated with Apache Camel connectors. </p>"},{"location":"coding/knative/#value-propositions","title":"Value propositions","text":"<p>Like any serverless (AWS lambda, openwhisk..) the benefits are:</p> <ul> <li>instantly spin up enough server resources to handle tens of thousands of incoming requests</li> <li>more expensive on compute time, but cost zero after</li> <li>spin up a completely separate version of the site to do some prototyping, no need to worry about forgetting a test running forever</li> <li>no Kubernetes configurations, no load balancers, no auto scaling rules</li> </ul>"},{"location":"coding/knative/#challenges-based-on-aws-lambda-but-may-apply-to-knative","title":"Challenges (based on AWS lambda but may apply to knative)","text":"<ul> <li>web socket support, SMTP session</li> <li>With AWS no control of the OS, so difficult to bring your libraries. Knative as container fixes this challenge.</li> <li>AWS: functions with less RAM have slower CPU speed. This can swing a lot between 5ms to 500ms response time. Charged by 100ms increment</li> <li>Using SPA app like React or Angular, the page will be downloaded from a ContentDN server, which leads to have a delay between this web server and the function.  If you want &lt;50ms response times, then hosting your backend behind API Gateway is not usable for function.</li> <li>App that needs non serverless services like VPC, NAT gateways to access other service like storage. </li> <li>to get a granular view of how long Lambdas are running, how many times... Measurement and Logging products are not serverless, so cost a lot.  Specially as you may not control the information sent to the logger (like CloudWatch).</li> <li>Idempotence: AWS Lambda, can execute the same requests more than once. Which will generate multiple records written into the DB.  This is due to retries on error, and event source set at least once delivery. Use request identifiers to implement idempotent Lambda functions that do not break if a function invocation needs to be retried.  Make sure to pass the original request id to all subsequent calls. The original request id is generated as early as possible in the chain.  If possible on the client side. Avoid generating your own ids.</li> <li>Time limit on execution to prevent deadlocked function. Difficult to see those functions as they disappear quickly.</li> <li>COLD start: if your function hasn\u2019t been run in a while, a pending request needs to wait for it to be initialized before it can be served: 3 to 5 seconds.  Nothing the end-user touches could be hosted on Lambda after all</li> <li>view Lambda mainly in terms of what it can do as a high-end parallel architecture </li> </ul>"},{"location":"coding/knative/#getting-started","title":"Getting started","text":"<ul> <li>Install Knative CLI with brew. It will be accessible via <code>kn</code></li> <li>Or use Knative CLI running in docker, see doc here</li> <li> <p>To prepare OpenShift see the serverless install instructions. </p> <ul> <li>The minimum requirement to use OpenShift Serverless is a cluster with 10 CPUs and 40GB memory. Use operator hub to install Knative serving and eventing servers and brokers. </li> <li>OpenShift Serverless Operator eventually shows up and its Status ultimately resolves to InstallSucceeded in the openshift-operators namespace.</li> <li>Creating the knative-serving namespace: <code>oc create namespace knative-serving</code>, and then within the project itself, create an instance. </li> <li>Verify the conditions: <code>oc get knativeserving.operator.knative.dev/knative-serving -n knative-serving --template='{{range .status.conditions}}{{printf \"%s=%s\\n\" .type .status}}{{end}}'</code> You should get:</li> </ul> <pre><code>DependenciesInstalled=True\nDeploymentsAvailable=True\nInstallSucceeded=True\nReady=True\n</code></pre> <ul> <li>With a yaml file: and <code>oc apply</code> it.</li> </ul> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeServing\nmetadata:\n  name: knative-serving\n  namespace: knative-serving\n  spec:\n    high-availability:\n      replicas: 2\n</code></pre> <ul> <li>We can do the same for <code>knative-eventing</code>: create a namespace and then an instance using the serverless operator. </li> <li>Verify with: <code>oc get knativeeventing.operator.knative.dev/knative-eventing -n knative-eventing --template='{{range .status.conditions}}{{printf \"%s=%s\\n\" .type .status}}{{end}}'</code></li> </ul> </li> </ul>"},{"location":"coding/knative/#knative-serving","title":"Knative serving","text":"<p>Knative Serving controller creates a Configuration, a Revision, and a Route. The Knative Configuration maintains  the desired state of your deployment, providing a clean separation of code and configuration.  Every change to the application configuration creates a new Knative Revision.</p>"},{"location":"coding/knative/#define-a-service-for-a-docker-image","title":"Define a service for a docker image","text":"<pre><code># tutorial example\nkn service create greeter --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus\n\nkn service list\n# Update service with env variable\nkn service update greeter  --env \"MESSAGE_PREFIX=Namaste\"\nkn service describe greeter\n# Call service\nhttp $(kn service describe greeter -o url)\n# Get revision list \nkn revision list \n# Delete a revision\nkn revision delete &lt;revision-id&gt;\n# List route\nkn route list\n# Destroy\nkn service delete greeter\n</code></pre> <p>A revision is the immutable application and configuration state that gives you the capability to roll back  to any last known good state.</p>"},{"location":"coding/knative/#define-a-service-using-yaml","title":"Define a service using yaml","text":"<p>oc apply the following definition. The Knative Service is associated with its apiVersion and kind <code>service.serving.knative.dev</code>.  Liveness and readiness ports are infered from deployment.</p> <pre><code>apiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: greeter\nspec:\n template:\n    metadata:\n      name: greeter-v2\n    spec:\n      containers:\n      - image: quay.io/rhdevelopers/knative-tutorial-greeter:quarkus\n        livenessProbe:\n          httpGet:\n            path: /healthz\n        readinessProbe:\n          httpGet:\n            path: /healthz\n</code></pre> <p>Get the detail of the configuration: <code>oc get configurations.serving.knative.dev greeter</code></p>"},{"location":"coding/knative/#knative-and-quarkus-app","title":"Knative and Quarkus app","text":"<p>Be sure to have the kubernetes or openshit plugin: <code>./mvnw quarkus:add-extension -Dextensions=\"openshift\"</code> Add the following property:</p> <pre><code>quarkus.kubernetes.deployment-target=knative\n</code></pre> <p>When doing <code>mvn package</code> a <code>knative.yaml</code> file is created under <code>target/kubernetes</code></p> <pre><code>mvn -Dcontainer.registry.url='https://index.docker.io/v1/' \\\n&gt; -Dcontainer.registry.user='jbcodeforce' \\\n&gt; -Dcontainer.registry.password='XXXXXXXYYYYYYYZZZZZZZZ' \\\n&gt; -Dgit.source.revision='master' \\\n&gt; -Dgit.source.repo.url='https://github.com/quarkusio/quarkus-quickstarts.git' \\\n&gt; -Dapp.container.image='quay.io/jbcodefore/quarkus-greetings' package\n</code></pre> <p>This command creates the resource files in <code>target/kubernetes</code> directory</p> <p>Deploy the service <code>oc apply --recursive --filename target/kubernetes/</code></p> <p>Some RedHat article.</p>"},{"location":"coding/knative/#bluegreen-deployment","title":"Blue/green deployment","text":"<p>Knative offers a simple way of switching 100% of the traffic from one Knative service revision (blue) to another newly rolled out revision (green). By default new revision receives 100% of the new traffic. To rollback we need to create a yaml with the <code>template.metadata.name</code> to the expected  revision and add the following spec.traffic</p> <pre><code> traffic:\n    - tag: v1\n      revisionName: greeter-v1\n      percent: 100\n    - tag: v2\n      revisionName: greeter-v2\n      percent: 0\n    - tag: latest\n      latestRevision: true\n      percent: 0\n</code></pre>"},{"location":"coding/knative/#canary-release","title":"Canary release","text":"<p>Knative allows you to split the traffic between revisions</p> <pre><code> traffic:\n    - tag: v1\n      revisionName: greeter-v1\n      percent: 80\n    - tag: v2\n      revisionName: greeter-v2\n      percent: 20\n    - tag: latest\n      latestRevision: true\n      percent: 0\n</code></pre>"},{"location":"coding/knative/#knative-eventing","title":"Knative eventing","text":"<p>To use Knative eventing, we need to create a <code>knative-eventing</code> project and then in the Knative (Red Hat OpenShift serverless) operator create one instance.</p> <pre><code>apiVersion: operator.knative.dev/v1alpha1\nkind: KnativeEventing\nmetadata:\n  name: knative-eventing\n  namespace: knative-eventing\nspec: {}\n</code></pre> <p>Verify it runs successfully by looking at the predefined services:</p> <pre><code>kubectl api-resources --api-group='sources.knative.dev'\n</code></pre> <p>There are three primary usage patterns with Knative Eventing:</p> <ol> <li>Source to Sink: It provides single Sink\u2009\u2014\u2009that is, event receiving service --, with no queuing, back-pressure, and filtering. The Source to Service does not support replies, which means the response from the Sink service is ignored</li> <li>Channel and subscription: the Knative Eventing system defines a Channel, which can connect to various backends such as In-Memory, Kafka and GCP PubSub for sourcing the events. Each Channel can have one or more subscribers in the form of Sink services  </li> </ol> <p></p> <p>When forwarding event to subscribers the channel transforms the event data as per CloudEvent specification</p> <p>To make a project using knative eventing label the namespace with: <code>kubectl label namespace jbsandbox knative-eventing-injection=enabled</code>. This will start the filter and ingress pods.</p>"},{"location":"coding/knative/#source-and-sink","title":"Source and sink","text":"<p>From Red Hat tutorial, the commands to test knative eventing</p> <pre><code># Under current namespace\n# 1- Create sink\nkn service create eventinghello \\\n  --concurrency-target=1 \\\n  --image=quay.io/rhdevelopers/eventinghello:0.0.2\n# A pod is created with name like  eventinghello-sdnfp-1-deployment-...\n\n# 2 Create source as a generator of json message\nkn source ping create eventinghello-ping-source \\\n  --schedule \"*/2 * * * *\" \\\n  --sink ksvc:eventinghello\n# Verify source start to send message\nkn source ping list\n# Verify sink got messages\noc logs &lt;sink-app-pod&gt;\n# clean first the source then the sink\nkn source ping delete eventinghello-ping-source\nkn service delete eventinghello\n</code></pre>"},{"location":"coding/knative/#channel-and-subscribers","title":"Channel and Subscribers","text":"<p>Here is a simple set of steps to demo channel and subscriber on the ping source</p> <pre><code># Create the in memory channel \nkn channel create eventinghello-ch\nkn channel list\n# Create source \nkn source ping create eventinghello-ping-source \\\n  --schedule \"*/2 * * * *\" \\\n  --sink channel:eventinghello-ch\nkn source ping list\n# create a subscriber service\nkn service create eventinghelloa --concurrency-target=1 --revision-name=eventinghelloa-v1 --image=quay.io/rhdevelopers/eventinghello:0.0.2\n# Add the subscription\nkn subscription create  eventinghelloa-sub --channel eventinghello-ch   --sink eventinghelloa\nkn subscription list\n# Clean up\nkn service delete eventinghello\nkn subscription delete eventinghelloa-sub\nkn source ping delete eventinghello-ping-source\n</code></pre>"},{"location":"coding/knative/#broker-and-trigger","title":"Broker and Trigger","text":"<p>See this tutorial for basics trigger and services explanation. </p> <p>Broker and Trigger supports filtering of events so subscribers specify interest on certain set of messages that flows into the Broker. For each Broker, Knative Eventing will implicitly create a Knative Eventing Channel. The Trigger gets itself subscribed to the Broker and applies the filter on the messages on its subscribed broker. The filters are applied on the on the Cloud Event attributes of the messages, before delivering it to the interested Sink Services(subscribers).</p> <p></p> <p>When eventing is set the filter and ingress pods are started. To get the address of the broker url:  <code>oc get broker default -o jsonpath='{.status.address.url}'</code></p> <p>Then the approach is to create different sinks, define triggers for each sink on what kind of event attribute  to subscribe to, so filtering can occur. The sink will respond depending of the cloud event 'type' attribute for example.</p>"},{"location":"coding/knative/#some-other-cli-commands","title":"Some other CLI commands","text":"<pre><code># get revisions for a service\noc get rev --selector=serving.knative.dev/service=greeter --sort-by=\"{.metadata.creationTimestamp}\"\noc delete services.serving.knative.dev greeter\n</code></pre> <pre><code># get revisions\nkn revision list\nkn revision describe greeter-xhwyt-1 \n\n# route\nkn route list\n</code></pre>"},{"location":"coding/knative/#troubleshooting","title":"Troubleshooting","text":"<p>Source of knowledge Debugging issues with your application.</p> <ul> <li>Revision failed</li> </ul> <pre><code>oc get configurations.serving.knative.dev item-kafka-producer\nNAME                  LATESTCREATED               LATESTREADY   READY     REASON\nitem-kafka-producer   item-kafka-producer-65kbv                 False     RevisionFailed\n</code></pre>"},{"location":"coding/knative/#sources","title":"Sources","text":"<ul> <li>knative.dev</li> <li>Red Hat knative tutorial on github.</li> <li>Knative Cookbook on O'Reilly portal</li> <li>REd Hat Knative Tutorial</li> <li>Quarkus Funqy Knative events binding guide</li> <li>Eventing example with cloud engine - lionel villard</li> <li>Twitter to knative - michael maximilien</li> </ul>"},{"location":"coding/playground/","title":"Playground","text":"<p>This is section is notes on my different environments.</p>"},{"location":"coding/playground/#isolated-development-using-a-container","title":"Isolated development using a container","text":"<p>Using the Dev-Dockerfile to build a container to do isolated development activities. </p> <ul> <li>Build a dev image</li> </ul> <pre><code>docker build -f Dev-Dockerfile -t j9r/dev-env .\n</code></pre> <ul> <li>Run it to access to git client</li> </ul> <pre><code>docker run -ti -v $(pwd):/home/me j9r/dev-env bash\n</code></pre> <ul> <li>Use it for git commands.</li> </ul> <pre><code>git config --global user.email \"...\"\ngit config --global user.name jbcodeforce\n</code></pre>"},{"location":"coding/playground/#vscode","title":"VSCode","text":"<ul> <li>Product doc</li> <li>Beginner guide</li> <li> <p>Tricks</p> </li> <li> <p>Ctrl+shift P to open command palette </p> </li> <li>Ctrl K + ctrl T for changing the theme for all windows</li> </ul> <p>Settings are at user level, so for all workspace and windows, or at workspace level.</p> <ul> <li>Command short cut sheet Windows mac</li> <li>Article on theme customization per workspace and theme color</li> </ul> <p>Assess heme like atom light and icon theme like material icon.</p>"},{"location":"coding/playground/#important-linux-command","title":"Important linux command","text":"<pre><code>uname -a\n# \nhostname\nip addr\nnslookup\n</code></pre>"},{"location":"coding/playground/#ubuntu-machine","title":"Ubuntu machine","text":"<pre><code>ssh jerome@ubuntu1\n# assess if minikube runs\nsudo systemctl status minikube\nminikube status\nminikube version\n# to start with podman driver\nminikube start\n# Deploy the k8s dashboard\nminikube dashboard --url\n</code></pre> <p>See this note to access Dashboard from remote host.</p>"},{"location":"coding/playground/#kubectl-basic","title":"Kubectl basic","text":"<pre><code>minikube kubectl cluster-info\n</code></pre> <p>To make it simple: <code>alias k=\"minikube kubectl\"</code></p> <pre><code>k describe node\n</code></pre>"},{"location":"coding/playground/#catalog-of-what-could-be-installed","title":"Catalog of what could be installed","text":"Image Command Kafka helm install bitmani/kafka Strimzi helm repo add strimzi https://strimzi.io/charts/  &amp;&amp; helm install strimzi-kafka strimzi/strimzi-kafka-operator"},{"location":"coding/playground/#minkube-as-a-local-k8s","title":"Minkube as a local k8s","text":"<p>See minikube dedicated notes</p>"},{"location":"coding/playground/#wsl2-tricks","title":"WSL2 tricks","text":""},{"location":"coding/playground/#networking","title":"Networking","text":"<p>To access a web app running in WSL2 from an external computer, we can port forward the desired ports in WSL 2 NAT. This is because WSL 2 creates a virtualized Ethernet adapter with its own IP address, which creates a NAT between the WSL instance and the Windows host computer.</p> <pre><code># get ip address of WSL@ machine\nip a\n# configure windows to have a port proxy\nnetsh interface portproxy add v4tov4 listenport=3000 listenaddress=0.0.0.0 connectport=3000 connectaddress=192.168.85.149\n# Add a rule in he Windows firewall to authorize access to port 3000\n</code></pre>"},{"location":"coding/playground/#local-image-registry","title":"Local Image Registry","text":"<p>Under the <code>tools/local-registry</code> there is a docker compose to start a local registry to be able to test helm on k8s.</p> <p>The URL is http://localhost:5002/v2/_catalog.</p> <p>An example of using this registry (docker compose in athena-owl-core/deployment/local):</p> <pre><code>services:\n  owl-backend:\n    hostname: owl-backend\n    image: localhost:5002/athena-owl-backend:1.0.0\n    container_name: owl-backend\n    build:\n      context: ../../owl-agent-backend/src\n</code></pre> <pre><code>docker compose build\ndocker compose push\n</code></pre>"},{"location":"coding/playground/#getting-a-k8s-deploymentyaml-from-docker-compose-using-kompose","title":"Getting a k8s deployment.yaml from docker compose using Kompose","text":"<pre><code># Build image\ndocker build -t kompose https://github.com/kubernetes/kompose.git\\#main\n# run within a folder with a docker compose file\ndocker run --rm -it -v $PWD:/opt kompose sh -c \"cd /opt &amp;&amp; kompose convert\"\n</code></pre>"},{"location":"coding/playground/#docker-desktop-with-kubernetes","title":"Docker Desktop with kubernetes","text":"<p>See interesting Blog on How Kubernetes works under the hood with Docker Desktop., which we get the following important concepts:</p> <ul> <li>Docker Desktop automatically generates server and client certificates for key internal services, including kubelet (node manager), service account management, frontproxy, API server, and etcd components.</li> <li>The global endpoint of the cluster is using the DNS name https://kubernetes.docker.internal:6443.</li> <li>For bootup, the life cycle runs <code>kubeadm init</code> to initialize the cluster and then start the kubelet process</li> <li>Services of type LoadBalancer are exposed outside the Kubernetes cluster.</li> <li>Vpnkit-controller is a port forwarding service which opens ports on the host and forwards connections to the pods inside the VM.</li> <li>Docker Desktop uses  dockershim to share the image cache between the Docker engine and Kubernetes. Kubernetes can create containers from images stored in the Docker Engine image cache. Be sure to have a policy to **IfNotPresent</li> <li>The tutorial yaml is in tools folder.</li> </ul> <p>Important commands to verify major control plane components:</p> <pre><code># Set a context to a cluster\nkubectl config  --kubeconfig=/home/jbcodeforce/.kube/config  use-context athena-demo\n# Get exposed services\nkubectl get svc\nkubectl get pods -n kube-system\n</code></pre> <p>See later section to install helm. Once installed test by deploying nginx as an ingress controller:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install my-ingress-nginx ingress-nginx/ingress-nginx\nkubectl get service --namespace default my-ingress-nginx-controller --output wide --watch\nkubectl get svc -n default\n</code></pre> <p>And use this kind of ingress declaration</p> <pre><code>apiVersion: networking.k8s.io/v1\n  kind: Ingress\n  metadata:\n    name: example\n    namespace: foo\n  spec:\n    ingressClassName: nginx\n    rules:\n      - host: www.example.com\n        http:\n          paths:\n            - pathType: Prefix\n              backend:\n                service:\n                  name: exampleService\n                  port:\n                    number: 80\n              path: /\n              ``\n</code></pre>"},{"location":"coding/playground/#using-helm","title":"Using Helm","text":"<ul> <li>Install Helm </li> </ul> <pre><code>curl https://get.helm.sh/helm-v3.16.0-rc.1-linux-amd64.tar.gz -o helm-v3.16.0-rc.1-linux-amd64.tar.gz\ntar -xzvf helm-v3.16.0-rc.1-linux-amd64.tar.gz\nmv linux-amd64/helm bin\nrm helm-v3.16.0-rc.1-linux-amd64.tar.gz\n</code></pre> <ul> <li> <p>Main concepts</p> <ul> <li>A Chart is a Helm package. It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster.</li> <li>A Repository is the place where charts can be collected and shared. Bitnami is a very useful repository.</li> <li>A Release is an instance of a chart running in a Kubernetes cluster</li> </ul> </li> <li> <p>Useful commands</p> </li> </ul> <pre><code>helm search hub\n# \nhelm repo add bitnami https://charts.bitnami.com/bitnami\n# install an image (kafka with kraft)\nhelm install my-kafka bitnami/kafka \n# list release\nhelm list\n# install a release\nhelm uninstall kafka-1725727453\n# To see what options are configurable on a chart\nhelm show values bitnami/kafka\n# You can then override any of these settings in a YAML formatted file,\nhelm install -f values.yaml my-kafka bitnami/kafka \n# Upgrade an existing release\nhelm upgrade -f new-value.yaml my-kafka bitnami/kafka \n# Upgrade an existing release, and in a specific context\nhelm upgrade  owl-backend owl-backend\nhelm upgrade  --kube-context athena-demo owl-backend owl-backend\n#  roll back to a previous release \nhelm rollback my-kafka 1\n</code></pre> <p>The IBM helm repository:</p> <pre><code>helm repo add ibmcharts https://raw.githubusercontent.com/IBM/charts/master/repo/ibm-helm\nhelm repo update\n</code></pre>"},{"location":"coding/playground/#developing-our-charts","title":"Developing our charts","text":"<p>See the summary of the commands</p> <pre><code>helm create &lt;chart_name&gt;\n</code></pre> <ul> <li>Update charts.yaml, values.yaml and may be some other templates.</li> </ul> From docker-compose to kubernetes <p>The Kompose.io tool can hel creating k8s manifests from a docker compose. Once done deployment and service can be added to the values and template of helm.</p> <pre><code>docker run --rm -it -v $PWD:/opt kompose sh -c \"cd /opt &amp;&amp; kompose convert\"\n</code></pre> <ul> <li>Package and validation</li> </ul> <pre><code>helm package &lt;chart_name&gt;\nhelm lint &lt;chart_name&gt;\nhelm install owl-backend --dry-run owl-backend\n</code></pre> <ul> <li>Tar file could be updloaded to s3, and the s3 bucket exposed as HTTP web server to be used as repository. </li> </ul> Configure volumes <ol> <li>Create a config map from a yaml file</li> </ol> <pre><code>kubectl create configmap --dry-run=client somename --from-file=./src/athena/config/config.yaml --output yaml\n</code></pre> <ol> <li>Add in the Values.yaml of the chart the volume and volume mount</li> </ol> <pre><code>volumes: \n - name: app-config-vol\n   configMap:\n    name: app-config-cm\n\nvolumeMounts:\n  - name: app-config-vol\n    mountPath: /app/config\n</code></pre>"},{"location":"coding/tekton/","title":"Tekton tutorial","text":"<p>This article is a summary based on OpenShift pipeline tutorial, Red Hat scholar - tekton tutorial, this blog and a log of painful experiences.</p> <ul> <li>Tekton is a flexible, Kubernetes-native, open-source CI/CD framework that enables automating deployments across multiple platforms (Kubernetes, serverless, VMs, etc)</li> <li>Build images with Kubernetes tools such as S2I, Buildah, Buildpacks, Kaniko,...</li> <li>With OpenShift Pipelines operator, CRD, service account and cluster binding are created automatically.</li> </ul>"},{"location":"coding/tekton/#concepts","title":"Concepts","text":"<ul> <li>Task: is a reusable, loosely coupled number of steps that perform a specific ci or cd task.  Tasks are executed by creating TaskRuns. A TaskRun will schedule a Pod. Each step  can contain elements such as command, script, volumeMounts, workingDir, parameters, resources, workspace, or image. </li> <li>Pipeline: is the definition of the chaining of tasks to perform</li> <li>Resources: build uses resources called PipelineResource to help configuring the source git repo url,  the final container image name.</li> </ul> <p>Task requires an input resource of type <code>git</code> which defines where the source is located.  The git source is cloned to a local volume at path <code>/workspace/source</code> where <code>source</code> comes from the name we gave to the resource.</p> <p>In Pipeline, the Tasks are arranged in a specific order of execution as part of our continuous integration flow:</p> <p></p> <p>The pipelineRun invokes the pipeline. </p> <p>The pipelines can be triggered by events coming from GitHub. To do so Tekton defines the following constructs:</p> <ul> <li>Triggers help to hook our Pipelines to respond to external github (or other csm tools) events.  Trigger combines TriggerTemplate, TriggerBindings and interceptors. They are used as reference inside the EventListener.</li> <li>TriggerTemplate is a resource which have parameters that can be substituted anywhere  within the resources of template. It is mapped to a PipelineRun.</li> <li>TriggerBindings is a map to capture fields from an github events, store them  as parameters, and replace them in TriggerTemplate whenever an event occurs. Here is an extract of such trigger binding definition:</li> </ul> <pre><code>apiVersion: triggers.tekton.dev/v1alpha1\nkind: TriggerBinding\nmetadata:\n  creationTimestamp: null\n  name: github-push-binding\n  namespace: risk-scoring-cicd\nspec:\n  params:\n  - name: gitrepositoryurl\n    value: $(body.repository.clone_url)\n  - name: fullname\n    value: $(body.repository.full_name)\n</code></pre> <ul> <li>Event Listener sets up a Service and listens for events in JSON format. It connects a TriggerTemplate to a TriggerBinding, into an addressable endpoint. Each EventListener can consist of one or more triggers. The <code>triggers.bingings</code> section list the bindings to use and the name of <code>TriggerTemplate</code> to use. Bindings may have <code>interceptors</code> to modify payload or behavior on the events:</li> </ul> <pre><code>    interceptors:\n    - github:\n        secretRef:\n          secretKey: webhook-secret-key\n          secretName: gitops-webhook-secret\n    - cel:\n        filter: (header.match('X-GitHub-Event', 'push') &amp;&amp; body.repository.full_name\n          == 'jbcodeforce/ads-risk-scoring-gitops')\n        overlays:\n        - expression: body.ref.split('/')[2]\n          key: ref\n</code></pre> <p><code>GitHub Interceptors</code> contain logic to validate and filter webhook events that come from GitHub.  To use this Interceptor as a filter, add the event types you would like to accept to the eventTypes field.</p> <p>For example, the CEL interceptors above is used to get only push events from a specif repo.</p> <p>To use those triggers, we need to define a webhook in the source git repository (using Settings &gt; Webhooks &gt; Add webhook), so <code>push</code> events can be sent to the event listener. The Webhook URL end point is retrieved by getting the route of the event listener for the cicd project:</p> <pre><code>oc get route -n solution-cicd\n</code></pre> <p>Here is a diagram to represent the relationship between all those elements:</p> <p></p> <ul> <li>Event Listener is exposed via a route so the Github webhook for the application source repository can send push events to the listener</li> <li>Most likely the event listener may use secret to avoid having any repo using the pipelines. The secret may be defined in OpenShift and then added to the webhook definition or defined in the git repo and added as secret and then defined in the event listener:</li> </ul> <p><pre><code>github:\n     secretRef:\n       secretKey: webhook-secret-key\n       secretName: gitops-webhook-secret\n</code></pre> * The event listener links template and triggers. * Triggers defines the binding and interceptors that will process the HTTP POST request coming from github.  * Trigger template defines pipeline run * PipelineRun reference pipeline definition and optionally resources.</p> <p>As <code>PipelineRun</code> are defined inside the <code>TriggerTemplate</code>, they are specifics to each application to build. </p>"},{"location":"coding/tekton/#installation","title":"Installation","text":"<ul> <li>Install the operator via OpenShift Operator Hub (Search pipeline) or using yaml: </li> </ul> <pre><code>  # under https://github.com/jbcodeforce/eda-gitops-catalog\n  oc apply -k openshift-pipelines-operator/overlays/stable\n</code></pre> <p>Attention not all versions are compatible between OpenShift version.</p> <ul> <li>Define a service account <code>pipeline</code> (created automatically by the OpenShift Pipeline Operator)</li> <li>Ensure Tekton pipelines is deployed and the API is available for use</li> </ul> <pre><code>kubectl api-resources --api-group='tekton.dev'\n</code></pre> <p>Results</p> <pre><code>NAME                SHORTNAMES   APIGROUP     NAMESPACED   KIND\nclustertasks                     tekton.dev   false        ClusterTask\nconditions                       tekton.dev   true         Condition\npipelineresources                tekton.dev   true         PipelineResource\npipelineruns        pr,prs       tekton.dev   true         PipelineRun\npipelines                        tekton.dev   true         Pipeline\nruns                             tekton.dev   true         Run\ntaskruns            tr,trs       tekton.dev   true         TaskRun\ntasks                            tekton.dev   true         Task\n</code></pre> <ul> <li>Install the tkn CLI</li> </ul> <p>09/30/2021 There is an issue on IBM ROKS as of now where the the buildah task to build docker image container needs to run with privileged. So we need to add security constraint to the <code>pipeline</code> service account.  Normally the command: </p> <pre><code>oc adm policy add-scc-to-user privileged -z pipeline\n</code></pre> <p>performed under the cicd project, should add the user to the <code>privileged</code> security constraint. It may not work on OCP 4.7+ so we need to add it manually (see line 12 in screen shot below):</p> <p></p>"},{"location":"coding/tekton/#developers-steps","title":"Developer's steps:","text":"<p>At the high level, the generic steps to perform for a given application are:</p> <ul> <li>Create custom task to define how to build your app or install existing reusable Tasks.</li> <li>Create PipelineResources to specify the github source repository and the docker image name to create.</li> <li>Create a Pipeline to define your application's delivery pipeline. If the pipeline uses different resources, use a workspace with PVC to share data between tasks.</li> <li>Create a PersistentVolumeClaim to provide the volume/filesystem for the pipeline execution or provide a VolumeClaimTemplate which creates a PersistentVolumeClaim</li> <li>Create a PipelineRun to instantiate and invoke the pipeline</li> <li>Add triggers to capture events in the source repository that are propagated by webhook.</li> </ul>"},{"location":"coding/tekton/#define-tasks","title":"Define tasks","text":"<p>Task contains at least one step to be executed to perform a useful function. TaskRuns are docker containers, running in one pod. Tasks execute steps in the order in which they are written, with each step completing before the next step starts. While <code>Pipelines</code> execute tasks  in parallel unless a task is directed to run after the completion of another task. This facilitates parallel execution of build / test / deploy   activities and is a useful characteristic that guides the user in the grouping of steps within tasks.</p> <p>Tekton can also being used to deploy images freshly built. But  with the adoption of GitOps practices, this deployment task is in fact done with ArgoCD applications.</p> <ul> <li>A first task is to clone a repo. In the Tekton hub we can find the yaml  for this task. But with the OpenShift Pipeline operator, it is part of the clustertask:</li> </ul> <pre><code># List predefined task at the 'cluster level'\ntkn clustertask list\n# See a particular task like git-clone\ntkn  clustertask describe git-clone\n</code></pre> <p>So we do not need to redefine this task. If we really need to get the last release of a task we can use a command like:</p> <pre><code>oc apply -f https://raw.githubusercontent.com/tektoncd/catalog/main/task/git-clone/0.3/git-clone.yaml\n</code></pre> <p>Here is an example of using this task in a pipeline</p> <pre><code>tasks:\n  - name: fetch-source\n    taskRef:\n      name: git-clone\n      kind: ClusterTask\n    workspaces:\n    - name: output\n      workspace: build-ws\n    params:\n    - name: url\n      value: $(params.repo-url)\n    - name: revision\n      value: $(params.revision)\n</code></pre> <p>The workspace is where our Task/Pipeline sources/build artifacts will be cloned and generated. </p> <p>See next pipeline section for how to configure the <code>git-clone</code> task.</p> <p>Remarks: when using resource of type git then a clone will be done implicitly, therefore this task is not needed.</p> <ul> <li>Define a Task to build a quarkus app: this is done by using the maven task: <code>tkn task describe maven</code> or by using custom task based on the maven docker image. </li> </ul> <p>To use the Tekton predefined maven task, do:</p> <pre><code>oc  apply -f https://raw.githubusercontent.com/tektoncd/catalog/main/task/maven/0.2/maven.yaml\n</code></pre> <p>There is an alternative: define the image to use for a step of the task.</p> <p>The source is a sub-path, under which Tekton cloned the application sources.</p> <ul> <li>Other task example to apply Kubernetes manifests (apply-manifests) to deploy an image. or update-deployment task to path the application deployment with a new <code>image name:tag</code>.</li> </ul> <p>The tasks are by default tied to a namespace. ClusterTask makes the task available in all namespaces.</p> <ul> <li>list the tasks defined in current project: </li> </ul> <pre><code>tkn task list\n</code></pre> <p>and use next command to list the Operator-installed additional cluster tasks such as <code>buildah</code>...</p> <pre><code>tkn clustertasks list\n</code></pre> <ul> <li> <p>In Tekton Hub we may find reusable tasks and pipelines like:</p> </li> <li> <p>git-clone has url as input and a workspace to clone code to.</p> </li> <li>maven</li> <li>buildah builds source into a container image and then pushes it to a container registry</li> </ul> <pre><code>tkn clustertask describe buildah\n</code></pre>"},{"location":"coding/tekton/#define-custom-buildah-task","title":"Define custom buildah task","text":"<p>The buildah wants to run with privileged access, so we need a custom task to use security constraint.</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\n  name: build-dr-image\nspec:\n   ....\n  steps:\n    - name: build-image\n      image: quay.io/buildah/stable:v1.15.0\n      securityContext:\n        privileged: true\n      ...\n</code></pre> <p>Be sure the <code>pipeline</code> service account is part of the scc named <code>privileged</code>, see the SCC declaration presented in a section above.</p>"},{"location":"coding/tekton/#define-resources","title":"Define resources","text":"<p>A reference to the resource is declared within the task and then the steps use the resources in the commands.  A resource can be used as an output in a step within the task.</p> <p>In Tekton, there is no explicit Git pull command. Simply including a Git resource in a task definition will result  in a Git pull action taking place, before any steps execute, which will pull the content of the Git repository  to a location of <code>/workspace/&lt;git-resource-name&gt;</code>.  In the example below the Git repository content is pulled to the implicit folder: <code>/workspace/source</code>.</p> <pre><code>kind: Task\n resources:\n   inputs:\n     - name: source\n       type: git\n   outputs:\n     - name: intermediate-image\n       type: image\n steps :\n   - name: build\n</code></pre> <p>PipelineResource defines resources to be used as input or output to task and pipeline, they are reusable. </p> <p>It looks it is still in alpha release as of sept 2021.</p> <p>Example of PipelineResource for git repo:</p> <pre><code>apiVersion: tekton.dev/v1alpha1\nkind: PipelineResource\nmetadata:\n  name: item-inventory-source\nspec:\n  type: git\n  params:\n    - name: url\n      value: https://github.com/ibm-cloud-architecture/refarch-eda-item-inventory\n    - name: revision\n      value: master\n</code></pre> <p>See other resource definitions like docker image names.</p> <ul> <li>get list of resources defined in the project:</li> </ul> <pre><code>tkn res ls\n</code></pre>"},{"location":"coding/tekton/#create-pipeline","title":"Create pipeline","text":"<p>Generic pipeline takes the source code of the application from GitHub and then builds jar and docker image, and deploys image to OpenShift.  The deployment part of the pipeline definition could also being done with ArgoCD application.</p> <ul> <li><code>volumeMounts</code> allows us to add storage to a step. Since each step runs in an isolated container,  any data that is created by a step for use by another step must be stored.  If the data is accessed by a subsequent step within the same task then it is possible to use the <code>/workspace</code> directory  to hold any created files and directories.  A further option for steps within the same task is to use an emptyDir storage mechanism which can be useful for separating  out different data content for ease of use. If file is to be accessed by a subsequent step that is in a  different task then a Kubernetes persistent volume claim is required to be used. </li> </ul> <p>Note that volumes are defined in a section of the task outside the scope of any steps, and then each step that needs the volume will mount it. </p> <ul> <li>The <code>workingDir</code> element refers to the path within the container that should be the current working directory when the command is executed.</li> <li> <p><code>parameters</code>: As with volumeMounts, parameters are defined outside the scope of any step within a task and then they are referenced from within the step.  Parameters, in this case, refer to any information in text form required by a step such as a path, a name of an object, a username etc.  </p> </li> <li> <p>A <code>workspace</code> is similar to a volume in that it provides storage that can be shared across multiple tasks.  A persistent volume claim  is required and then the volume is declared within the pipeline and task before mapping the  workspace into an individual step. Workspaces and volumes are similar in behavior but are defined in  slightly different places.</p> </li> <li> <p><code>Image</code>: Since each Tekton step runs within its own image, the image must be referenced as shown in the example below:</p> </li> </ul> <pre><code>steps :\n  - name: build\n    command:\n      - buildah\n      - bud\n      - '-t'\n      - $(resources.outputs.intermediate-image.url)\n    image: registry.redhat.io/rhel8/buildah\n</code></pre> <p>A Pipeline requires PipelineResources to provide inputs and store outputs for the Tasks that comprise it.</p> <ul> <li>Declare the pipeline in a yaml file like tutorial build and deploy  or the item inventory aggregator pipeline in rt-inventory-gitops</li> <li>In previous section there is an example of git clone task declared in a pipeline. It uses the pipeline parameters to get URL and revision and output to the workspace.</li> </ul> <p>The workspace is declared in the pipeline, and the names must match</p> <pre><code>spec:\nparams:\n- name: repo-url\n  type: string\n  description: The git repository URL to clone from.\n- name: revision\n  type: string\n  description: The git tag to clone.\nworkspaces:\n  - name: build-ws\n</code></pre> <p>This workspace will be specified in the pipelinerun (as well as url and revision):</p> <pre><code>apiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\n  generateName: build-quarkus-app-result-\nspec:\n  pipelineRef:\n    name: build-quarkus-app\n  workspaces:\n  - name: build-ws\n    emptyDir: {}\n</code></pre> <p>The mechanism for adding storage to a step is called a volumeMount, as described further below. </p> <p>In our case, a persistent volume claim called pipeline-storage-claim is mounted into the step at a specific path.  Other steps within the task and within other tasks of the pipeline can also mount this volume and reuse any data placed there by this step.  Note that the path used is where the Buildah command expects to find a local image repository.  As a result any steps that invoke a Buildah command will mount this volume at this location.</p> <p>We need to use persistence storage when our data must still be available, even if the container, the worker node, or the cluster is removed.  We should use persistent storage in the following scenarios:</p> <ul> <li>Stateful apps</li> <li>Core business data</li> <li>Data that must be available due to legal requirements, such as a defined retention period</li> <li>Auditing</li> <li> <p>Data that must be accessed and shared across app instances. For example: </p> <ul> <li>Access across pods: When we use Kubernetes persistent volumes to access our storage, we can determine the number of pods that can mount the volume at the same time. Some storage solutions, such as block storage, can be accessed by one pod at a time only. With other storage solutions, we can share volume across multiple pods.</li> <li>Access across zones and regions: we might require our data to be accessible across zones or regions. Some storage solutions, such as file and block storage, are data center-specific and cannot be shared across zones in a multizone cluster setup.</li> </ul> </li> </ul> <ul> <li>Execute it using a pipeline run</li> </ul> <p><pre><code>oc create -f build/pipelinerun.yaml\n</code></pre> * Or using <code>tkn</code> pipeline start:</p> <pre><code>tkn pipeline start \n</code></pre> <ul> <li>List pipeline runs</li> </ul> <pre><code>tkn pipelinerun list\n</code></pre>"},{"location":"coding/tekton/#buildah","title":"Buildah","text":"<p>Buildah is a tool that facilitates building Open Container Initiative (OCI) container images.  It provides a command line tool that can be used to create a container from scratch or using an image as a starting point. It can use existing Dockerfile and so instead of <code>docker build ....</code>, we use <code>buildah bud ...</code></p> <p>But it can also replace docker file by doing a script to build the image, commit any step the process when needed..</p> <p>It can use <code>mountpoint</code> to expose the root folder of the container within the Host filesystem. This is valuable to use host command instead of installing them in each docker image.</p> <p>See this getting started blog.</p> <p>As this is a for Linux tool, it can run on mac via a docker image. </p> <pre><code>docker run -ti quay.io/buildah/stable:v1.15.0 bash\n</code></pre>"},{"location":"coding/tekton/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Not able to clone git with error like: <code>translating TaskSpec to Pod: secrets \"regcred\" not found.</code>.  Need to create a secret named <code>regcred</code> using the kam generated secrets and using sealed secret mechanism.</li> <li> <p>Build failed to access internal registry with <code>x509: certificate signed by unknown authority</code>.  We may need to do not verify TLS while pushing image to the internal docker registry or use a public registry</p> </li> <li> <p>Github events not propagated or accepted</p> <ul> <li>First verify the webhook settings, it needs to include <code>http</code> URL, uses application/json  and reference a git secret    which includes the password of the webhook secret defined for this application in the <code>-cicd</code> project. </li> <li>Go to the event listener pod in the <code>-cicd</code> project to assess the log</li> <li><code>interceptor stopped trigger processing: rpc error: code = FailedPrecondition desc = no X-Hub-Signature header set</code> looks to be linked to secret not sent wrong.  To verify if the secret is sent, see the request in github:</li> </ul> <pre><code>Request URL: http://gitops-webhook-event-listener-route-rt-inventory-cicd.ac-dal10-b3c-4x16-1e3af63cfd19e855098d645120e18baf-0000.us-south.containers.appdomain.cloud/\nRequest method: POST\nAccept: */*\ncontent-type: application/json\nUser-Agent: GitHub-Hookshot/2d9cb65\nX-GitHub-Delivery: c82ac020-1ffb-11ec-8eb9-f5811b108768\nX-GitHub-Event: push\nX-GitHub-Hook-ID: 320462617\nX-GitHub-Hook-Installation-Target-ID: 375430795\nX-GitHub-Hook-Installation-Target-Type: repository\nX-Hub-Signature: sha1=.....secretkeyt\nX-Hub-Signature-256: sha256=....secretkey-inanotherformat\n</code></pre> <ul> <li><code>interceptor stopped trigger processing: rpc error: code = FailedPrecondition desc = payload signature check failed</code> looks to be also a wrong secret.  Try to use the webhook secret defined by KAM with name like <code>webhook-secret-rt-inventory-dev-item-inventory.yaml</code> and use the decoding like: <code>oc get secret webhook-secret-rt-inventory-dev-item-inventory -o jsonpath='{.data.webhook-secret-key}' | base64 &amp;&amp; echo</code> There is bug open as the HTTP return code should be 401 and not 202. </li> <li>Verify the secret used in the event-listener declaration. </li> </ul> </li> </ul> <p>Set debug level via the configmap <code>config-logging-triggers</code></p> <pre><code>oc patch cm config-logging-triggers -p '{\"data\": {\"loglevel.eventlistener\": \"debug\"}}'\n# Back to info\noc patch cm config-logging-triggers -p '{\"data\": {\"loglevel.eventlistener\": \"info\"}}'\n</code></pre>"},{"location":"coding/tekton/#enhancing-your-solution","title":"Enhancing your solution","text":"<p>We can use nexus to keep the maven downloaded jars. </p> <pre><code>oc apply -f https://raw.githubusercontent.com/redhat-scholars/tekton-tutorial/master/install/utils/nexus.yaml\noc expose svc nexus\n</code></pre>"},{"location":"coding/tekton/#other-readings","title":"Other readings","text":"<ul> <li>Tekton dev documentation</li> <li>OpenShift Pipelines</li> <li>Red Hat Scholars - Tekton tutorial</li> <li>OpenShift Pipelines tutorial</li> <li>TaskRun description</li> <li>Deploy a Knative application using Tekton Pipelines</li> <li>IBM Tekton tasks</li> </ul>"},{"location":"coding/testcontainer/","title":"Testcontainers","text":"<p>Testcontainers is a Java library that supports JUnit tests, providing lightweight, throwaway instances that can run in Docker.</p> <p>The product quickstart documentation explains the basic practices. Below are some practices for the components I am often using.</p>"},{"location":"coding/testcontainer/#test-container-with-kafka","title":"Test container with Kafka","text":"<p>The Kafka test container uses Confluent image. </p> <p>The following code includes a StrimziKafka test container maas kafka backend. The approach is to  use a BasicIT test class that uses the Strimzi container. As the exposed port is mapped to a port dynamically allocated by docker, we can get the bootstrap address and expose it via system variable:</p> <pre><code>public abstract class BasicIT {\n\n    @ClassRule\n    public static StrimziContainer kafkaContainerForTest = new StrimziContainer()\n            .withNetwork(network)\n            .withNetworkAliases(\"kafka\")\n            .withExposedPorts(9092);\n\n    @BeforeAll\n    public static void startAll() {\n        kafkaContainerForTest.start();\n        System.setProperty(\"KAFKA_BOOTSTRAP_SERVERS\", kafkaContainer.getBootstrapServers());\n    }\n\n    @AfterAll\n    public static void stopAll() {\n        kafkaContainerForTest.stop();\n\n    }\n}\n</code></pre> <p>Then each test extends this class and can access the bootstrap URL</p> <pre><code> clusterInfo.adminURL = kafkaContainerForTest.getBootstrapServers();\n</code></pre> <p>Or in the case of quarkus test, we need to specify in the application.properties what to get while using the test profile</p> <pre><code>%test.kafka.bootstrap.servers=${KAFKA_BOOTSTRAP_SERVERS}\n</code></pre> <p>Which means a test at the API level will work fine (extracted from the store simulator project):</p> <pre><code>    @Test\n    public void shouldStartSendingOneMessageToKafka(){\n        System.out.println(System.getProperty(\"KAFKA_BOOTSTRAP_SERVERS\"));\n        given().when().post(\"/start/kafka/1\").then().statusCode(200);\n    }\n</code></pre>"},{"location":"coding/istio/","title":"Use ISTIO for service mesh","text":""},{"location":"coding/istio/#service-mesh","title":"Service Mesh","text":"<p>A service mesh is the network of microservices that make up applications in a distributed microservice architecture.</p> <p>Istio helps operators to connect, secure, control and observe services and microservices.  It aims to manage service mesh. It is a dedicated infrastructure layer that you can add to your applications.  It allows you to transparently add capabilities like observability, traffic management, and security,  without adding them to your own code.</p> <p>The main concepts are presented in the Istio main page.</p> <ul> <li>Merge several projects into one offering: Istio (v1.6), Kiali (monitoring), Jaeger (distributed tracing).</li> <li>Use istiod to reduce control panel resource usage, startup time and improves performance (CRD: <code>ServiceMeshControlPlane</code>).</li> <li>Use Secret Discovery Service to deliver certificates to Envoy: no more k8s secrets, easier to integrate with other certificate providers.</li> <li>Jaeger supports ElasticSearch clusters.</li> <li>Istio simplifies configuration of service-level properties like circuit breakers, timeouts, and retries,</li> </ul> <p>RedHat offers the Service mesh as an extension to Istio:  deployed with operator, </p>"},{"location":"coding/istio/#value-propositions","title":"Value propositions","text":"<ul> <li>Support the cloud native requirements: service discovery, application load balancing (layer 7), failure recovery, metrics,  and monitoring, A/B testing, canary releases, rate limiting, access control, and end-to-end authentication</li> <li>Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.</li> <li>Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection.</li> <li>A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.</li> <li>Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.</li> <li>Secure service-to-service communication in a cluster</li> </ul>"},{"location":"coding/istio/#recall-concepts","title":"Recall concepts","text":"<ul> <li>Canary release is a technique to reduce the risk of introducing a new software version in production by slowly rolling out the change to a small subset of users before rolling it out to  the entire infrastructure and making it available to everybody. (Phased rollout)</li> <li>A/B testing is a way to test a hypothesis using variant implementations. A/B testing is a way to compare two versions  of a single variable, typically by testing a subject's response to variant A against variant B, and determining which of the  two variants is more effective</li> </ul>"},{"location":"coding/istio/#architecture","title":"Architecture","text":"<p>The Istio service mesh is composed of the data plane and control plane.</p> <p>The data plane groups Envoy proxies deployed as sidecar to the microservice, and used to mediate and control communication.</p> <p>The control plane manages and configures Istiod to enforce proxies to route traffic. Istiod provides service discovery,  configuration and certificate management. It converts high-level routing rules to Envoy configurations and propagates them  to the sidecars at runtime.</p> <p></p> <p>Kiali provides visibility into your service mesh by showing you the microservices in your service mesh,  and how they are connected. a basic Grafana integration is available for advanced queries.  Distributed tracing is provided by integrating Jaeger into the Kiali console. Tracing follows the path of a request through various microservices that make up an application.</p> <p>The following is an example of pod assignment within kubernetes. Egress gateway and servicegraph run on a proxy,  while the other components run in the worker nodes.</p> <p></p> <p>The command used to get this assignment are:</p> <p><pre><code>kubectl get nodes\nkubectl describe node &lt;ipaddress&gt;\n</code></pre> To get the pods: <code>kubectl get pods -n istio-system</code>.</p> <p>The  component roles were:  </p> Component Role Envoy Proxy to mediate  all inbound and outbound traffic for all services in the service mesh. It is deployed as a sidecar container inside the same pod as a service. It supports load balancing, circuit breakers, health checks, fault injection, metrics... Istiod It provides service discovery, configuration and certificate management. It consists of the following sub-components: <p>| Galley | Responsible for validating, ingesting, aggregating, transforming and distributing config within Istio. | | Pilot | Supports service discovery, traffic management, resiliency (timeouts, retries, circuit breakers), intelligent routing (A/B testingm canary deployment..). | | Citadel | Used for service-to-service and end-user authentication. Enforce security policy based on service identity. | </p>"},{"location":"coding/istio/#installation","title":"Installation","text":"<p>With OpenShift service mesh, the installation is via Operators:</p> <ul> <li>Jaeger</li> <li>Kiali</li> <li>Service Mesh</li> </ul> <p>Once those operators are installed and running create a control pane, by creating <code>istio-system</code> project,  and then create <code>ServiceMeshControlPlane</code> using the mesh operator.</p> <p>For a more traditional way on Kubernetes, for example, Istio is using custom resources like virtualService and destination rules, we need to apply the CRD templates.</p> <pre><code>oc apply -f istio-1.3.3/install/kubernetes/helm/istio/templates/crds.yaml\n</code></pre> <p>Installing a simple Istio deployment for demonstration purpose use:</p> <pre><code>oc apply -f istio-1.0.5/install/kubernetes/istio-demo.yaml\n</code></pre> <p>On OpenShift we need to define Route to the Istio services:</p> <pre><code>oc expose svc istio-ingressgateway -n istio-system; \\\noc expose svc servicegraph -n istio-system; \\\noc expose svc grafana -n istio-system; \\\noc expose svc prometheus -n istio-system; \\\noc expose svc tracing -n istio-system\n</code></pre>"},{"location":"coding/istio/#cli","title":"CLI","text":"<p><pre><code>istioctl version\n</code></pre> Deploy an app and inject Envoy sidecar into the same pod.</p> <pre><code>oc apply -f &lt;(istioctl kube-inject -f ../../kubernetes/Deployment.yml) -n tutorial\n</code></pre>"},{"location":"coding/istio/#others","title":"Others","text":"<p>Service Graph displays a high-level overview of how systems are connected, a tool called Weave Scope provides a powerful visualisation and debugging tool for the entire cluster</p> <pre><code>kubectl create -f https://cloud.weave.works/launch/k8s/weavescope.yaml\n</code></pre>"},{"location":"coding/istio/#ab-testing-with-routing-rules","title":"A/B testing with routing rules","text":"<p>Route 50% traffic to one image</p> <p>Route based on http header argument</p>"},{"location":"coding/istio/#compendium","title":"Compendium","text":"<ul> <li>OpenShift Service Msh 2.0</li> <li>Installation on kubernetes</li> <li>Troubleshooting Istio</li> <li>Tutorial on Katacoda</li> <li>Istio on openshift training</li> <li> </li> <li> <p>A repo for demo</p> </li> </ul>"},{"location":"coding/reactjs/","title":"Reactjs and Next.js","text":""},{"location":"coding/reactjs/#nextjs","title":"Next.js","text":"<p>Next.js is a flexible React framework that gives developers building blocks to create fast, full-stack web applications.</p> <p>Learn Next.js tutorial. Some important concepts:</p> <ul> <li>Next.js uses file-system routing: instead of using code to define the routes of the application, you can use folders and files</li> <li>Next.js uses React Server Components, a new feature that allows React to render on the server. By moving rendering and data fetching to the server, you can reduce the amount of code sent to the client, which can improve your application's performance</li> </ul>"},{"location":"coding/reactjs/#create-a-nextjs-project","title":"Create a nextjs project","text":"<p>Better use pnpm for package manager. The following command is based on the dashboard app creation tutorial:</p> <pre><code>npx create-next-app@latest nextjs-dashboard --example \"https://github.com/vercel/next-learn/tree/main/dashboard/starter-example\" --use-pnpm\n</code></pre>"},{"location":"coding/reactjs/#reactjs","title":"React.js","text":""},{"location":"coding/reactjs/#key-concepts","title":"Key concepts","text":"<p>React components are JavaScript functions that return markup. React component names must always start with a capital letter, while HTML tags must be lowercase.</p>"},{"location":"coding/reactjs/#getting-started","title":"Getting started","text":"<p>React is a JavaScript library for building interactive user interfaces. To use it ina html page, load two scripts:</p> <pre><code>&lt;script src=\"https://unpkg.com/react@18/umd/react.development.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://unpkg.com/react-dom@18/umd/react-dom.development.js\"&gt;&lt;/script&gt;\n&lt;-- to compile jsx code to js --&gt;\n&lt;script src=\"https://unpkg.com/@babel/standalone/babel.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>In React a component is a function that returns UI elements. See the index.html</p> <p>In React, data flows down the component tree. This is referred to as one-way data flow. Props are read-only information that's passed to components. State is information that can change over time, usually triggered by user interaction. State is initiated and stored within a component</p> <ul> <li> <p>React foundation course</p> </li> <li> <p>Start a new project: Need a framework with reactjs to support webapp dev like next.js. </p> <ul> <li>Create backend folder with squeleton for a FastAPI tutorial from testdrivent.io</li> <li>Create a new Next.js project  (See Create React App)</li> </ul> <pre><code>npx create-next-app@latest frontend\n</code></pre> </li> <li> <p>This tutorial from product documentation.</p> </li> </ul>"},{"location":"data/","title":"Data Management Studies","text":"<p>There is perhaps nothing more valuable to an organization than its data.  In the ever-evolving landscape of digital transformation and customer empowerment, data serves for informed decision-making and propels businesses towards success.</p> <p>Organizations must be able to generate real-time insights based on accurate and high-quality data.</p> <p>In this chapter, we delve into the fundamental importance of leveraging data as a strategic asset. </p>"},{"location":"data/#business-cases","title":"Business cases","text":"<p>In today's data-driven world, organizations must address the following key objectives:</p> <ul> <li>Ensure that data is accessible to the individuals and teams who need it, empowering them to make informed decisions and drive innovation. Consider Data as an organizational asset.</li> <li> <p>Leverage advanced analytics and machine learning techniques to extract meaningful insights from data, to improve operational efficiencies and to discover new business opportunities.</p> </li> <li> <p>Implementing a data strategy centered around data lakes provides a scalable solution for ingesting, persisting, and consuming data, achieving an optimal price-performance ratio. It encompasses various components, including:</p> <ul> <li>Non-relational databases</li> <li>Big Data Analytics</li> <li>Relational database</li> <li>Machine learning</li> <li>Data Catalog and Governance</li> <li>Data warehousing</li> <li>Logs analytics</li> </ul> <p></p> </li> <li> <p>To gain a deeper understanding of how to put your data to work effectively. read the AWS presentation: Put your data to work with data lake.</p> </li> </ul>"},{"location":"data/#important-concepts","title":"Important concepts","text":""},{"location":"data/#big-data-and-the-5-vs","title":"Big Data and the 5 V's","text":"<p>When discussing Big Data, it refers to data that is rapidly generated from diverse sources, characterized by its massive volume, complexity, and the challenges organizations face in securing, analyzing, and extracting valuable insights from it.</p> <p>Traditional database and processing solutions are inadequate for addressing Big Data challenges. The concept of the \"5 V's\" encapsulates the key characteristics of Big Data:</p> <ol> <li> <p>Volume: Big Data involves extremely large amounts of data, ranging from terabytes to petabytes, and even exabytes. The size of data stored by organizations continues to grow exponentially, with global data creation projected to reach 180 zettabytes by 2025.</p> </li> <li> <p>Variety: Big Data encompasses a wide range of data types and formats, including structured, semi-structured, and unstructured data. It includes text, images, videos, sensor data, social media posts, and more.</p> </li> <li> <p>Velocity: Big Data is generated at a high velocity, often in real-time or near real-time. The speed at which data is produced and needs to be processed poses significant challenges for organizations.</p> </li> <li> <p>Veracity: Veracity refers to the trustworthiness and reliability of the data. Data integrity plays a crucial role in establishing the veracity of data. Ensuring data integrity is essential at every stage of the data lifecycle, including data creation, aggregation, storage, access, sharing, and archiving.</p> </li> <li> <p>Value: The ultimate goal of dealing with Big Data is to extract value and gain meaningful insights from it. By effectively analyzing and interpreting Big Data, organizations can derive valuable insights that lead to informed decision-making, innovation, and competitive advantage.</p> </li> </ol>"},{"location":"data/#data-lake","title":"Data Lake","text":"<p>A data lake serves as a powerful platform for unlocking the potential of big data and enabling AI and analytics capabilities.</p> <p>In the 1990s, enterprise data warehouses were commonly used as a means to support analytics and business intelligence. However, they often required significant upfront investments and were limited in their ability to handle the scale and variety of modern big data.</p> <p>In the mid-2000s, the emergence of technologies like Hadoop and ELK stacks revolutionized the concept of data lakes. These open-source frameworks embraced the use of open data formats and horizontal scaling, enabling organizations to ingest, process, and analyze massive amounts of data in a cost-effective manner.</p> <p>In the 2020s, data lakes have further evolved with the advent of cloud computing. Cloud-based data lakes offer numerous advantages, including pay-per-job pricing models, elastic scaling, and the ability to store data in object stores like Amazon S3 or Azure Blob Storage. </p> <p>Organizations are now leveraging streaming technologies and real-time data processing frameworks like Apache Kafka or Apache Flink to ingest and process data in near real-time before moving it to data lake.</p>"},{"location":"data/#data-topology","title":"Data topology","text":"<p>A data topology is an approach to classify, structure and manage real-world data scenarios. Data topology is an important consideration in designing and managing data architectures and systems. See separate note.</p>"},{"location":"data/#data-gravity","title":"Data gravity","text":"<p>Data gravity refers to the concept that as data accumulates and grows in size, it becomes increasingly difficult and costly to move that data to other locations or platforms.  The larger the volume of data and the more interconnected it becomes with applications and services, the more difficult it is to move data from its current location.</p> <p>Organizations can minimize the latency, costs, and complexities associated with data movement. Leverage data where it is. Organizations are encouraged to consider the location and distribution of their data when designing systems and workflows. They should consider edge processing and distributed processing.</p>"},{"location":"data/#data-management-requirements-and-context","title":"Data Management Requirements and Context","text":"<p>The requirements and context surrounding data management are influenced by various factors, including the distributed nature of data, the challenges introduced by hybrid cloud deployments, the impact of data gravity, and the need for efficient data preparation for analytics and AI.</p> <p>Let's explore these requirements and the broader context in more detail:</p> <ol> <li> <p>Accessibility of Data: Data is distributed across various sources such as applications, data repositories, data centers, and the cloud. Ensuring easy access to data regardless of its location is crucial for effective data management.</p> </li> <li> <p>Complexity of Hybrid Cloud Deployments: With hybrid cloud architectures, where analytics workloads may be in a dedicated environment separate from the data, complexities arise in integrating and managing data across different environments.</p> </li> <li> <p>Challenges of Data Transfer and Copying: Extensive copying and transferring of data can lead to performance issues, security vulnerabilities, governance concerns, data quality problems, and increased operational costs.</p> </li> <li> <p>Data Lake as a Gravity Force: Data lakes, due to their ability to store diverse and large datasets, act as a gravitational force that attracts data and applications. Leveraging the data within a data lake is an essential aspect of data management.</p> </li> <li> <p>Inefficiency of Moving Massive Datasets: Moving massive datasets into analytics clusters can be inefficient, expensive, and complex. Cloud technology provides scalable processing capabilities closer to the data, addressing some of those challenges.</p> </li> <li> <p>Data as the Foundation of Digital Business Transformation: Data plays a crucial role in digital business transformation initiatives, for developing new products, improving services, and driving innovation.</p> </li> <li> <p>Real-Time Insights and Operational Processes: Organizations aim to derive real-time insights from data to enable immediate use in business and operational processes. Low-latency data processing is a key requirement to achieve this objective.</p> </li> <li> <p>Data Preparation Challenges: Data preparation for analytics and AI poses several challenges. These include managing latency (time from which a transaction occurs to the time when the data is available for query) to ensure insights are available when needed, ensuring data security and quality, addressing data integration complexities, and automating manual data integration processes.</p> </li> <li> <p>Ethical and Regulatory Considerations: Organizations face ethical and regulatory challenges related to deploying AI models into production. Compliance with data governance, privacy regulations, and ethical considerations surrounding AI usage are significant concerns.</p> </li> </ol>"},{"location":"data/#recommendations","title":"Recommendations","text":"<p>Based on the requirements and context outlined, here are some recommendations for effective data management:</p> <ul> <li>Implement real-time analytics capabilities that process and derive insights from data streams at the source. This approach reduces the need for data movement and enables faster and more immediate decision-making.</li> <li>When adopting hybrid cloud deployments, consider the data gravity concept. Design a strategy that minimizes data movement and remote processing to reduce security risks and optimize performance.</li> <li>Whenever data needs to be migrated, assess the impact on data movement, performance, and associated costs.</li> <li>Consider implementing data virtualization technologies that allow for unified access to data across distributed sources. Data virtualization can help minimize data movement and simplify data integration processes.</li> </ul> Data Virtualization <p>Data Virtualization provides a unified and integrated view of data, regardless of its physical location or underlying data storage systems. Data sources may include databases, data warehouses, cloud storage, APIs, web services.  The data virtualization platform catalogs and indexes the metadata of the available data sources. Metadata includes information about the data structures, schemas, relationships, and access methods of the underlying data. This is an abstraction layer to hide the complexities of the physical data storage. The platform intelligently distributes the query processing across the relevant data sources, executing portions of the query where the data resides. The query result is presenting the data as if it were coming from a single data source. It allows real-time access to data, providing users with up-to-date information from the underlying sources. The platform ensures data confidentiality, integrity, and access control. Some open source projects that may help building a Data Virtualization platform: Apache Drill to query NoSQL DB, Apache Calcite to address SQL query optimization on any dat, anywhere. Apache Kafka is also considered as a virtualization platform. </p> <ul> <li>Leverage machine learning algorithms and models at the transactional processing platform. This approach can assist with data aggregation, summarization, integration, and transformation, enabling faster analysis and reducing the need for extensive data transfers.</li> <li>Leverage data topology methodologies to identify data sources, understand data semantics, and establish relationships between data producers and consumers. This helps in comprehending data flow and optimizing data management strategies.</li> </ul> <p>By implementing these recommendations, organizations can enhance their data management practices, reduce data movement complexities, optimize performance, improve security, and unlock the full potential of their data assets for analytics and AI initiatives.</p>"},{"location":"data/#sources","title":"Sources","text":"<ul> <li>Forester paper on data gravity</li> <li>Talend: Data Gravity: What it Means for Your Data</li> </ul>"},{"location":"data/#data-mesh","title":"Data Mesh","text":"<p>Data mesh is an architecture style that revolves around four key principles::</p> <ul> <li>Domain-oriented Decentralization of Data Ownership and architecture: Distribute data ownership to individual domains within an organization. Each domain takes ownership of its data and treats it as a valuable product.</li> <li>Domain-oriented Data served as a Product: teams are responsible for the quality, availability, and usability of their data products.</li> <li>Self-serve data infrastructure as a platform: to enable autonomous, domain-oriented data teams, to independently manage their data products.</li> <li>Federated governance to enable ecosystems and interoperability: enables collaboration and interoperability across domains. Data products are discoverable, addressable, and adhere to common standards and harmonization rules.</li> </ul> <p>Implementing a data mesh architecture requires not only the use of suitable tools but also organizational restructuring. One important aspect is introducing a data product role within each domain. Domains own and share their analytical data as products.</p> <p>Data mesh represents a paradigm shift from traditional approaches like data lakes, data warehouses, and business intelligence. It aligns with the third generation of data platforms, which are built on the Kappa architecture pattern, leverage real-time data processing, and utilize cloud-based object storage.</p> <p>In complex enterprise environments with diverse domains, numerous data sources, and multiple consumers, a centralized data platform often leads to adoption and usage challenges.</p> <p>When considering data mesh, it's essential to evaluate the size of the domain and the growth of data sources within its bounded context. Ingesting and storing data in a single place may hinder the easy addition of new data sources. </p> <p>A big data platform can inhibit the test and learn innovation cycle. On the other hand, siloed domain-oriented data is not a viable solution either. Some architectures organize data pipelines based on team capacity and skill, leading to processing feature requests without sufficient business knowledge of the source team and the consumers team.</p> <p>It is recommended to adopt the following practices within a data mesh architecture:</p> <ul> <li>Domains should host and serve their domain datasets in an easily consumable manner. Ownership and content of the data remain within the domain that generates them. This may involve duplicating data across different domains as it is transformed into a shape suitable for each specific domain.</li> <li>Shift from an ETL push and ingest model to a \"pull model\" that leverages event streams for serving data. This promotes real-time data access and agility.</li> <li>Consider the domain as the architecture quantum instead of focusing on specific pipeline stages. This ensures that data management aligns with the domain boundaries and requirements.</li> <li>Source domain datasets should represent the facts and reality of the business. These facts, often referred to as \"truths of the business domains,\" can be stored and served as append logs with immutable, time-stamped records.</li> <li>Domains should provide business aggregates that are easy to consume. For example, the \"account opened per month\" aggregate can be available within the account domain.</li> <li>New data domains can be created by joining and correlating data from existing domains. This enables the creation of richer and more comprehensive datasets.</li> <li>Distinguish between physical system datasets and domain datasets. Physical system datasets are specific to the underlying systems and differ from domain datasets, which are larger in volume, have timed records, and change less frequently.</li> <li>Some domains align closely with specific consumption patterns and have different characteristics compared to source domains. They transform source domain events into aggregate views and structures that fit a particular access model.</li> <li>Data pipeline activities such as cleansing, preparation, aggregation, and data serving remain important but should stay within the domain boundaries.</li> <li>Adopt a product thinking approach for data assets, treating them as products and considering data scientists and data engineers as the customers of these products.</li> </ul> <p></p> <ul> <li>Establish a data catalog that provides metadata such as ownership, source of origin, structure, and samples for each data product. Each data product should register with the catalog to ensure discoverability.</li> <li>Ensure each data product has a unique address per storage format, making it easily accessible and addressable.</li> <li>Provide service level objectives (SLOs) that ensure data truthfulness. Data provenance and data lineage associated with each data product enhance consumer confidence.</li> <li>Foster interoperability by following standards and harmonization rules when correlating data across domains.</li> <li>Apply secure access controls to datasets at the time of access for each data product.</li> <li>Implement a common data infrastructure as a self-serving platform combined with global governance to ensure interoperability and consistency across the data mesh ecosystem.</li> </ul>"},{"location":"data/#sources_1","title":"Sources","text":"<ul> <li>How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh</li> <li>The Data Dichotomy: Rethinking the Way We Treat Data and Services</li> </ul>"},{"location":"data/cassandra/","title":"Cassandra Summary","text":"<p>This article is summary of Apache Cassandra which is a NoSQL, row-oriented, highly scalable and highly available database. It was created by Facebook team and given as an open-source to Apache Foundation.</p> <p>Cassandra addresses linear scalability and high availability to persist a huge  data set. It uses replication to multiple nodes, managed in cluster, even deployed cross data centers. <p>The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data.</p>"},{"location":"data/cassandra/#key-value-propositions","title":"Key value propositions","text":"<ul> <li>Handle massive datasets distributed across multiple nodes, allowing for high read and write performance</li> <li>High availability and fault tolerance even between data centers. It supports always-on availability with zero downtime. It uses a peer-to-peer architecture and automatic replication to ensure that data is never lost.</li> <li>Distributed and Decentralized Architecture: Cassandra's decentralized architecture eliminates single points of failure and provides fault tolerance.</li> <li>Linear Scalability: even as more nodes are added to the cluster</li> <li>Flexible Data Model: Cassandra's data model is based on a wide column store, which allows for flexible schema design. It supports dynamic column addition and removal, making it suitable for applications with evolving data requirements.</li> <li>Fast writes: Cassandra is optimized for high-speed writes, making it well-suited for applications that need to capture large volumes of data in real-time, such as IoT, sensor data, and logging</li> <li>Query flexibility: Cassandra supports a variety of query methods, including lightweight transactions, batches, and secondary indexes, to provide flexible data access.</li> </ul>"},{"location":"data/cassandra/#concepts","title":"Concepts","text":"<p>Cassandra uses a ring based Distributed Hash Table servers but without finder or routing tables. Keys are stored in DHT to one server and replicated on the 2 next neighbor servers.  There is one ring per data center.  The coordinator forwards the query to a subset of replicas for a particular key. Every server that could be a coordinator needs to know the 'key to server' assignment.  </p> <p>Two data placement strategies:</p> <ul> <li> <p>simple strategy: use two kinds of partitioners:</p> </li> <li> <p>random, which does operation like hashing</p> </li> <li> <p>byte ordered, which assigns range of keys to servers: easier for range queries</p> </li> <li> <p>network topology strategy: used for multiple data centers. It supports different configuration, like 2 replicas of each key per data center.</p> </li> </ul> <p>First  replica is placed according to the Partitioner logic: it makes sure to store the other replica to different rack, to avoid a rack failure which may make all key copies not available.  Go around the ring clockwise until encountering a server in a different rack.</p> <p>Snitches: is a mechanism to map ip addresses to racks in DC. Cassandra supports such configuration.</p> <p>Client sends writes to one coordinator node in Cassandra cluster. Every write operation is first stored in the commit log. It is used for crash recovery. Coordinator may be per key or per client or per query. </p> <p>Write operations are always successful, even in case of failure: the coordinator uses the Hinted Handoff mechanism (it assumes ownership of the key until being sure it is supported by the replica), as it writes to other replicas and keeps the write locally until the down replica comes back up. If all the replicas are done, the Coordinator buffers writes for few hours. </p> <p>After data is written to the commit log it then is stored in Mem-Table(Memory Table) which remains there till it reaches to the threshold. </p> <p>Finally, Sorted-String Table or SSTable is a disk file which stores data from MemTable once it reaches to the threshold. SSTables are stored on disk sequentially and maintained for each database table.</p> <p>There are three types of read operations: 1/ Direct-request where coordinator send read query to one of the replicas and a digest request to other replicas to ensure returned data are up-to-date. 2/ Digest-request: returned rows from each replica are compared in memory for consistency 3/ Read-repair: in case of data is not consistent across the node, a background read repair request is initiated that makes sure that the most recent data is available across the nodes.</p> <p>It uses partitioner to send query to all replica nodes responsible for a partition key. The process to write should be fast and not involving lock on resource. It should not involve read and disk seeks.</p> <p>Here are some key concepts of Cassandra to keep in mind:</p> <ul> <li>Cluster:  the set of nodes potentially deployed across data centers, organized as a 'ring'.</li> <li>Keyspace: like a schema in SQL DB. It is the higher abstraction object to contain data. The important keyspace attributes are the Replication Factor, the Replica Placement Strategy and the Column Families.</li> <li>Column Family: they are like tables in Relational Databases. Each Column Family contains a collection of rows which are represented by a Map&gt;. The key gives the ability to access related data all together <li>Column \u2013 A column is a data structure which contains a column name, a value and a timestamp. The columns and the number of columns in each row may vary in contrast with a relational database where data are well structured.</li>"},{"location":"data/cassandra/#define-assets-table-structure-with-cql","title":"Define Assets Table Structure with CQL","text":"<p>Cassandra Query Language (CQL) is very similar to SQL but suited for the JOINless structure of Cassandra.</p> <p>Using the csql tool we can create space and table. To use <code>cqlsh</code> connect to cassandra container:</p> <pre><code>docker exec -ti cassandra_1 cqlsh\n# k8s\n$ kubectl exec -tin greencompute cassandra_1 cqlsh\n</code></pre> <p>You are now in cqlsh shell and you can define assets table under keyspace  <code>assetmonitoring</code>:</p> <pre><code>sqlsh&gt;  create keyspace assetmonitoring with replication={'class':'SimpleStrategy', 'replication_factor':1};\nsqlsh&gt; use assetmonitoring;\nsqlsh:assetmonitoring&gt; create TABLE assets(id text PRIMARY KEY, os text, type text, ipaddress text, version text, antivirus text, current double, rotation int, pressure int, temperature int, latitude double, longitude double);\n</code></pre> <p>Add an index on the asset operating system field and one on type.</p> <pre><code>CREATE INDEX ON assetmonitoring.assets (os);\nCREATE INDEX ON assetmonitoring.assets (type);\n</code></pre> <p>If you reconnect to the pod using cqlsh you can assess the table using</p> <pre><code>describe tables\n\ndescribe assets\n</code></pre>"},{"location":"data/cassandra/#some-useful-cql-commands","title":"Some useful CQL commands","text":"<pre><code># See the table schema\ncqlsh&gt; describe table assets;\n\n# modify a table structure adding a column\ncqlsh&gt; alter table assets add flowRate bigint;\n\n# change column type. example the name column:\ncqlsh&gt; alter table assets alter name type text;\n\n# list content of a table  \ncqlsh&gt; select id,ipaddress,latitude,longitude from assets;\n\n# delete a table\ncqlsh&gt; drop table if exists assets;\n</code></pre>"},{"location":"data/cassandra/#cassandra-deployment","title":"Cassandra deployment","text":"<p>Running locally with docker compose on a two nodes topology. See docker compose file</p> <p>Looking to hosted managed service like AWS Keyspaces , DataStax enterprise</p>"},{"location":"data/cassandra/#deployment-on-openshift","title":"Deployment on OpenShift","text":"<p>Cassandra is deployed via operator. Through monitoring pod state via Kubernetes callbacks the operator handles day to day operations such as restarting failed processes, scaling clusters up, and deploying configuration changes in a rolling, non-disruptive, fashion.</p> <p>The operator is designed to be Namespace scoped.</p> <p>Deploying stateful distributed applications like Cassandra is not easy. You will leverage the kubernetes cluster to support high availability and deploy c7a to the worker nodes.</p> <p></p> <p>We also recommend to be familiar with this kubernetes tutorial on how to deploy Cassandra with Stateful Sets.</p>"},{"location":"data/cassandra/#performance-considerations","title":"Performance considerations","text":"<p>The resource requirements for higher performance c7a node are:</p> <ul> <li>a minimum of 32GB RAM (JVM + Linux memory buffers to cache SSTable) + <code>memory.available</code> defined in Kubernetes Eviction policy + Resources needed by k8s components that run on every worker node (e.g. proxy)</li> <li>a minimum of 8 processor cores per Cassandra Node with 2 CPUs per core (16 vCPU in a VM)</li> <li>4-8 GB JVM heap, recommend trying to keep heaps limited to 4 GB to minimize garbage collection pauses caused by large heaps.</li> <li>Cassandra needs local storage to get best performance. Avoid to use distributed storage, and prefer hostPath or localstorage. With distributed storage like a Glusterfs cluster you may have 9 replicas (3x Cassandra replica factor which is usually 3)</li> </ul> <p>Cassandra nodes tend to be IO bound rather than CPU bound:</p> <ul> <li>Upper limit of data per node &lt;= 1.5 TB for spinning disk and &lt;= 4 TB for SSD</li> <li>Increase the number of nodes to keep the data per node at or below the recommended capacity</li> <li>Actual data per node determined by data throughput, for high throughput need to limit the data per node.</li> </ul> <p>The use of Vnodes is generally considered  to be a good practice as they eliminate the need to perform manual token assignment, distribute workload across all nodes in a cluster when nodes are added or removed. It helps rebuilding dead nodes faster. Vnode reduces the size of SSTables which can improve read performance. Cassandra best practices set the number of tokens per Cassandra node to 256.</p> <p>Avoid getting multiple node instances on the same physical host, so use <code>podAntiAffinity</code> in the StatefulSet spec.</p> <pre><code>spec:\n  affinity:\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        topologyKey: \"kubernetes.io/hostname\"\n</code></pre>"},{"location":"data/cassandra/#using-yaml-configurations","title":"Using  yaml configurations","text":"<p>You can reuse the yaml config files under <code>deployment/cassandra</code> folder to configure a Service to expose Cassandra externally, create static persistence volumes, and use the StatefulSet to deploy Cassandra image.</p> <p>The steps to deploy to K8S are:</p> <ol> <li> <p>Connect to your k8s. You may want to get the admin security token using the Admin console and the script: <code>scripts/connectToCluster.sh</code>.  </p> <p>We are using one namespace called 'greencompute'.</p> <p>You can also use our script <code>deployCassandra.sh</code> under the <code>../scripts</code> folder to automate this deployment.</p> </li> <li> <p>create Cassandra headless service, so application accesses it via KubeDNS. If you do wish to connect an application to cassandra, use the KubeDNS value of <code>cassandra-svc.greencompute.svc.cluster.local</code>, or <code>cassandra-svc</code> or <code>cassandra-0</code>. The alternate solution is to use Ingress rule and set a hostname as cassandra.green.case. The <code>casssandara-ingress.yml</code> file defines such Ingress.</p> <pre><code>$ kubectl apply -f deployment/cassandra/cassandra-service.yaml --namespace greencompute\n$ kubectl get svc cassandra-svc -n greencompute\n\nNAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\ncassandra-svc   ClusterIP    None          &lt;none&gt;        9042/TCP   12h\n</code></pre> </li> <li> <p>Create Create static persistence volumes to keep data for cassandra: you need the same number of PV as there are cassandra nodes (here 3 nodes)</p> <pre><code>$ kubectl apply -f deployment/cassandra/cassandra-volumes.yaml\n\n$ kubectl get pv -n greencompute | grep cassandra\n    cassandra-data-1  1Gi  RWO  Recycle   Bound       greencompute/cassandra-data-cassandra-0 12h\n    cassandra-data-2  1Gi  RWO  Recycle   Available                                           12h\n    cassandra-data-3  1Gi  RWO  Recycle   Available   \n</code></pre> </li> <li> <p>Create the StatefulSet, which defines a cassandra ring of 3 nodes. The cassandra image used is coming from dockerhub public repository.</p> <p>if you are using your own namespace name or you change the service name, modify the service name and namespace used in the yaml :</p> <pre><code>env:\n    - name: CASSANDRA_SEEDS\n    value: cassandra-0.cassandra-svc.greencompute.svc.cluster.local\n</code></pre> <p>Cassandra seed is used for two purposes:</p> <ul> <li>Node discovery: when a new cassandra node is added (which means when deployed on k8s, a new pod instance added by increasing the replica), it needs to find the cluster, so here it is set the svc</li> <li>assist on gossip convergence: by having all of the nodes in the cluster gossip regularly with the same set of seeds. It ensures changes are propagated regularly.</li> </ul> <p>Here it needs to reference the headless service we defined for Cassandra deployment.</p> <pre><code>$ kubectl apply -f deployment/cassandra/cassandra-statefulset.yaml  -n greencompute\n$ kubectl get statefulset -n greencompute\n\nNAME                                        DESIRED   CURRENT   AGE\ncassandra                                   1         1         12h\n</code></pre> </li> <li> <p>Connect to the pod to assess the configuration is as expected.  </p> <pre><code>$ kubectl get pods -o wide -n greencompute\nNAME          READY     STATUS    RESTARTS   AGE       IP              NODE\ncassandra-0   0/1       Running   0          2m        192.168.35.93   169.61.151.164\n\n$ kubectl exec -tin greencompute cassandra-0 -- nodetool status\n\nDatacenter: DC1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address          Load       Tokens        Owns (effective)  Host ID                               Rack\nUN  192.168.212.174  257.29 KiB  256          100.0%            ea8acc49-1336-4941-b122-a4ef711ca0e6  Rack1\n</code></pre> </li> </ol> <p>The  string UN, is for Up and Normal state.</p>"},{"location":"data/cassandra/#removing-cassandra-cluster","title":"Removing cassandra cluster","text":"<p>We are providing a script for that <code>./scripts/deleteCassandra.sh</code> which remove the stateful, the pv, pvc and service</p> <pre><code>grace=$(kubectl get po cassandra-0 -o=jsonpath='{.spec.terminationGracePeriodSeconds}') \\\n    &amp;&amp; kubectl delete statefulset -l app=cassandra -n greencompute \\\n    &amp;&amp; echo \"Sleeping $grace\" \\\n    &amp;&amp; sleep $grace \\\n    &amp;&amp; kubectl delete pvc,pv,svc -l app=cassandra\n</code></pre>"},{"location":"data/cassandra/#high-availability","title":"High availability","text":"<p>Within a cluster the number of replicas in the statefulset is at least 3 but can be increased to 5 when code maintenance is needed. The choice for persistence storage is important, and the backup and restore strategy of the storage area network used.</p> <p>When creating connection to persist data into a keyspace, you specify the persistence strategy and number of replicas at the client code level. Mostly using properties file.</p> <pre><code>p.setProperty(CASSANDRA_STRATEGY, \"SimpleStrategy\");\np.setProperty(CASSANDRA_REPLICAS, \"1\");\n</code></pre> <p>When deploying on Staging or test cluster, ring topology, SimpleStrategy is enough: additional replicas are placed on the next nodes in the ring moving clockwise without considering topology, such as rack or datacenter location. For HA and production deployment NetworkTopologyStrategy is needed: replicas are placed in the same datacenter but on distinct racks.</p> <p>For the number of replicas, it is recommended to use 3 per datacenter.</p> <p>The <code>spec.env</code> parameters in the statefulset defines the datacenter name and rack name too.</p>"},{"location":"data/cassandra/#cassandra-client-api","title":"Cassandra client API","text":"<p>In the pom.xml we added the following dependencies to get access to the core driver API:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;\n  &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;\n  &lt;version&gt;3.1.4&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The DAO code is <code>CassandraRepo.java</code> and it basically connects to the Cassandra cluster when the DAO class is created...</p> <pre><code>Builder b = Cluster.builder().addContactPoints(endpoints);\ncluster = b.build();\nsession = cluster.connect();\n</code></pre> <p>The trick is in the endpoints name. We externalize this setting in a configuration properties and use the cassandra-svc name when deploy in ICP.</p> <p>It is possible also to create keyspace and tables by API if they do not exist by building CQL query string and use the session.execute(aquery) method. See this section below</p>"},{"location":"data/cassandra/#coding","title":"Coding","text":""},{"location":"data/cassandra/#use-cassandra-java-api-to-create-objects","title":"Use Cassandra Java API to create objects","text":"<ul> <li>Create keyspace:</li> </ul> <pre><code>StringBuilder sb =\n    new StringBuilder(\"CREATE KEYSPACE IF NOT EXISTS \")\n      .append(keyspaceName).append(\" WITH replication = {\")\n      .append(\"'class':'\").append(replicationStrategy)\n      .append(\"','replication_factor':\").append(replicationFactor)\n      .append(\"};\");\n\n    String query = sb.toString();\n    session.execute(query);\n</code></pre> <ul> <li>Create table</li> </ul> <pre><code>StringBuilder sb = new StringBuilder(\"CREATE TABLE IF NOT EXISTS \")\n         .append(TABLE_NAME).append(\"(\")\n         .append(\"id uuid PRIMARY KEY, \")\n         .append(\"temperature text,\")\n         .append(\"latitude text,\")\n         .append(\"longitude text);\");\n\n       String query = sb.toString();\n       session.execute(query);\n</code></pre> <ul> <li>insert data: there is no update so if you want a strict insert you need to add \"IF NOT EXISTS\" condition in the query.</li> </ul>"},{"location":"data/cassandra/#issues","title":"Issues","text":"<ol> <li>When having the cassandra replica set to more than two, the cassandra operations are not happenning parallely on both the pods at a time. Some operations are happenning on one node and some others on the other node. Due to this, inconsistent data is retrieved since both the tables doesnot have the same data.</li> </ol>"},{"location":"data/cassandra/#future-readings","title":"Future Readings","text":"<ul> <li>Getting Started documentation</li> <li>Getting started article on Cassandra and Python</li> <li>10 steps to set up a multi-data center Cassandra cluster on a Kubernetes platform</li> <li>IBM Article: Scalable multi-node Cassandra deployment on Kubernetes Cluster</li> <li>Running Cassandra on Kubernetes</li> </ul>"},{"location":"data/data-integration/","title":"Data integration","text":"<p>There is no universal database solution to address the variety of data requirements business application's needs. And it is not wish-able to have such monster product. So the needs for data integration should come by considering the dataflows across a business process at stake or across an entire business organization and even sometime in a network of business partners (a blockchain network as an example). </p>"},{"location":"data/data-integration/#dataflows","title":"Dataflows","text":"<p>At the application level, dataflow definition focuses to address where the data is created, where it is persisted, and how other components access the data. With microservice adoption, the root entity or aggregate will be managed by one service, and persisted in its boundary. There will be multiple databases, or multiple schemas within database cluster, and there will be different database type: document, relational, graph, object, files...  User interface external to this microservice will get the forms, for user will use to enter data. B2B integration can also receive data. User interface or B2B integration needs to use APIs at the microservice level to push those data. Event driven microservice will produce and consume events with un-mutable data.  At the higher level, a domain level, independent applications will communicate by exchanging data via service operations, via point to point messaging, via pub/sub mechanism, or by getting data using batch jobs and files.   All those things are part of the dataflows.</p> <p>As another example when implementing microservices using the CQRS pattern, we need to pay attention on what the write model is, and what are the read representation needs to be, and built from which sources. For each expected query, developer needs to define the formats, the local persistence mechanism, and how it will get data from other microservices. The good news is that, as query part are autonomous deployed microservice or even function as a service, they can be build independant to the bigger microservice responsible of the write operation.</p> <p>As part of this clear separation of read, write models, and aggregate per microservice, we also need to think about data consistency.</p> <p>The classical approach to keep different systems data consistent, is to use two-phase commit transaction, and the orchestrator to keep the write operation in expected sequence. In the world of microservice with REST end points, it is not possible to do two phase commit transaction across multiple microservices (see next section on the CAP theorem). Alternate model is to use event sourcing to keep write order in a log, and adopt eventual consistency, where we accept a small lag time in the data view.</p>"},{"location":"data/data-integration/#cap-theorem","title":"CAP Theorem","text":"<p>Data replication in distributed computing, like the cloud, falls into the problem of the CAP theorem where only two of three properties consisting of Consistency, Availability and netowrk Partition Tolerance can be met simultaneously. In most microservice implementation context, Consistency (see the same data at any given point in time) and Availability (reads and writes will always succeed, may be not on the most recent data) are in trade off. </p> <p>The diagram below explains the mapping of data persistence product types with the CAP theorem dimensions.</p> <p></p>"},{"location":"data/data-integration/#transaction","title":"Transaction","text":"<p>We do not want to write another book on transaction, but this is important to keep the followings in mind when addressing application design and architecture:</p> <ul> <li>With ACID transaction, atomicity, isolation and durability are database characteristics and are the key mechanisms for fault tolerance . Consistency is application specifics and depends on what the code set in the transaction.</li> <li>Atomicity guarantees that write operations are all done in one unique operation or aborded in case of fault. </li> <li>Consistency assesses that the data invariants are always true. This is in the application scope, database can only verifies some constraints, like on null, key integrity, data type, length,... Database can guaranty foreign key and constraints, but not business logic invariants.</li> <li>Isolation guarantees concurrently executing transactions are isolated from each other. It protects the clients to read the same row at the same time or write in the same table at the same time. Transactions are isolated from each other. This approach impacts performance. So modern databases are using weaker algorithm for isolation to address a good level of scalability.</li> <li>Durability is to keep data without losing it overtime. It means saving to a durable disk, and avoid data corruption. The transaction could only be considered successful if the write operation on disk is done, or in case of cross node replication, when all the nodes have done their work.</li> <li>ACID transaction is not about concurrency (like it is in multithreading) but it is the ability to abort a transaction on error and have all writes from that transaction discarded.</li> </ul>"},{"location":"data/data-integration/#transaction-in-distributed-system","title":"Transaction in distributed system","text":"<p>Any application using data may consider some potential issues like:</p> <ul> <li>database software or server hardware failure</li> <li>network failure</li> <li>application crashes while writing to the database</li> <li>concurrent update at the same time, overwriting each others</li> <li>reading data while write operation happened</li> </ul> <p>So making application reliable is quite a challenge and is a lot of work and testing. The way to simplify the coding of data centric application is to use transaction.</p>"},{"location":"data/data-integration/#when-to-use-transaction","title":"When to use transaction","text":"<p>A transaction is a way for an application to group several reads and writes together into a logical unit. The read or write operations succeed (commit) or fail (abort and rollback to avoid side effect).</p> <p>The majority of stateful business service, micro or monolithic, should use transaction. The aggregate and data responsibility define the scope of the transaction and the microservice bounded context.</p> <p>For higher availability and higher performance, application needs to run in parallel, and therefore some safety guards needs to be done without transaction.</p> <p>The introduction of document oriented database, focused on a non relational data model, where relationship integrity is less important, while partitioning, replication are more important to address fault tolerance and scalability.</p>"},{"location":"data/data-partitioning/","title":"Data partitioning","text":"<p>The goal of data partitioning is to spread the data and query load evenly across multiple machines, avoiding nodes with disproportionately load: it helps to scale. Partitions are placed in different nodes in a shared nothing cluster. Query in one partition scale well, and can address load evenly accross nodes. Partition is combined with replication techniques for fault tolerance, so copies of each partition are stored on multiple nodes. Using the leader-follower approach, one node could be leader on one partition and follower for others. This is how Kafka does partition replication. </p> <p>Partitioning algorithm needs to take care of data assignment by avoiding one partition to get all the load. One approach is to use key-value data model and use the key to assign to partition. Sequential keys are assigned into the same partition until reaching a specific boundary. Those boundaries are adapted to the data and keys. The partitioning by key hashing code is the choice for a lot of document oriented databases. The hash function needs to be language agnostic, for example Object.hashCode() of java could not be used. Partition assignement is then based on a range of hashes.</p> <p>Still this technique may not work for highly used key: for example a very famous twitter account, will have millions of reads on the same partition. The application may manage the key assignment and may append small random number to the key string.</p> <p>Keys are good for partitioning, but persisted data need to be easily searchable. So secondary index are needed. Index can be managed locally to each partition or globally cross partitions. With local indexing, write operations are efficient as they work on a unique partition but a read / search goes accross all partitions to aggregate the results which takes more time. This is the scatter / gather approach.</p> <p>With global indexing, the secondary indexes are partitionned and the query operation may be more efficient as it will reach a unique partition. But the write operation will be costly. In RDBMS the index write operation will be transactional. In distributed system it will be asynchronous, and so a query done immediately after a write may not see the consistent data.</p>"},{"location":"data/data-partitioning/#rebalancing-partitions","title":"Rebalancing partitions","text":"<p>A lot of things happen in distributed system: more data are added forcing to add more nodes, node fails, query throughput increases... Data partition needs to be reallocated to different nodes. This is the partition rebalancing process. It needs to ensure data is kept fairly distributed over the cluster after done. It needs to be active and serves new read and write operations, and it needs to move the minimum of data to get good performance.</p> <p>The approaches include using fixed number of partitions, more than current number of nodes and then reallocate a full partition when new node is added, or use dynamic partitioning where the rebalancing occurs when the size of the first partition reach a certain threshold. With dynamic partitioning, the number of partitions is proportional to the size of the dataset, since the splitting and merging processes keep the size of each partition between some fixed minimum and maximum numbers. Another approach is to have a fixed number of partition per node. When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of one half of each of those splitted partitions while leaving the other half of each partition in place. For example, Cassandra uses 256 partitions per node.</p> <p>Rebalancing is an expensive operation, because it requires rerouting requests and moving a large amount of data from one node to another. So some implementation is expecting to have human configuring and triggering the rebalancing according the observed state of the partitioning.</p>"},{"location":"data/data-partitioning/#request-routing-or-service-discovery","title":"Request routing or service discovery","text":"<p>How a client find data in partition and node? Again, there are different approaches:</p> <ul> <li>client can contact any node, and if a node getting the request does not have the data, it forwards to another node that may respond to it, so the first node passes the reply to the client.</li> <li>client sends the request to a routing agent, which determines which node to reach. It is a partition aware load balancer.</li> <li>client is aware of the partition to node assignment. so it can contact the node directly.</li> </ul> <p>On each case, partition rebalancing impacts the routing. There are some complex protocol to achieve consensus. Zookeeper is a coordination system to keep track of the cluster metadata. Another approach is to use a gossip protocol to dissimate changes of the cluster state between nodes.</p>"},{"location":"data/data-partitioning/#parallel-queries","title":"Parallel queries","text":"<p>In analytics we need to perform complex queries combining filtering, joins, grouping by and aggregates.  Massively parallel processing is used to split the query into stages and partitions to be executed in parallel. </p>"},{"location":"data/data-replication/","title":"Data replication","text":"<p>Abstract</p> <p>In this article, we are dealing with data replication from traditional Database, like DB2, to a  microservice environment with document oriented or relational data. Database replications can be used  to move data between environments or to separate read and write models, which may be a viable solution   with microservices, but we need to assess how to support coexistence where older systems run in   parallel to microservices and use eventual data consistency pattern. </p> <p>We recommend reading the 'Ensure data resilience' article from Neal Fishman  to understand the problems related to data resilience and how they fit into data topology and governance broader discussions.</p>"},{"location":"data/data-replication/#concepts","title":"Concepts","text":"<p>Data Replication is the process of storing data on more than one storage location on same or different data centers.  It fits well, when dataset is small enough to be persisted in one machine, and when data do not change over time. Data replication encompasses duplication of transactions on an ongoing basis, so that the replicates are in a consistently  updated state and synchronized with the source.</p> <p>The database centric replication mechanism involves different techniques to apply data replication between databases of  the same product (DB2 to DB2, Postgresql to Postgresql). The approach is to control availability and consistency.</p> <p>Replication implementations are most of the time, black box for the business application developers, but we want to present  three types of replication architecture:</p> <ul> <li> <p>single-leader: one of the replica is the leader, and receive write operations from client services. Other replicas  are followers, listening to data updates from a replication log and modify their own data store. Read operations can  happen on any node, and followers are specifically read-only. This is a pattern used by Kafka, Postgresql, RabbitMQ...</p> <p></p> </li> <li> <p>multi-leader: multiple nodes accept write operations (they are leaders), and act as follower for other leaders. </p> </li> <li>leaderless: each node accept write operations. Client sends write to multiple replicas, accept p acknowledges  (p &lt;= number of nodes), but also performs n reads to assess eventual stale data. (This type is used in Cassandra or DynamoDB)</li> </ul>"},{"location":"data/data-replication/#single-leader","title":"Single leader","text":"<p>All write requests are done on the leader, but reads can happen on followers as well. It helps to scale out the solution  and also to support long distance replications. Replication can be done synchronously or asynchronously. With synchronous the leader waits until at least one follower  has confirmed replication before reporting to the client that the write operation is successful. From the client point  of view, it is a synchronous call.</p> <p></p> <p>With asynchronous the leader does not wait for followers to replicate. With synchronous, we are sure the data are not  lost as at least one follower responded, but if the follower node has failure then the write operation is not consider  completed, even if leader has the data. The client can do a retry. But the leader may need to block other write requests  until it sees a follower alive. With synch, the data is also consistent with the leader. The figure above illustrates  a synchronous mechanism for client - leader and first follower to respond, and asynchronous for the other followers.</p> <p>It is important to note that asynch replica may be the only possible solution when the number of followers is important  or when they are separated by long distance with high latency networking communication.  Because a full synchronous  mechanism will block the write operations in case of a follower failure and will not be reliable.</p> <p>With asynch, two reads at the same time on the leader and one of the follower will not get the same results.  This inconsistency is temporal: when there is no more write then the followers will become eventually consistent.  This elapse time is called replication lag.</p> <p></p> <p>This lag can have some interesting derived issues, like seeing data in one query (done on a follower with small lag)  and then not seeing the data from the same query done on a bigger lagged follower. To avoid that, the pratice is to use  a monolitic read: user makes several reads in sequence, they will not see time go backward. Which can be achieved  by ensuring the reads for a given user are done on the same follower. This is what kafka does by assigning a consumer  to partition.</p> <p>With asynch replica, if the leader fails, data not yet replicated to the followers, is lost (write operation 2 and 3 in figure below). When a new leader is elected, write operation succeed. </p> <p></p> <p>Also adding a new follower brings other challenges: as the data are continuously being written to the leader database,  copying the database files to the new follower will not be consistent. One adopted solution is to snapshot the database,  without locking write operations, and copy the data to the follower.  Then from this snapshot, consumes the update log. The snapshot position needs to be known in the replication log. </p> <p></p> <p>To support follower failure, the log needs to keep position of the last commited read, so when the follower restarts,  it can load data from this position in the log. </p> <p>When the leader fails, a follower needs to take the leadership, and then other followers need to get data from the new  leader. Kafka, combined with zookeeper, uses those last two mechanisms to support replication. Selecting the new leader  is based on consensus algorithm, to ensure minimum data lost.  </p> <p>When using single leader, it is possible to reach a split brain state, when the old leader comes back to live, and  thinks it is still a leader: both leaders accept writes which leads to data corruption. </p> <p>Most RDBMS uses the master-slave pattern combined with asynchronous replication. Postgresql uses a file based log  shipped to the followers when a transaction is committed. Logical replication starts by copying a snapshot of the data  on the publisher database. Once that is done, changes on the publisher are sent to the subscriber as they occur in real time. The subscriber applies data changes in the order in which commits were made on the publisher side so that transactional consistency is guaranteed for the publications within any single subscription.  DB replications are covered with RDBMS features and it offers low administration overhead. It is important to note  that source and target data structures have to be the same, and change to the table structure is a challenge but  can be addressed (usually mitigated by 3<sup>rd</sup> party tooling).</p> <p>Now the adoption of database replication features is linked to the data movement requirements. It may not be a suitable  solution for microservice coexistance as there is a need to do data transformation on the fly.</p>"},{"location":"data/data-replication/#multi-leaders","title":"Multi leaders","text":"<p>With multi-leader configuration each leader gets write operations, and propagates data update notifications to all nodes.  This is an active-active replication. And it is the practice when dealing with multiple datacenters. </p> <p></p> <p>The write operations are propagated asynchronously which is more permissive to network failure. It is important to  note that the following problems need to be addressed with this topology:</p> <ul> <li>Update of the same data at the same time, leading to write conflict</li> <li>Using automatic primary key generation by the database may create duplicate keys</li> <li>Triggers that work on conditions on the data to do something could never be triggered due to data conditions  that will never happen in this active - active configuration.</li> <li>Integrity constraints can be violated, as one records may be processed without the others being yet present.</li> </ul> <p>For the write conflict resolution, there are different strategies, one uses timestamps so the last write wins  the record update, but this could lead to data lost. In fact conflict resolution will be dependant of the  business logic and data knowledge. So custom code needs to be done to apply this logic to write conflicts.</p> <p>When there is more than two leaders, the replicas topology can be use a ring, star or all-to-all model, as illustrated  in the figure below:</p> <p></p> <p>With Ring, a leader forwards and propagates its own writes to one neighbor. With Star one designated root node  forwards writes to all of the other nodes. Finally with All-to-all, every leader sends its writes to every other leader.</p> <p>With Ring and Star, a node failure impacts the data replication to any node, and with all-to-all we need to address  looping on the same write. The mitigation is to use a unique identifier for each node, so a write event coming with the  same node_id at the current node id is discarded.</p>"},{"location":"data/data-replication/#leaderless","title":"Leaderless","text":"<p>In this last replication technique, the client application is doing write operation on any replicas. There is no leader.  This is the approach used by Cassandra and DynamoDB system. The write operations order is not maintained. With leaderless  failover does not exist, in case of node failure the client has to accept n missing acknowledges. If the client read  back data from a previously failed node, that just restarted, it may read old / stale data. To mitigate this problem,  reads are done to multiple nodes in parallel, and the client needs to consolidate the returned value (may be using  timestamp or version number). </p> <p>A node that is restarting can catch up on the replicated data by doing read repair or by using an anti-entropy process.  Read repair, is done by the client seeing old value from one of the replica, and push back the new value to it.  The anti-entropy process is in the database mechanism, with a daemon process running to replicate missing data to any  replicas.</p>"},{"location":"data/data-replication/#considerations","title":"Considerations","text":"<p>The main advantages for data replication (providing a consistent copy of data across data storage locations) are:</p> <ul> <li>increased data availability</li> <li>increased access performance by moving data close to users</li> <li>decreased network load</li> </ul> <p>Maintaining Data consistency at all different sites involves complex processes, and increase physical resources.</p>"},{"location":"data/data-replication/#business-motivations","title":"Business motivations","text":"<p>Among the top three priority of data integration and integrity solution, for 2019 and 2020, are (source IDC):</p> <ul> <li>Data intelligence (57%)</li> <li>Data replication (50%)</li> <li>Application data sync (51%)</li> </ul> <p>Two important derived use cases:</p> <ul> <li> <p>Business in Real Time:</p> <ul> <li>Detect and react to data events as they happen to drive the business, and propagate those changes for others for consumption</li> <li>Optimize decision to sub-second latency level, i.e. real time analytics  </li> </ul> </li> <li> <p>Always On Information</p> <ul> <li>High availability with Active-Standby and Active-Active data replication deployments</li> <li>Data synchronization for zero down time data migrations and upgrades</li> </ul> </li> </ul>"},{"location":"data/data-replication/#technology-overview","title":"Technology overview","text":""},{"location":"data/data-replication/#data-lake-technologies","title":"Data lake technologies","text":"<p>There are multiple alternatives to build a data lake solution. In the field, the following technologies are usually used: Hadoop, S3 bucket, Apache Spark, cloud object storage, and Apache Kafka. </p> <ul> <li> <p>Hadoop/HDFS (Hadoop File System) is designed to process large relatively static data sets. It provides a cost effective vehicle for storing massive amounts of data due to its commodity hardware underpinnings  that rely on built in fault tolerance based on redundancy. Hadoop is ideal for highly unstructured data and for data that  is changing at the file level.  With Hadoop, you don\u2019t change a record in a file.  Rather, you write a new file. A process reads through the files to discover the data that matters and to filter out unrelated data. It is massively  scalable processing. </p> </li> <li> <p>Apache Spark is a data processing engine tightly coupled with the Hadoop ecosystem. It is one of the most active  open source projects ever. Spark is mainly used as ETL engine capable of integrating data from various SQL and NoSQL data  sources and targets including RDBMS, NoSQL DB, Cloud Object Store, HDFS and other cluster file systems</p> </li> <li> <p>COS (Cloud Object Storage) in conjunction with a Query Engine like Apache SparkSQL or IBM Cloud SQLQuery Service  is a legit data lake solution, especially because, as a managed service, it provides unlimited capacity and very high  scalability and robustness against failures. IBM Cloud SQL Query is a serverless service to read data in Cloud Object  Storage, process it with full standard SQL syntax, and write the results to Cloud Object Storage or Db2. With SQL Query,  you analyze and process data where it is stored - there is no ETL, no databases, and no infrastructure to manage.  The service is highly available, offers a Multi-AZ deployment, and autoscales based on your workload.</p> </li> <li> <p>AWS S3.</p> </li> <li>Kafka is designed from the outset to easily cope with constantly changing data and events. It has built in  capabilities for data management such as log compaction that enable Kafka to emulate updates and deletes. The data  storage may be self described JSON document wrapped in Apache Avro binary format.  Kafka exploits the scalability and availability of inexpensive commodity hardware. Although Kafka supports persisting data in queues for weeks or even months, it's not yet a proved technology for long term storage, even if companies are already adopting it for event sourcing. Kafka provides a means of maintaining one and only one version of a \u201crecord\u201d much like in a keyed database. But an adjustable persistence time window lets you control how much data is retained. Important to note that RedPanda is a modern, fully compatible, to Kafka to support the same use cases, but also supporting webassembly to deploy code to broker.</li> </ul> <p>Data Replication solutions provide both bulk and continuous delivery of changing structured operational data to both  Hadoop and Kafka.  </p> <p>There are more and more organizations choosing to replicate their changing operational data to Kafka rather than directly  into Hadoop. Kafka\u2019s ability to self manage its storage, emulate the concept of a keyed record and provide self describing structural metadata combined with the benefits of scalability and open source interfaces makes it an ideal streaming and staging area for enterprise analytics.  </p> <p>If needed, data can be staged in Kafka for periodic delivery into Hadoop for a more controlled data lake, preventing the lake from becoming a swamp with millions of files.  </p> <p>Data stored in Kafka can be consumed by real time microservices and real time analytics engines.</p> <p>Kafka can also be used as a modern operational data store. It has the built in advantages of low cost scalability and  fault tolerance with the benefits of open interfaces and an ever growing list of data producers (feeding data into Kafka) and data consumers (pulling data from Kafka), all with self managed storage.</p> <p>Other use cases are related to auditing and historical query on what happened on specific records. Using event  sourcing, delivered out of the box with kafka, this will be easier to support. It can be used to propagate data changes to remote caches and invalidate them, to projection view in CQRS microservices, populate full text search in  ElasticSearch, Apache Solr, etc...</p>"},{"location":"data/data-replication/#change-data-capture-cdc","title":"Change data capture (CDC)","text":"<p>Another important part of the architecture is the change data capture component. The following diagram presents a  generic architecture for real time data replication, using transaction logs as source for data update, a change data  capture agent to load data and send then as event over the network to an \"Apply / Transform\" agent responsible to  persist to the target destination. </p> <p></p> <p>The data replication between databases by continuously propagate changes in real time instead of doing it by batch with  traditional ETL products, brings data availability and consistency cross systems. It can be used to feed analytics  system and data warehouses, for business intelligence activities.</p> <p>For example, IBM's InfoSphere Data Replication (IIDR)  captures and replicates data in one run or only replicate changes made to the data, and delivers those changes to other  environments and applications that need them in a trusted and guaranteed fashion, ensuring referential integrity and  synchronization between sources and targets. The architecture diagram below presents the  components involved in CDC  replication:</p> <p></p> <p>Product explanations can be obtained  here.</p> <p>We can combine Kafka and IIDR to support a flexible pub/sub architecture for data replication where databases are  replicated but event streams about those data can be processed in real time by any applications and microservices.  This is known as lambda-architecture.</p> <p>The combined architecture of a deployed solution looks like in the diagram below:</p> <p></p> <p>With the management console, the developer can define a data replication project that can include one to many subscriptions. Subscriptions define the source database and tables and target kafka cluster and topics.  </p> <p>The Kafka cluster can run on Kubernetes. </p> <p>The first time a subscription is running, a \"Refresh\" is performed: to allow the source and target to be exactly synchronized before the incremental, changes only get replicated down to the target. This means all the records in the source table will be written as Kafka events. </p> <p>When running a subscription the first time, Kafka topics are added: one to hold the records from the source table, and the second to keep track of which records have already been committed to the target. </p> <p>For more details about this solution see this product tour.</p>"},{"location":"data/data-replication/#debezium","title":"Debezium","text":"<p>Debezium is an open source distributed platform for change data capture. It retrieves change events from transaction logs from different databases and use Kafka as backbone, and Kafka connect. It uses the approach of replicating one table to one Kafka topic. </p> <p>It can be used for data synchronization between microservices using CDC at a service level and propagate changes via Kafka. The implementation of the CQRS pattern may be simplified with this capability. </p> <p></p>"},{"location":"data/data-replication/#why-adopting-kafka-for-data-replication","title":"Why adopting Kafka for data replication","text":"<p>Using Kafka as a integration layer brings the following advantages:</p> <ul> <li>Offload processing</li> <li>Data aggregation from multiple sources</li> <li>Deliver a common platform for staging to other data consumers</li> <li>Provide a storage system for duplicating data</li> <li>Buffer unprocessed messages</li> <li>Offers throughput and low end-to-end Latency\u00a0</li> <li>Offers real time processing and retrospective analysis</li> <li>Can correlate streaming feeds of disparate formats</li> <li>Flexibility of input source and output targets</li> <li>Built in stream processing API on real time feeds with Kafka streams</li> <li>Commit Log</li> <li>Fault tolerance, scalability, multi-tenancy, speed, light-weight, multiple landing-zones.</li> </ul>"},{"location":"data/data-replication/#kafka-connect","title":"Kafka connect","text":"<p>Kafka connect simplifies the integration between Kafka and other systems. It helps to standardize the integration via connectors and configuration files. It is a distributed, fault tolerant runtime able to easily scale horizontally. The set of connectors help developers to not re-implement consumers and producers for every type of data source.</p> <p>To get started please read this introduction from the product documentation. </p> <p>The Kafka connect workers are stateless and can run easily on Kubernetes or as standalone docker process. <code>Kafka Connect Source</code> is to get data to Kafka, and <code>Kafka Connect Sink</code> to get data out of Kafka.</p> <p></p> <p>A worker is a process. A connector is a re-usable piece of java code packaged as jars, and configuration. Both elements define a task. A connector can have multiple tasks. </p> <p>In distributed deployment, the connector supports scaling by adding new workers and performs rebalancing of worker tasks in case of worker failure. The configuration can be sent dynamically to the cluster via REST API.</p>"},{"location":"data/data-replication/#recommended-readings","title":"Recommended Readings","text":"<ul> <li>IBM InfoSphere Data Replication Product Tour</li> <li>Kafka connect hands-on learning from St\u00e9phane Maarek</li> <li>Integrating IBM CDC Replication Engine with kafka</li> <li>Very good article from Brian Storti on data replication and consistency</li> <li>PostgreSQL warm standby replication mechanism</li> <li>Change Data Capture in PostgreSQL</li> <li>eBook: PostgreSQL Replication - Hans-J\u00fcrgen Sch\u00f6nig - Second Edition</li> <li>Weighted quorum mechanism for cluster to select primary node</li> <li>Using Kafka Connect as a CDC solution</li> <li>Debezium tutorial</li> <li>Ensure data resilience - author: Neal Fishman</li> <li>RedPanda</li> </ul>"},{"location":"data/data-topology/","title":"Data Topology","text":"<p>Being able to understand and manage data becomes an essential foundation, for any organization that wants to be successful with analytics and AI,</p> <p>A data topology is an approach for classifying and managing real-world data scenarios. The data scenarios may cover any aspect of the business  from  operations, accounting, regulatory and compliance, reporting, to advanced analytics, etc.</p> <p>A properly designed data topology is sustainable over time, and highly resilient to future needs, new technologies, and the continuous changes  associated with data characteristics including volume, variety, velocity, veracity, and perception of the data\u2019s value.</p> <p>A properly designed data topology will provide the foundation for any enterprise to be successful with any type of analytics.</p>"},{"location":"data/data-topology/#core-elements-of-a-data-topology","title":"Core elements of a Data Topology","text":"<p>There are three Core elements of a data topology:</p> <ul> <li>Zone Map</li> <li>Data Flow</li> <li>Data Layer</li> </ul>"},{"location":"data/data-topology/#zones-and-zone-maps","title":"Zones and Zone Maps","text":"<p>Zones represent something which can be used to group/cluster data or with an instantiation/deployment of data.</p> <p>Zones are inherently abstract and conceptual in nature as a zone is not a deployed object.</p> <p>A Zone Map identifies and names each zone.</p> <p>Figure 1 shows the primitive zones that are used in a zone map.</p> <p></p> <p>It is only a leaf zone that is associated with the instantiation of data.</p> <p>Non-primitive zones include a virtual zone that may cluster multiple zones together and  reflect groups for data virtualization or data federation.</p>"},{"location":"data/data-topology/#data-flow","title":"Data Flow","text":"<p>The data flow shows the flow of Data and helps to illustrate the points of integration or interoperability between the leaf zones.  </p> <p>This can also help to detect circular data flows occurring across zones, to be flagged for  investigation as data integrity may potentially be compromised if not well managed (designed and governed).</p>"},{"location":"data/data-topology/#data-layer","title":"Data Layer","text":"<p>The data layer is reflective of where data may be persisted. In a modern enterprise we should consider the full landscape of possibilities which could include:</p> <ul> <li>Public cloud</li> <li>On premise private cloud</li> <li>Edge</li> <li>Device</li> </ul> <p>We can think of this as being a cloud, fog, edge  node topology, where the different nodes have different characteristics with compute power, storage capacity,  and may be constrained by network connectivity, here data will likely be highly distributed across an organization and certainly across an enterprise.</p> <p>For example, mixed deployment data layers include:</p> <ul> <li>Cloud - Public cloud, unconstrained</li> <li>Fog - Private cloud, constrained by available infrastructure</li> <li>Edge - Smart device, the most constrained by the limitations of compute capabilities, network bandwidth and availability</li> </ul>"},{"location":"data/data-topology/#characteristics-to-consider-when-designing-a-data-topology","title":"Characteristics to Consider When Designing a Data Topology","text":"<p>Designing a data topology is an iterative process</p> <ol> <li>Group users (or end-points) into communities of interest to determine shared needs.</li> <li>Classify and cluster data into zones with shared qualitative characteristics (use, purpose, need) unconstrained by particular technologies or quantitative characteristics.</li> <li>Map and align communities of interest to data zones.</li> <li>Add constraints to further develop the zone map and align with functional and non-functional requirements and capabilities.</li> <li>Work backwards in the data pipeline to identify areas of synergy and re-use.</li> <li>Define the flow of data (movement, dependencies) across and within zones in support of the defined constraints.</li> </ol>"},{"location":"data/data-topology/#keeping-an-organic-data-topology","title":"Keeping an organic data topology","text":"<p>A data topology is intended to be organic, alive, in nature. Although a static topology is a choice, an organic data topology promotes the development  of disciplines for addressing an enterprise with changing needs and priorities over time.</p> <ul> <li>Zones can be regarded as being ephemeral or temporal.</li> <li>New zones can be added as required.</li> <li>A new zone can be added as a diagrammatic placeholder without instantiation.</li> <li>Old zones can be removed or deprecated.</li> <li>Covers zones that have been expired, sunsetted, retired, archived.</li> <li> <p>Leaf zones are intended to have independent aging policies, For example:</p> </li> <li> <p>data can be removed after a given period of time (such as removing data from a raw zone after 7 business days)</p> </li> <li>A zone may be designated as being immutable in that data cannot be updated or removed; but that data can be added</li> </ul>"},{"location":"data/data-topology/#leaf-zone-guides","title":"Leaf zone guides","text":"<p>A leaf zone is the zone that reflects the instantiation of data.</p> <p>In that regard, the leaf zone is the least abstract or conceptual of all zone types.</p> <p>For simplicity purposes, general recommendations to consider when establishing a leaf zone are:</p> <ul> <li>to limit the database technology to a single type.</li> <li>the location (virtual or physical) should be singular.</li> </ul> <p>A conceptual non-leaf zone can be added, that is a grouping of multi-leaf zones together in order to address multiple technologies or multiple locations.</p>"},{"location":"data/data-topology/#simplifying-security-with-leaf-zones","title":"Simplifying security with leaf zones","text":"<p>A leaf zone can also be used to help simplify certain complex security profiles.</p> <p>There can be situations where a shared data resource requires numerous security policies to address each user group type. For example, some users may have read/write access while other users may only have access to data with obfuscated values.</p> <p>In the case of an obfuscated value, a user gets access to a certain metatag or field, but the value they receive is actually a substituted value that hides the real value.</p> <p>As a means to help address complex security needs on a shared data resource, a copy of the data store can be placed in a separate leaf zone. The copy is a form of controlled redundancy.</p> <p>By having two or more independent leaf zones with the same information, simplified security profiles can be deployed to each leaf zone with the intent to help mitigate the potential for a security breach.</p>"},{"location":"data/data-topology/#enterprises-and-organizations","title":"Enterprises and Organizations","text":"<p>When designing the data topology, it is useful to consider the characteristics and differences between organizations within an enterprise and the enterprise itself. </p> <p>An organization is often inwardly looking \u2013 even when taking into account customers; while an enterprise is outward looking recognizing the place of the organization in a complete ecosystem.</p> <p>When viewing the enterprise as an ecosystem, it is easier to understand the place and purpose of data. Readily being able to delineate between what data is created and consumed by an organization and what data is created and consumed by the enterprise.</p> <p>Within an organization, all data is often assumed to be governable through some type of data governance program.</p> <p>Whereas, within the auspices of the enterprise (the ecosystem), not all data can automatically be assumed to be governable by a data governance program.</p> <p>This draws into question the horizontal bar that many organizations will create in a system diagram that is labeled as governance and includes third-party data.</p> <p>For example, if the system includes data ingestion from a company providing social media data, you can\u2019t complain on data quality from a particular tweet if they spelt your company name wrong. You have to have a data quality assessment process in place correcting spelling mistakes or simply remove those data points.</p>"},{"location":"data/olap/","title":"Online analytical processing","text":""},{"location":"data/olap/#concepts","title":"Concepts","text":"<ul> <li>performing multidimensional analysis at high speeds on large volumes of data from a data warehouse, data mart, or some other unified, centralized data store</li> <li>Need to support multiple dimensions: sales figures might have several dimensions related to location (region, country, state/province, store), time (year, month, week, day), product (clothing, men/women/children, brand, type), and more.</li> <li>OLAP extracts data from multiple relational data sets and reorganizes it into a multidimensional format that enables very fast processing and very insightful analysis. </li> <li>OLAP cube is an array-based multidimensional database. The top layer of the cube might organize sales by region; additional layers could be country, state/province, city and even specific store.</li> <li>The drill-down operation converts less-detailed data into more-detailed data through one of two methods\u2014moving down in the concept hierarchy or adding a new dimension to the cube.</li> <li>Roll up aggregates data on an OLAP cube by moving up in the concept hierarchy or by reducing the number of dimensions. </li> <li>The slice operation creates a sub-cube by selecting a single dimension from the main OLAP cube (time dimension). </li> <li>The dice operation isolates a sub-cube by selecting several dimensions within the main OLAP cube.</li> <li>The pivot function rotates the current cube view to display a new representation of the data\u2014enabling dynamic multidimensional views of data.</li> <li>OLAP tools are designed for multidimensional analysis of data in a data warehouse, which contains both transactional and historical data.</li> <li>OLTP is designed to support transaction-oriented applications by processing recent transactions as quickly and accurately as possible.</li> </ul>"},{"location":"data/olap/#problems","title":"Problems","text":"<ul> <li>SQL and relational database reporting tools can certainly query, report on, and analyze multidimensional data stored in tables, but performance slows down as the data volumes increase.</li> </ul>"},{"location":"data/olap/#resources","title":"Resources","text":"<ul> <li>IBM article on OLAP</li> </ul>"},{"location":"fun/puzzles/","title":"Coding interview and fun problems to solve","text":"<p>Source of common software engineering problems to study and play with:</p> <ul> <li>geeks for geeks</li> <li>leet code</li> </ul>"},{"location":"fun/puzzles/#practices","title":"Practices","text":"<ol> <li>Ask questions about the problem to be sure to get a good understanding</li> <li>Continuously explain the reasoning</li> <li>Present the code, assess complexity</li> <li>Always test the solution</li> <li>Try optimizing after</li> </ol>"},{"location":"fun/puzzles/#big-o-notation","title":"Big O notation","text":"<p>O is to specify the \"Order of\" for an algorithm complexity according to the number of elements</p> <ul> <li>Inserting element in an unordered array is constant time. It is not linked to the number of elements in the array. Time is = K a basic constant linked to compilor, microprocessor speed...</li> <li>Linear search is proportional to N. The search time T = K * N/2 on average</li> <li>Binary search is log(N): T = log base 2(N) but as any log  is related to any other by a constant (3.322 to go from base 2 to base 10), we can say T = log(N). Doubling the N will add only one step of processing. Time follows a log.</li> <li>The quasilinear time of O(n.log n) is worse than O(n) but better than O(N^2)</li> </ul> <p>O(N) is the same as T = K .N   </p> <p>Big O - good article from HappyCoder.</p>"},{"location":"fun/puzzles/#review-sorting","title":"Review Sorting","text":"<p>Java Arrays class offers different sort methods.</p> <p>To sort any custom object, we need to implement the interface java.lang.Comparable. Or gives a comparator to the sort method.</p> <pre><code>public static final Comparator&lt;Customer&gt; NAME_COMPARATOR = Comparator\n    .comparing(Customer::getLastName)\n    .thenComparing(Customer::getFirstName);\n</code></pre> <p>Sorting algorithm review Sorting in Java from Happy coder.</p> <ul> <li>For many sorting algorithms, the additional memory requirement is independent of the number of elements to be sorted.</li> <li>In stable sorting methods, the relative sequence of elements that have the same sort key is maintained.</li> <li>A recursive sorting algorithm requires additional memory on the stack</li> <li>Quicksort, merge sort, heap sort are in O(n log n) where the insertion sort, selection sort and bubble sort are in O(n^2) on average.</li> <li> <p>Counting sort is a particular sorting algorithm in O(n + k): k = number of possible values. </p> </li> <li> <p>Quicksort works according to the \"divide and conquer\" principle. The data set is divided into small and large elements: small elements move to the left, large elements to the right. Each of the created partitions is then recursively partitioned again until a partition contains only one element and is therefore considered sorted.</p> </li> <li> <p>Heap sort: utilizes the heap data structure to perform comparison-based sorting. Heap sort is an in-place algorithm. Heap can be represented by an array and then if the parent node is at index i, then the left child node is at 2i+1 and the right child is at 2i+2. Using a maxheap. We will swap the first element, i.e., the largest element with the last element of the heap, and then reduce the size of the heap by 1. Once we have successfully done that, we will call the heapify method for the root of the tree. We will then repeat this step until the size of the heap is greater than 1.  The time complexity of heap sort is O(nlog(n)). See code HeapSort class</p> </li> <li> <p>Merge two sorted lists, see example using iteration or recursion.</p> </li> </ul>"},{"location":"fun/puzzles/#basic-problems-from-leetcode","title":"Basic problems from leetcode","text":""},{"location":"fun/puzzles/#roman-to-integer","title":"roman to integer","text":"<ul> <li>Transform a string representing a roman number to its integer value</li> </ul>"},{"location":"fun/puzzles/#palindrome","title":"Palindrome","text":"<ul> <li>Verify a string with digits in it,  to be a palindrome? Given the head of a singly linked list, return true if it is a palindrome.</li> </ul>"},{"location":"fun/puzzles/#string-with-only-char","title":"String with only char","text":"<ul> <li>Search if a string includes only alphabet using lambda function.</li> </ul> Solution <pre><code>public static boolean isStringOnlyAlphabet(String str)\n    {\n        return (\n            (str != null) &amp;&amp; (!str.equals(\"\"))\n            &amp;&amp; (str.chars().allMatch(Character::isLetter)));\n    }\n</code></pre> <ul> <li>Generic function to remove element in a list that matches a predicate. </li> </ul> Solution <pre><code>    public static &lt;T&gt; List&lt;T&gt;\n    removeUsingIterator(List&lt;T&gt; l, Predicate&lt;T&gt; p) {\n        Iterator&lt;T&gt; itr = l.iterator();\n        while (itr.hasNext()) {\n            T t = itr.next();\n            if (!p.test(t)) {\n                itr.remove();\n            }\n        }\n        return l;\n    } \n\n// Creating a Predicate condition checking for null\nPredicate&lt;String&gt; isNull = item -&gt; Objects.nonNull(item);\n</code></pre>"},{"location":"fun/puzzles/#fibonacci-sequence","title":"Fibonacci sequence","text":"<p>f(n) = f(n-2) + f(n-1)</p> <p>Can be used in different problem, like finding the combinaison of steps to climb a staircase of N steps.</p> <pre><code>def staircase(n):\n    if n &lt;= 1:\n        return 1\n    return staircase(n - 1) + staircase(n - 2)\n</code></pre>"},{"location":"fun/puzzles/#array-play","title":"Array play","text":"<p>Sources of information:</p> <ul> <li>IQ opengenus - array problems</li> <li>Dynamic array means we double the size of the array when the size and the number of elements in the arrays are equals. So resize needs to create a temp array and then copy existing content to the temp array (with double size) and then return the temp array.</li> </ul> <p>To represent a dynamic array that can extend at less memory footprint, is to use the Hash Arrays Tree, which is a 2 dimension arrays with 2^x elements in each dimensions.</p>"},{"location":"fun/puzzles/#binary-search","title":"Binary search","text":"<p>Take middle index, go left if node.value &lt; a[idx] or right otherwise. Use recursion and test lowerIdx &gt; highIdx as exit with not found. Return the idx of the element in the array.</p> <pre><code>        int[] a = {1,2,3,4,5,6,7,8,9};\n        BinarySearch search = new BinarySearch();\n        Assertions.assertEquals(0,search.find(a,1));\n        Assertions.assertEquals(4,search.find(a,5));\n</code></pre> <p>See code BinarySearch.java</p>"},{"location":"fun/puzzles/#find-the-least-frequent-element-presents-in-an-array","title":"Find the least frequent element presents in an array","text":"<p>Use HashMap to keep the count, and go over the element one at a time. Ex in python:</p> <pre><code>def findLeastFreqElementOptimized(c):\n    d = {}\n    for i in range(len(c)):\n        if (c[i] in d.keys()):\n            d[c[i]] += 1\n        else:\n            d[c[i]] = 1\n    leastElementCount = min(d.values())\n    for i in d:\n        if d[i] == leastElementCount:\n            leastElement = i\n            break\n    return leastElement,leastElementCount\n</code></pre>"},{"location":"fun/puzzles/#snapshot-array","title":"Snapshot array","text":"<p>Implement an optimized approach to take a snapshot of the array of int and returns the snap_id. Contract:</p> <pre><code>SnapshotArray snapshotArr = new SnapshotArray(3); // set the length to be 3\nsnapshotArr.set(0,5);  // Set array[0] = 5\nsnapshotArr.snap();  // Take a snapshot, return snap_id = 0\nsnapshotArr.set(0,6);\nsnapshotArr.get(0,0);  // Get the value of array[0] with snap_id = 0, return 5\n</code></pre> Solution <p>Code is in Java Study fun-interview - snapshot-array.  Use a list of list of value, idx (a node). The first list is for snapshot, the second is a list of nodes with value not equals to zero. When the snap shot is not done yet, use the source value. Else get the list for a given snapshot and search for the given idx, return the matching value or zero.</p>"},{"location":"fun/puzzles/#reverse-a-linkedlist","title":"Reverse a linkedList","text":"<p>Need to keep previous, current, and currentNext pointers.</p> <p>InverseLinkedList</p> Solution <pre><code>while (current != null) {\n            Node nextOfCurrent = current.next;\n            current.next = previous;\n            previous = current;\n            current = nextOfCurrent;\n        }\n        Node reversedList = previous;\n</code></pre>"},{"location":"fun/puzzles/#find-the-tuple-from-two-arrays-that-the-sum-of-their-number-are-closed-to-a-target-number","title":"Find the tuple from two arrays that the sum of their number are closed to a target number","text":"<p>A = [-1, 3, 8,  2, 9,  5] B = [ 4, 1, 2, 10, 5, 20] Target 24 . response -&gt; (5,20) or (3,20) A and B have same size. Not ordered.</p> <ul> <li>Brute force solution: compute each pair based on element of (A,B)- Which is a O(N^2), sort them by their sum and find where the target is in the sorted collection, take the exact sum or the left and right elements as the closest to the target.</li> <li> <p>Think to a simpler problem by searching what is the expected number to get the target by searching in A for each value of (target - b). The following solution is in O(N)</p> <p><pre><code>HashSet aSet = new HashSet(A);\nfor (int b : B) {\n    int n = target - b\n    if (aSet.contains(n) {\n        return (b,n)\n    }\n}\n</code></pre> if we can not find then change the target by increase and then decreasing it and retry. In this case the algo is in O(x.n).</p> </li> <li> <p>Think of the problem with simpler samples. </p> </li> <li>Try to visualization in a matrix or a tree</li> </ul> <p>Final solution is to sort the two arrays and then walk the matrix from the one of the highest number. The sort could is in O(n.log(n)) and then go over the matrix will be O(n) </p> <pre><code>```java\nsort(A), sort(B)\n\nfor (i = A.size() - 1; i &gt; 0; i--) {\n    if ((A[i] + B[i]) &lt; target) return;\n}\n```\n</code></pre>"},{"location":"fun/puzzles/#binary-tree","title":"Binary tree","text":"<p>A tree with node having value, left and right branches. It can keep order at insertion time.</p> <p>Binary tree helps to have efficient search in O(log(N)) as the binary search does, and have efficient insert and delete operations as a LinkedList does. Adding an element in a sorted array is costly as we have to move, on average, N/2 elements.</p> <ul> <li> <p>Tree traversal in Python to visit each node of a binary tree exactly once. The approach is:</p> <ul> <li>use a recursive method to go from the left branch of the node down to the leaf. then go up to the right branch. Keep the list of visited nodes as part of the recurring function.</li> <li>or append to the list of visited nodes only at the leaf level.</li> </ul> <pre><code>    def PostOrderTraversal(self, nodes):\n        if (self.getLeft() is not None):\n            self.getLeft().PostOrderTraversal(nodes)\n        if (self.getRight() is not None):\n            self.getRight().PostOrderTraversal(nodes)\n        nodes.append(self.getValue())\n        return nodes\n</code></pre> </li> <li> <p>Same approach, different implementation, of the binary tree traversal in python</p> </li> <li>See a very complete example of BinaryTree class in java + test cases.</li> </ul> <p>Special binary tree is the heap which includes as root the highest priority node.  In a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C.</p>"},{"location":"fun/puzzles/#traversing-graph-or-tree","title":"Traversing Graph or tree","text":"<p>There are three different orders for traversing a tree using DFS:</p> <ul> <li> <p>In preorder traversal, we traverse the root first, then the left and right subtrees. It uses recursion. If we use iteration we need to use a stack and use the following algorithm</p> <ul> <li>Push root in our stack</li> <li> <p>While stack is not empty</p> <ul> <li>Pop current node</li> <li>Visit current node</li> <li>Push right child, then left child to stack</li> </ul> </li> </ul> </li> <li> <p>For Inorder Traversal we traverse the left subtree first, then the root, then finally the right subtree. This keeps the order of the elements in the tree.</p> </li> <li>In postorder traversal, we traverse the left and right subtree before we traverse the root.</li> </ul> <p>See Java code Node.java</p> <p>The main difference between graphs and trees is that graphs may contain cycles.</p> <p>See Java code Graph.java</p> <ul> <li>Breadth first search: In python Graph.py explores all of the neighbor nodes at the present depth prior to moving on to the nodes at the next depth level.</li> <li> <p>Depth first search In Python: go over the branches of the graph from a root and visit all vertex by going farther from the root as possible. The return parameter is a list of vertex visited, in their order of visit</p> <ul> <li>rule 1: add non visited neighbors in a LIFO stack.</li> <li>rule 2: when on a vertex with no more neighbor visited then pop one vertex from the stack</li> <li>rule 3: when there is no more vertex to traverse, the stack is empty</li> </ul> </li> </ul>"},{"location":"fun/puzzles/#example-of-problems-involving-traversing-a-tree","title":"Example of problems involving traversing a tree","text":"<ul> <li> <p>Sum of all the numbers that are formed from root to leaf paths: Path: 6-&gt;3-&gt;2 : 632  then Path 6 -&gt; 3 -&gt;5 -&gt;2: 6352 + 632 total is 6984. The idea is to do a preorder traversal of the tree. In the preorder traversal, keep track of the value calculated till the current node, let this value be val. For every node, we update the val as val*10 plus node\u2019s data</p> </li> <li> <p>Calculate depth of a full Binary tree from Preorder: The tree is represented by a string like \"nlnnlll\" for node n and leaf l. </p> </li> <li>Get size with recursion: at each node the size of the node is the current level + the left and right size.</li> </ul> Solution <pre><code>public int getSize() {\n    return getSizeRecursive(root);\n}\n\nprivate int getSizeRecursive(IntNode current) {\n    return current == null ? 0 : getSizeRecursive(current.left) + 1 + getSizeRecursive(current.right);\n}\n</code></pre>"},{"location":"fun/puzzles/#binary-search-tree","title":"Binary search tree","text":"<p>The BST has an important property: every node\u2019s value is strictly greater than the value of its left child and strictly lower than the value of its right child. It does not allow duplicate values. Binary Search Tree can be either balanced and unbalanced.</p> <p>Suppose n to be the number of nodes in a BST. The worst case of the insert and remove operations is O(n). But, in a balanced Binary Search Tree, for instance, in AVL or Red-Black Tree, the time complexity of such operations is O(log(n)).</p> <p>The other major fact is that building BST of n nodes takes O(n * log(n)) time. We have to insert a node n times, and each insertion costs O(log(n)). The big advantage of a Binary Search Tree is that we can get traverse the tree and get all our values in sorted order in O(n) time</p>"},{"location":"fun/puzzles/#minimum-spanning-tree","title":"Minimum spanning tree","text":"<ul> <li> <p>Minimum spanning tree: In Python A path in the graph going to all nodes by using the less costly edges. The return structure is a list of vertex in the order of navigation from the root.</p> <ul> <li>find neighbour of current vertex connected by the lowest edge weight still in the queue</li> </ul> </li> </ul> <p>A graph is a set of vertex and edge with a weight.</p>"},{"location":"fun/puzzles/#maxium-path-sum-of-a-binary-tree","title":"Maxium path sum of a binary tree.","text":"<p>Given a binary tree, find the maximum path sum. The path may start and end at any node in the tree. Examples: </p> <pre><code>Input: Root of below tree\n\n\n       1\n      / \\\n     2   3\nOutput: 6\n</code></pre> <p>Or</p> <pre><code>Input:\n     10\n    /  \\\n   2   -25\n  / \\  /  \\\n 20 1  3  4\nOutput: 32\n</code></pre> <p>Time Complexity: O(n) where n is number of nodes in Binary Tree.</p> Solution <p>Use binary tree of node with value, left, right sub nodes. Build a recursive approach to traverse the tree deep first.  At each node compute the  max between the two children with current value and the current value, this is the value at that node for parent. As part of the recursion as we want to keep the max path sum, we need to keep this value outside of the tree traversal. </p> <p><pre><code>   public int maxPathSum(Node currentRootNode, Result result) {\n\n    if (currentRootNode == null) return 0;\n\n    int leftResult = maxPathSum(currentRootNode.left,result);\n    int rightResult = maxPathSum(currentRootNode.right,result);\n    // max path sum of this node for a parent of this node\n    int max_single = Math.max(Math.max(leftResult, rightResult) + currentRootNode.value, currentRootNode.value);\n\n    int max_top = Math.max(max_single, leftResult + rightResult + currentRootNode.value);\n    // Store the Maximum Result.\n    result.value = Math.max(result.value, max_top);\n\n    return max_single;\n}\n</code></pre> See code in Java Study fun-interview - maxPathSum. </p>"},{"location":"fun/puzzles/#find-the-maximum-sum-from-leaf-to-root-path-in-a-binary-tree","title":"Find the maximum sum from leaf to root path in a Binary Tree","text":"<p>Use traversal of the tree left and right at each node. At each node select the max sum to reach  this node from its leaves so far. it is the max between right or left branches. Here is the recurring implementation. (See TestLeafToRootMaxSum class.</p> <pre><code>public int maxSumLeafToRoot(Node currentNode, List&lt;Node&gt; path) {\n        if (currentNode != null) {\n            List&lt;Node&gt; leftPath = new ArrayList&lt;Node&gt;(); // use to accumulate the path\n            int leftSum = maxSumLeafToRoot(currentNode.left, leftPath) + currentNode.value;\n            List&lt;Node&gt; rightPath = new ArrayList&lt;Node&gt;();\n            int rightSum = maxSumLeafToRoot(currentNode.right, rightPath) + currentNode.value;\n            if (leftSum &gt; rightSum) {\n                path= leftPath;\n                path.add(currentNode);\n                return leftSum;\n            } else {\n                path = rightPath;\n                path.add(currentNode);\n                return rightSum;\n            }\n\n        }\n        // leaf\n        return 0;\n    }\n</code></pre>"},{"location":"fun/puzzles/#find-your-appartment-location-in-a-street","title":"Find your appartment location in a street","text":"<p>Looks at each block and try to minimize the distance between your needed requirements in term of school, gym, store. The distance is zero if your block has the needed requirement, or the minimum distance to the closed one from your block.</p> <p>The cost is to find the mininum of the maximum distance between a block and the needed requirement.</p> <p>See implementation in SearchBestBlock</p>"},{"location":"fun/puzzles/#inverting-a-binary-tree","title":"Inverting a binary tree","text":"<p>Given a binary tree move left and right branches at every node level. It is simple with recursion:</p> <ul> <li>Call invert for left-subtree.</li> <li>Call invert for right-subtree.</li> <li>Swap left and right subtrees.</li> </ul> <pre><code>    Node invert(Node node) \n    { \n        if (node == null) \n            return node; \n\n        /* recursive calls */\n        Node left = invert(node.left); \n        Node right = invert(node.right); \n\n        /* swap the left and right pointers */\n        node.left = right; \n        node.right = left; \n\n        return node; \n    }   \n</code></pre>"},{"location":"fun/puzzles/#heap","title":"Heap","text":"<p>Heap is a Complete Binary Tree. A node is at level k of the tree if the distance between this node and the root node is k.  The level of the root is 0. The maximum possible number of nodes at level k is 2^{k}. At each level of a Complete Binary Tree, it contains the maximum number of nodes. But, except possibly the last layer, which also must be filled from left to right.</p> <p>Heap is not an ordered data structure, as it is balanced. Heap allows duplicates.</p> <p>It is a tree that satisfies the heap property: </p> <ul> <li>max heap for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C </li> <li>min heap P key is lower than the key of C</li> </ul> <p>In an array, where the heap nodes are stored, the children of a node at index i are nodes at indices 2 * i + 1 and 2 * i + 2.</p> <p>we can build a Heap in O(n * log(n)) time. But, there exists an algorithm, which allows building a Heap in O(n) time. The insert and remove operations cost  O(log(n)).</p> <p>However, the Heap is an unordered data structure. The only possible way to get all its elements in sorted order is to remove the root of the tree n times. This algorithm is Heap Sort and takes O(n * log(n)) time.</p> <p>Java implements these structures with the PriorityQueue and the TreeMap. </p> <p>MaxHeap code</p> <p>Good article</p>"},{"location":"fun/bg/","title":"Boardgames consolidate notes","text":"<ul> <li>Summoner Wars</li> <li>Arkham Horror</li> </ul>"},{"location":"fun/bg/arkhamhorror/","title":"Arkham Horror LCG","text":""},{"location":"fun/bg/arkhamhorror/#rules-we-may-forget","title":"Rules we may forget","text":"<ul> <li>Skill test modified is always &gt;=0. Token + base + skill icon + modifier on assets &gt;=0.  Then compare to the skill level to test.</li> <li>After an enemy attacks, it is exhausted, so not ready, so we may be able to do action without attack of opportunity. Attention an enemy does not exhaust while making an attack of opportunity. We can still make damage to it with a reaction card (Like Delilah O'Rourke).</li> <li>We can not discover more clues than were already available to be discovered from a location, regardless of the number of successes or card effects.</li> <li>All dooms in play are removed when agenda progresses, even the ones on player or enemy cards</li> <li>Prey only cares when an enemy needs a tiebreaker for who it moves to or engages to.  Apply when moving but also when ready.</li> <li>As long as an investigator isn't resolving a treachery with the \"peril\" keyword, each other investigator on the same location can commit one (only one per skill test) of their cards to that investigator's skill tests.</li> <li>Revealed tokens when we cancel or ignore a token, it doesn't count as being revealed. But partially ignored token still count as being revealed. A \"skull\" during a fight using Baseball bat, breaks the bat, even if we added \"Defiance\" to the skill test.</li> <li>\"Limit 1 per investigator means, \"in play\". Different than 1 limit per deck.</li> <li>Arrow action on card cannot be combined with other action on card (investigate + a flash light).</li> <li>Heal is only on personal investigator, except when explicitly saying at the location, or something else.</li> <li>Trauma can be healed, when game starts.</li> <li>Revelation is only triggered on cards when we draw them, not when we look at or discard them.</li> <li>While seaching, search for any card that meets the criteria, not just the first one.</li> <li>The per invvestigator symbol is when the game starts, so if one is eliminated, the quota is still as at the beginning.</li> <li>Automatically evade (like in cunning distraction) is a not an evade action, so could not be a successful action. </li> </ul>"},{"location":"fun/bg/arkhamhorror/#strategies","title":"Strategies","text":"<ul> <li>Apply the 2 point rules: to succeed a test in standard mode at least be sure to be two level higher than the test. +3 when bag becomes nasty (will give an odd around 85% in most test): which also means when the campaign progresses we need to be able to be at 9, 10 skill value.</li> <li>Use mulligan to get more useful cards like damage soak cards, character' specific card and card dealing with enemies as soon as possible</li> <li>Advance the act at the beginning of the turn or when having actions left, as enemy can spawn at the investigator's location</li> <li>Cards that help draw from encounter deck should be played at the beginning of the turn</li> <li>When taking a draw action, do it as first action, to deal with potential weakness or other bad things we could draw</li> <li>We do not have to pass every treachery. Better to keep good cards for next turns and loose health or horror at the beginning of the game.</li> <li>We do not have to kill all enemies. Evade no hunter enemy. Evade, investigate then move to other location is a good turn. Perfectly valid in solo play.</li> <li>Keep munition for 3 damages attack for big guy. Do not waste on 1 life enemies.</li> <li> <p>We do not have to get every experience points of the scenario. Consider how many actions to go to the location and resolve the solution versus the scenario time line.</p> <ul> <li>actions are our most important resources</li> <li>take advantage of action economy: free move, picking clue without using action, free damage\u2026</li> <li>track our actions and the phase we are in.</li> </ul> </li> <li> <p>We should not rely on expected cards; use what you have at the time. We do not need to play all our assets.</p> </li> <li> <p>Do not think we need to be ready before moving in the scenario act.</p> </li> <li>Assess if we need to go to a location or wait later when more ready.</li> <li>Card drawing and resource gathering actions should be less a priority when there is so much better things to do.</li> <li>Playing the campaign multiple times to adapt the deck with better cards and prepare the play. What enemy to fight...</li> <li>Assess before moving as a last action: may be better to stay with others; or better set in current location to fight an encounter.</li> <li>Know when to evade. Remember monster without hunter does not move. </li> <li>Try to get a 2 damages at least per attack</li> </ul>"},{"location":"fun/bg/arkhamhorror/#for-solo-play","title":"For solo play","text":"<ul> <li>Avoid situational card, like barricade, as the situation may never occur with a good timing</li> <li>Try to get more actions</li> <li>Avoid loosing test to do not loose action</li> <li>Remove card like deduction</li> <li>Evasion matters</li> <li>Characters good for solo: Roland Banks. </li> <li>Cards that help us to get clues: Archaic Glyphs: Guiding Stones and both variants of Deduction</li> </ul>"},{"location":"fun/bg/arkhamhorror/#action-economy","title":"Action Economy","text":"<p>When we can complete 2 or more tasks using 1 action or less actions than it would normally take.</p> <p>Cards:</p> <ul> <li>emergency cache to get resources</li> <li> <ul> <li>1 damage weapon</li> </ul> </li> <li>deduction: 1 more clue in case of success</li> <li>short cut: move without action</li> <li>astral travel </li> </ul>"},{"location":"fun/bg/arkhamhorror/#resource-economy","title":"Resource economy","text":""},{"location":"fun/bg/arkhamhorror/#deck-building","title":"Deck building","text":"<p>For standalone mode, we can build our deck with starting exp, which allows us to include pipped cards within the deckbuilding instructions. Add one weakness per 10 XP used. For campaign starts with 0 XP cards.</p> <p>Always try to get answers to the following questions:</p> <ul> <li>How do we plan to deal with 4 shroud locations and 4 combat enemies?</li> <li>What goes in the Hands/Ally/Arcane/Accessory slots?</li> <li>How will I pay for those expensives cards?</li> <li>What is my upgrade strategy?</li> </ul> <p>Decide on our play style for that deck. If playing solo or with one other investigator, we'll probably want to build more conservatively, adding more cards to the deck that help to boost the lower skill scores. At 3 or 4 players boost on the character's strengths.</p> <p>Some investigators have similar scores in fight and evade. Pick one as the primary. Focusing on both equally will lead to an unfocused deck and inconsistent games.</p> <p>Try to get 12 to 17 assets per deck, 4 to 6 skill cards, specially while running a low resource investigator, then the remaining as events.  Consider tarot cards.</p> <p>Pay attention to the costs of the cards we're adding to our deck. Add cards to generate resources. Common to add 2 emergency cash. Pay attention to the Skill bonus icons when choosing Asset and Event cards! These can be very useful when forming a \"Plan B\" for the deck.</p> <p>Use neutral cards to boost some characteristics.</p> <p>Any player cards that have an encounter set logo in the upper right should never be added to a player deck until we are specifically instructed to (most commonly this will be at the end of a scenario).</p> <p>When playing 2 handed investigators: Combination: guardian - mystic  or guardian - survivor or seeker - rogue or seeker - survivor or mystic - rogue.</p> <p>Cards</p> <ul> <li>Aquinnah: level 1 is very expensive for survivors. So could be used by other classes with access to resources. While level 3 is only accessible to survivors, with a cost 4 and better skill: it avoids one physical damage per round, specially good on aoo.  It can damage aloof, and enemy engaged with another investigator. Try to combo with cards that make her ability trigger multiple times per round. It Should be useful to peter, william yorik to bring her back when her sanity reached 4. Use charisma and peter silvestre.  Inspiring presence helps to reuse her and remove horror from her.</li> </ul>"},{"location":"fun/bg/arkhamhorror/#dunwich-campaign","title":"Dunwich campaign","text":"<ul> <li>Necromencian book is used in miskatonic museum scenario even if available as base card in dunwich box</li> <li>Modify the content of the bag if the scenario tells us to do it.</li> </ul>"},{"location":"fun/bg/arkhamhorror/#the-circle-undone-campaign","title":"The circle undone campaign","text":"<p>Diane needs to get cancelled cards asap to increase her mental. Need spell attack, evade and investigate.</p>"},{"location":"fun/bg/arkhamhorror/#investigators","title":"INVESTIGATORs","text":""},{"location":"fun/bg/arkhamhorror/#diane-stanley-pairing-with-preston-in-circle-undone","title":"Diane Stanley - Pairing with Preston in Circle Undone","text":"<p>Her deck on ArkhamDB</p> <p>Goal: lean more on mystic side than guardian to avoid too much cards consumption to bump her 3 attack base. Very interesting study of diane stanley</p>"},{"location":"fun/bg/arkhamhorror/#preston-fairmont","title":"Preston Fairmont","text":"<p>Deck for TCU 2<sup>nd</sup> campaign with Nathaniel Cho</p>"},{"location":"fun/bg/arkhamhorror/#jim-culver","title":"Jim Culver","text":"<p>Recommended deck for Jim Culver, with the goal to recycle grotesque statue</p> Category Roland Bank - Guardian Daisy Walker - Seeker Skids o toole - Rogue Agnes Baker - Mystic Wendy Adams Survivor Zoey Rex the reporter Jenny Jim Culver - Mystic Ashcan Peter- Survivor Assets Roland\u2019s .38 Special, .45 Automatic, Physical Training, Beat Cop, First Aid, Machete,Guard Dog, Magnifying Glass, Old Book of Lore, Research Librarian, Dr. Milan Christopher, Hyperawareness, Medical Texts, 2 copies of Knife, 2 copies of Flashlight Daisy\u2019s Tote Bag, Magnifying Glass, Old Book of Lore, Research Librarian, Dr. Milan Christopher, Hyperawareness, Medical Texts, Forbidden Knowledge, Holy Rosary, Shrivelling, Scrying, Arcane Studies, Arcane Initiate, 2 copies of Knife, 2 copies of Flashlight .45 Automatic, Physical Training, Beat Cop,First Aid, Machete, Guard Dog, Switchblade, Burglary, Pickpocketing, .41 Derringer, Leo De Luca, Hard Knocks, 2 copies of Knife, 2 copies of Flashlight Heirloom of Hyperborea, Forbidden Knowledge, Holy Rosary, Shrivelling, Scrying, Arcane Studies, Arcane Initiate, Leather Coat, Scavenging, Baseball Bat, Rabbit\u2019s Foot, Stray Cat, Dig Deep, 2 copies of Knife, 2 copies of Flashlight Wendy\u2019s Amulet, Switchblade, Burglary, Pickpocketing, .41 Derringer, Leo De Luca, Hard Knocks, Leather Coat, Scavenging, Baseball Bat, Rabbit\u2019s Foot, Stray Cat, Dig Deep, 2 Knifes, 2  Flashlight Zoey\u2019s Cross, .45 Automatic, Physical Training, Beat Cop, First Aid, Machete, Guard Dog, Holy Rosary, Rite of Seeking, Clarity of Mind, Blackjack, 2 copies of Knife, 2 copies of Flashlight, Kukri Magnifying Glass, Old Book of Lore, Research Librarian, Dr. Milan Christopher, Hyperawareness, Medical Texts, Burglary, Laboratory Assistant, Pickpocketing, 2 copies of Knife, 2 copies of Flashlight Jenny\u2019s Twin .45s, Switchblade, Burglary, Pickpocketing, 41 Derringer, Leo De Luca, Hard Knocks, Guard Dog, 2 copies of Liquid Courage, Blackjack, 2 copies of Knife, 2 copies of Flashlight Jim\u2019s Trumpet, Forbidden Knowledge, Holy Rosary, Shrivelling, Scrying, Arcane Studies, Arcane Initiate, Rabbit\u2019s Foot, Rite of Seeking,2 copies of Ritual Candles, Clarity of Mind, Peter Sylvestre, 2 copies of Knife, 2 Flashlight, Kukri Duke, Leather Coat, Scavenging, Baseball Bat, Rabbit\u2019s Foot, Stray Cat, Dig Deep, Magnifying Glass, Laboratory Assistant, Old Book of Lore, 2 copies of Fire Axe, Peter Sylvestre, 2 copies of Knife, 2 Flashlight Events Evidence!, Dodge, Dynamite Blast, Mind over Matter, Working a Hunch, Barricade, 2 Emergency Cache Mind over Matter, Working a Hunch, Barricade, Drawn to the Flame, Ward of Protection,Blinding Light, 2 Emergency Cache On the Lam, Evidence!, Dodge, Dynamite Blast, Elusive, Backstab, Sneak Attack, 2 Emergency Cache Drawn to the Flame, Ward of Protection, Blinding Light, Cunning Distraction, \u201cLook what I found!\u201d, Lucky!, 2 Emergency Cache Elusive, Backstab, Sneak Attack, Cunning Distraction, \u201cLook what I found!\u201d, Lucky!, 2 Emergency Cache Evidence!, Dodge, Dynamite Blast, Blinding Light, 2 copies of Taunt, Teamwork, 2 Emergency Cache Search for the Truth, Mind over Matter, Working a Hunch, Barricade, Elusive, 2 Shortcut, 2 Seeking Answers, Think on Your Feet,2 Emergency Cache Elusive, Backstab, Sneak Attack, Dynamite Blast, Think on Your Feet, Teamwork, 2 Emergency Cache Drawn to the Flame, Ward of Protection, Blinding Light, Cunning Distraction, \u201cLook what I found!\u201d, Lucky!, 2 Emergency Cache Cunning Distraction, \u201cLook what I found!\u201d, Lucky!, Working a Hunch, 2 Bait and Switch, 2 Emergency Cache Skills Vicious Blow, Deduction, 2x Guts, 2x Manual Dexterity Deduction, Fearless, 2x Perception, 2x Manual Dexterity Vicious Blow, Opportunist, 2x Guts, 2x Overpower Fearless, Survival Instinct, 2x Perception, 2x Unexpected Courage Opportunist, Survival Instinct, 2x Overpower, 2x Unexpected Courage Vicious Blow, Fearless, 2x Overpower, 2x Guts Deduction, Opportunist, 2x Perception, 2x Manual Dexterity Opportunist, 2x Double or Nothing, Vicious Blow, 2x Manual Dexterity, 2 copies of Overpower Fearless, 2 Guts, 2x of Unexpected Courage Survival Instinct, Deduction, 2x Unexpected Courage, 2x Perception"},{"location":"fun/bg/arkhamhorror/#akashy-onyele","title":"Akashy Onyele","text":"<p>Deck in arkhamdb level 0 from travis explanations. Rite of seeking to help on clue. Upgrades: grotesque statue, arcane initiate, ward of protection 2, book of shadow, the lvl 4, arcane insight, suggestion, seal of the elder sign.</p>"},{"location":"fun/bg/arkhamhorror/#sefina-rousseau","title":"Sefina Rousseau","text":"<ul> <li>Her special ability can transform any event into a 5 instance card.</li> <li>Consider keeping events within the same goal: clue, combat, resources... And complement the others with assets. It improve consistency.</li> <li>Select agility or will power but not both. She is the only rogue with agility and will at 4. Very good for her.</li> <li>Protect her 5 lifes with dmg soak cards or healings.</li> <li>video on her. </li> </ul>"},{"location":"fun/bg/arkhamhorror/#harrigan","title":"Harrigan","text":"<p>An interesting deck..</p>"},{"location":"fun/bg/arkhamhorror/#ursula-downs","title":"Ursula Downs","text":"<p>Played her in FA. A YouTube presentation.</p> <p>She is very single-minded stat set, GET ALL THE CLUES, EVADE ALL THE ENEMIES</p> <ul> <li>Focus on clues and move around. Use intellect to combat and evade most of the time.</li> <li>Add shrewd analysis to upgrade two copies of archaic glyth</li> <li>interesting relics: chthonian stone, tooth of eztli,  ornate bow, elder sign, key of ys, gold pocket watch. Now if playing with relic I need to have Eli Horowitz. </li> <li>Boost her willpower for treasury : tooth of etzil, elder sign, logical reasoning</li> <li>Think to play the archaic glyth, so resolve it on first scenario of a campaign.</li> <li>2 of each: unexpected courage, flashlight, emergency cash, trench coat</li> <li>Hyper awareness</li> <li>Add 2x doctor christopher, anatomical diagram, I have a plan, mind over matter to help on combat and evasion.</li> <li>Fieldwork is mostly mandatory to improve speed</li> <li>2x shortcut, deduction, fieldworks, pathfinder,  no stone unturned.</li> <li>Muligan to get milan christopher out asap, and fieldwork</li> <li>Think to add charisma to have both christopher and jake out at the same time.</li> <li>Deduction can be upgraded</li> <li>+7 on investigation with fieldworks and Dt Christopher</li> </ul>"},{"location":"fun/bg/arkhamhorror/#leo-anderson","title":"LEO ANDERSON","text":"<ul> <li>Leo is going to want a lot of allies, and Beat Cop and Venturer are standout level 0 ally cards for him</li> <li>Use Rogue cards to boost his resources gathering: Lone Wolf, Sleight of hand</li> <li>Not sure about leo de lucas\u2026 consider all allies in neutral, guardian and rogues</li> <li>charisma is mandatory to get more allies</li> <li>Mitch Brown helps to combo Beat Cop and Venturer. Venturer means .45 automatic to reload it</li> <li>Think about calling in favors</li> <li>interesting to consider: daring maneuver, decorated skull, eavesdrop, liquid courage, lone wolf [Solo], lucky dice [a must], narrow escape, quick thinking</li> <li>give more intellect power with treasure hunter, keen eye, and flash light. </li> </ul>"},{"location":"fun/bg/arkhamhorror/#scenario-commments","title":"Scenario commments","text":"Return of the night of the zealot: 2/29/2020 <p>with Daisy Walker and Skid O\u2019Toole.</p> <ul> <li>Daisy and Skid get rid of the goulhs priest at the last turn.</li> <li>Lita Chantler burns the house to the ground. and joins Daisy to work on enemies</li> <li>Daisy has 2 traumas after the gathering</li> <li>The team gets insight into the hidden world of the Mythos: total victory: 6 VP for gouls 2 VP location + 2 = 10 VPs</li> <li>Daisy spent 6 xp and skids 10 xp</li> <li>The midnight masks:  R2. </li> <li>Cultists we interrogated: ruth turner, peter warren</li> <li>Cultists who got away: bill cooper, herman collins, victoria devereux, jeremiah pierce</li> <li>It is past midnight</li> <li>5xp but skids has his hospital debts so only 3xps for him</li> <li>Ghoul priest is still alive.</li> <li>The cult worships Umordhoth a monstrous entity from another realm. But we discovered that lita sets  this event in motion. From a journal possessed by the cultist, there is a dark ritual to be performed in the wood of south Arkham to bring umordhoth back.</li> </ul> The Circle Undone <p>2023 with Nathaniel Cho and Preston Frairmont. Select the choice to be against the Twillight Circle. Not easy campaign. Evade can be a good strategy, at least for the Watcher. </p>"},{"location":"fun/bg/arkhamhorror/#best-5-scenarios","title":"Best 5 scenarios","text":"Rank Bryn Justin Travis 5 the essex country express the search for kadath the secret name 4 the unspeakable oath the depths of yoth beyond the gates of sleep 3 the city of archives a phantom of truth before the black throne 2 before the black throne blood on the altar dim carcossa 1 dim carcossa dim carcossa the midnight masks"},{"location":"fun/bg/summonerwars/","title":"Summoner wars notes","text":"<p>This is a hand management game. You get a random draw of cards. In Summoner Wars, you end up with a near infinite number of decisions to make regarding your hand. What to play? In what order? What to keep for a future turn? What to build as magic? What is the likelihood I'll get another one of these? To what benefit is it to play this card now vs. waiting until I get the perfect card to combo with it?</p>"},{"location":"fun/bg/summonerwars/#general-practices","title":"General practices","text":"<ul> <li>Understanding how to get the most out of your hand--and to ensure that your next hand will be as good as possible--is a key cog in Summoner Wars strategy.</li> <li>Memorize unit abilities: faction abilities are what separates your army from your opponent</li> <li>Summon unit for attack same turn or to move it into position to attack next turn.</li> <li>Don't summon new units when you've already got enough units on the board to reach your move and attack limits</li> <li>Plan the whole turn before you summon anything</li> <li>Move to a strategy based on Events: Event cards are the lifeblood of your deck, swinging the game in your favor as much as a solid unit, usually without costing you any magic.</li> <li>Use common as magic, unless there is a good reason to summon them. Know the number of card per common. Build magic.</li> <li>Keep track of magic piles and champions occurence and appearance on next turn. In the same way, be aware of how much magic your opponent has, and think through which champions they could be preparing to summon.</li> <li>Keep track of opponent's powerful events</li> <li>Learn the power of phase-wraping events and abilities: Ex: <code>Burn</code> and <code>Greater Burn</code> add wounds before the movement phase. Summoning during event phase is great too (<code>Summoning Surge, Forced Summon, Mirage, Channeled Summon and Reinforcements</code>): place a forward wall, then play the event, then summon a unit from a place (probably near the enemy summoner).</li> <li>Understand the unique card economy: every unit's true cost is its summoning cost + 1 (the cost of not using it as magic). Every unit you summon and every event you play is one less magic you could've used for something else. A no draw is truly terrible because even the worst-timed hand is worth 5 built magic.</li> <li>Do not complain about dice rolls, learn to fix potential flaws.</li> <li>Summoner can do a lot of attack in battlefield. Always have an escape route. Most opponents will spend their resources inefficiently in order to try to wound your summoner at all costs. If you have an effective escape plan, your summoner will get away with 3 or so life left, and you will have a much taller magic pile and draw pile than your opponent for the rest of the game. </li> </ul>"},{"location":"fun/bg/summonerwars/#phoenix-elves-prince-eilien","title":"Phoenix Elves - Prince Eilien","text":"<p>Burn and Great burn events make differences. Archers have to be kept far, and can hit at 4 spaces. </p>"},{"location":"golf/","title":"Lessons analysis","text":"<p>See also new Renaud Poupard training</p>"},{"location":"golf/#training-session-plan","title":"Training session plan","text":"<ul> <li>20 balls: Wedge warm up - line on the ground - goal is to make contact - 30, 50 and 70 yards. Work way up to full swing.</li> <li>30 balls: Mechanicals: back to the feeling with 9 iron.</li> <li>25 balls: Skill: work on ball fligh control. Games with a score. </li> <li>25 balls: Game like strikes: Driver, club, pitch.</li> </ul> <p>Routine: Target -&gt; Two swings -&gt; Feels -&gt; Visualize the shot -&gt; Take immediate target -&gt; Breeze -&gt; Line up -&gt; View target - ball x2 -&gt; Swing.</p> <ul> <li>Try to train a lot on: Wedges, Driver, Putter.</li> </ul>"},{"location":"golf/#prepare-before-a-18-holes-cource","title":"Prepare before a 18 holes cource","text":"<ul> <li>Warm slowly but cover all game plays.</li> <li>Divide by segments.</li> </ul> <ul> <li>Warm-up: all muscles - stretch - 1 minutes</li> <li>Sensation: 10 to 15 balls - Half swing - SandWedge or Approach Wedge - Use head weight - feeling.</li> <li>Rythm: 30 balls max Wedge - full swing with alignment and preparation - view the shot before doing it.</li> <li>Full Sing: 20 balls - iron 7 or 6 - 2 clubs on the ground for alignment - use the routine.</li> <li>Alignment: 10 balls - wood and driver - focus on rythm - finish with half swing on wedge.</li> <li>Chipping: 15 minutes - 10 shots with different clubs: S, P, 8. -&gt; put 9 balls in 3m circle. Think landing zone and adapt with club and swing height.</li> <li>Putting: 5 minutes - sensation and speed.</li> </ul>"},{"location":"golf/#full-swing","title":"Full Swing","text":""},{"location":"golf/#2021-811-full-swing-analysis","title":"2021 8/11: Full swing analysis","text":"<ul> <li>Need to get shoulder tilt at 8% right</li> <li>Hands too high at the beginning of the back swing, hips do not start to turn.</li> <li>Swing started by hands moving not by hip turn</li> <li>Zach has the hands lower.</li> </ul> <ul> <li>He keeps them lower at the end of the back swing and not going too far.</li> </ul> <ul> <li>I go way too far on back swing</li> </ul> <ul> <li>Shoulders tilt is good, down enought.</li> <li>Shoulders turn is over exagerating at 95/99 degrees</li> <li> <p>Hip turn should be 44 and not 37 degrees</p> </li> <li> <p>Front view, the hands needs to be lightly moved on the left.</p> </li> </ul> <p></p> <ul> <li>At position 2, hands by the belt buckle, club horizontal // to the ground</li> </ul> <p></p> <ul> <li>At position 3: Club should point vertically, arm straight, // to the ground</li> </ul> <p></p> <p>We can see also the left part of the body did not move. It has to move forward to help turning the hips  </p> <ul> <li>Head starts to move out of the position to the right. Avoid head latteral movement.</li> <li> <p>Shoulders turn supposed to be at 77, here 72 is good, and tilt should be 35. 30 is still good.</p> </li> <li> <p>Position 4: Zach starts hip sway on the left, going to the target. His head does not move at all.</p> </li> <li>My arms bend, arms should not be used</li> </ul> <p></p> <p>arms just hold the club.</p> <ul> <li> <p>Too much back swing, left knee blocked.</p> </li> <li> <p>when going way down, hit sway should be around 3 toward and not 5.1</p> </li> <li>Zach has a longer distance hand to belt buckler because he started to move his hips to target before his back swing</li> </ul> <p></p> <ul> <li>At impact the golf arms do a triangle with arm extension. my right arm has a light flex</li> </ul> <p></p> <p>But the hand are wrong as the right hand is going over the left (flipped) to compensate a pulling and turning effect. Right hand palm should be parallel to the club face.</p> <ul> <li>Finally extension of the arms and lower right shoulder.</li> </ul> <p></p>"},{"location":"golf/#actions-to-improve","title":"Actions to improve","text":"<ul> <li>Fligh pattern observed: push slices.</li> <li>Resolution: keep weight on the left side</li> <li>Left quad pushes left knee over the left foot, at the same time the left shoulder should go down to the ground as well. Feel the hands go behind you as you take the club back.</li> </ul>"},{"location":"golf/#drills","title":"Drills","text":"<ul> <li>Problem: hit the groud after the ball. This may come from the hip sway on the left, so us the KNEE FLEX FOR HIP SWAY EXERCISE:  flex left knee to the left toes to avoid the hip to move laterally. you will stay more over the ball.</li> </ul> <ul> <li>Problem: swing plane. most golfer are moving the hands high too quickly or too far from the hip as soon as they start doing a backswing and then drag the golf club back leading to inconsistency. </li> </ul> <p>Use the HAND PATH EXERCISE WITH HULA HOOP: hands are tracing the hula loop circle.</p> <p></p>"},{"location":"golf/#2021-824","title":"2021 8/24","text":"<ul> <li>Back swing going too far. Once shoulder have turned, arms continue.</li> <li>Arms are taking over the shoulders</li> <li>Hands go too high. </li> <li>Left elbow too far from the rips: train with a towel under the arm to learn to keep arms compact and not moving hands too high.</li> </ul> <ul> <li> <p>Head moving back to the left, shoulders too flat.</p> </li> <li> <p>Fix the elbow, (left photo) closer now to the rips, which improves the shoulder tilt to 33d. Target could be 37d.</p> </li> </ul> <p></p> <ul> <li>Shoulder turn is over exagerated (blue)</li> <li>Shoulder bend is at 4d </li> <li>Zach is showing a more compact upper body.</li> </ul> <p></p>"},{"location":"golf/#drills_1","title":"Drills:","text":"<p>To avoid moving the arms too high, place tees or towels under both arm pits and swing</p> <p>left knee over left foot. left shoulder goes down to the ground. feel you hands go behind you as you take the club back.</p>"},{"location":"golf/#2021-1214","title":"2021 12/14","text":"<ul> <li>Back swing is better now:</li> </ul> <ul> <li>Down swing, club is coming from outside and hands finish inside</li> </ul> <ul> <li>When hitting the ball hands are inside.</li> </ul> <ul> <li>Improve by getting right should lower (at 30d) and extending arms:</li> </ul>"},{"location":"golf/#drills_2","title":"Drills:","text":"<ul> <li>Shoulder tilt exercise: tilt to the right on the followthrough. Pros have their shoulders tilted on average at 49d. Use the image of water dripping from right ear.</li> </ul> <p>On followthrough the right shoulder should go down to the ground. Then swing more from the inside and swing out rightward.</p>"},{"location":"golf/#other-full-swing-videos-from-practices","title":"Other full swing videos from practices","text":"<ul> <li>Practice 09/02-2022 - bad swings: after 2 weeks of hitting thin, topping, with bad draws, let tape what's going on...</li> </ul> <ul> <li>Better swings: still need to work on arms extension at and after impact, be sure to start by the belt buckler.</li> </ul>"},{"location":"golf/#pitching","title":"Pitching","text":"<p>From Pete Cowen's article core principles and his video:</p> <ul> <li>Working down, up and around to the finish, the rotation of the left shoulder effectively governs the movement of the arms, hands and clubhead</li> <li>The momentum of the pitch is controlled by the turning motion of the upper body</li> <li>The continuous turning of the left shoulder makes for a natural release of the left side and enables you to control arms and club</li> </ul>"},{"location":"golf/#drill-1","title":"Drill #1","text":"<ul> <li>Comfortable set-up, the feet fairly close together, left toe turned out slightly, and weight just favouring the left side.</li> <li>Take the right hand off the grip and place it on the left shoulder. Get the sensation of left side control. Hold on the grip, while placing the right hand on the left shoulder simply raises the awareness of that area of the upper body.</li> </ul> <ul> <li>Make a simple swing, and focus being on controlling the motion with the turning of the left shoulder </li> <li>Feel that the right hand pulls the left shoulder down and around into the backswing, setting off a chain reaction as the arms and hands respond, the left wrist cocking gently in response to the weight of the clubhead.</li> </ul> <ul> <li>Feel the upper body paces the return journey, the left arm/hand following this momentum, the left wrist uncocking naturally and the clubhead collecting the ball in the process. There is no manipulation whatsoever in the strike. It is neutral as the left arm and body work in unison. </li> <li>The shaft is seen to be leaning toward the target at impact.</li> </ul> <p>The key here is that you feel and mobilise the muscles within the left shoulder to control the movement of the arms and, ultimately the club</p>"},{"location":"golf/#ball-fly","title":"Ball fly","text":"<ul> <li>For a lower ball, move the ball back a fraction, lean the clubshaft gently toward the target (delofting the face) and build your stance accordingly. </li> <li>The swing follows pattern above, weight just favouring the left side throughout, and the low shot is always characterised by a fairly low finish.</li> </ul> <ul> <li>(L) Hands gently ahead of the ball, leaning the shaft angle toward the target.</li> <li>(C) Left shoulder works down, left arm swings across the chest to create controlled backswing, hands relatively passive.</li> <li>(R) Weight rotates into the left side as the upper body controls the delivery of the arms and club.</li> </ul> <ul> <li>For higher ball, play the ball a little further forward in the stance, lean the shaft away from the target a little</li> </ul> <ul> <li>(L) Hands slightly behind the ball, leaning the shaft gently away from the target </li> <li>(R) Wrists this time encouraged to hinge in order to maximise the loft on the clubface</li> </ul> <p>A higher, floating shot requires more clubhead speed, so extend the length of the swing.</p> <p></p> <ul> <li>(L) Right hand releases much earlier in order to slide the open face beneath the ball</li> <li>(R) The higher the shot, the fuller the finish \u2013 note the way the left shoulder has rotated out of the way to allow the right side to turn through</li> </ul>"},{"location":"golf/#distance-control-a-factor-of-swing-length-rhythm","title":"Distance control: A factor of swing length &amp; rhythm","text":"<p>To control distance in the pitching arena; a \u2018clockface\u2019 principle that involves matching body rotation and length of arm swing on either side of the ball to identify with the \u2018best\u2019 (i.e. most consistent) landing distances using the lofted scoring clubs.</p>"},{"location":"golf/#personal-videos","title":"Personal videos","text":"<ul> <li>Pitch Practice 09/02-2022</li> </ul>"},{"location":"golf/#putter","title":"Putter","text":"<ul> <li>Use long putt to putt in a 5 foot square.</li> <li>3 to 5 foot putts exercise with 10 balls</li> <li>Alignment exercise with two poles or club on the ground - follow the rails</li> <li>Pass the gate with straight line</li> <li>Pass the gate with curve.</li> </ul>"},{"location":"golf/#mental-game","title":"Mental Game","text":""},{"location":"golf/#difficult-shots","title":"Difficult shots","text":"<ul> <li>Think you are already after a penality or a second bunker shot -&gt; it will increase the chance to get a better shot.</li> </ul>"},{"location":"golf/#recover-from-bad-hole","title":"Recover from bad hole","text":"<ul> <li>Take 10 steps: Let yourself get mad about the shot you\u2019ve just hit for 10 steps. When you take that 11<sup>th</sup> step, move on. </li> <li>Breathe in for five seconds, hold that breath for five seconds and breathe out for five seconds.</li> <li>Yaking things \u201cone shot at a time\u201d </li> </ul>"},{"location":"golf/golf/","title":"Renaud Poupard - coursdegolf.net","text":""},{"location":"golf/golf/#inertia-club-training","title":"\"Inertia club\" training","text":"<p>The swing mechanics includes 5 dimensions:</p> <ol> <li>Dynamic oft of the club</li> <li>Club path</li> <li>Attack angle</li> <li>Contact spot</li> <li>Speed</li> </ol> <p>9 trajectories: start straight, right, left, then ball stays straight, go to the right or left.</p> <p>There is only one straight fly: a club path at 0 and club face at 90 degres. most of the balls goes on right and left, with different intensity, finishing at 1m of the target to 30 m. So we have only 2 types of trajectory.</p> <p>The problem is that if we try to search to get the ball straight, which does not realisticly exist, then we will not be able to make it and we will not control the ball fly. We could not play straight ball.</p> <p>The ball initiates its fly from the club face. Radar technologies demonstrated that the club square to the target did not brings the ball to the target.</p> <p>The natural trajectory is the one to keep. Make trajectory that comes to the target.</p> <ul> <li>a slicer makes the ball flies to the right. a slice arriving to the target is a FADE.</li> <li>a hooker makes the ball flying to the left. A hook arriving at the target is a DRAW.</li> </ul> <p>Club in the hands can help to make the ball flying where we want. Alignment too. Grip too.  Lie of the club face.</p> <p>A pro player approximates to the left or right. never straight. Theit path is constant</p>"},{"location":"golf/golf/#training","title":"Training","text":"<ul> <li>Back to the fundamentals: static with grip, alignment, position, stance, balance</li> <li>Dynamic: RRRE: relancher, rotation, Rythm, Equilibre</li> <li>Work on feelings, when feeling is good then technics will be, not the opposite: light in hands, no tension, equilibre.</li> <li>Avoid to work too much on technics, and no more than working on one technic per training</li> <li>Not try to get the best shot each time. Not hit too much on the ball. 70% of power. Try to get it correct. Slower = more centred balls.</li> <li>Golf score is about avoiding worse shots, but not best one.</li> <li>Get a training plan. Do not work on what was done in pervious shot. Focus on one thing. Plan in advance.</li> </ul>"},{"location":"golf/golf/#1-learn-the-swing-momentum","title":"1. Learn the swing momentum","text":"<ul> <li>The body weight helps to create momentum. The swing of the club. Set the image to push a child in a swing.</li> <li> <p>In this club swing, the hands finish vertical and the club face is vertical.</p> <p></p> </li> <li> <p>Clinic to work on momentum: close to the green, do shipping, with just momentum created by body rotation. No tension in arms, hands, feel the \"balancier\" and inertia after the ball stroke.</p> </li> <li> <p>With a pitch, work on the counterweight to the club with the body. A party of the body, the left, go to the target, while the club is on the back swing: the body acts as a counterweight. Loose backswing.</p> <p></p> </li> <li> <p>Do not accept bad sensation.</p> </li> <li>Shallow swing exercise: send the club shallow, flat. The left shoulder falls under the shin, the head of club pass under the ball. Imagine to send the ball as a ricochet. </li> <li>Build the snail image (head of the club designing a snail): short low backswing, being confortable, most important is to get the club inertia, extended arms, vertical club at 180 and finish on balance. The term is under swing.</li> <li> <p>Let the gravity play, on the descent the club fall with gravity, the right shoulder goes down with the club too.</p> <p></p> </li> <li> <p>The club is hitting the ball in descent, not the human, the lowest point is in front of sternum. Even when hands are on left leg, the club head will still be at the bottom of the arc. </p> </li> <li>To get longer distance, we need to build a bigger circle: so go farther and lower on backswing, ease the arms, to get them extended at the impact, and be taller.</li> </ul>"},{"location":"golf/golf/#module-1","title":"Module 1","text":"<ul> <li>intertia / \"balancier\": counterweight with the body at backswing and then on finish.</li> <li>finish in auto-pilot: in balance, club face vertical. shoulder face to target. </li> <li>The swing is with the hips. The rotation of the hips, the shoulders drag the club. Exercise to ship by pulling the club and the ball.</li> <li>Look in front to the horizon, to set the body and squeleton in good position at address. Can even do dry swing while looking at the horizon.</li> </ul>"},{"location":"golf/golf/#shipping","title":"Shipping","text":"<ul> <li>keep body straight, club crossing the grass.</li> <li>use inertia and momentum</li> <li>target 3 meters after the green entrance, for the ball to land</li> <li>adapt the golf club according to the distance of the roll on the green to the flag.</li> <li>Also shipping by looking to the target is a good exercise for sensation and dosage</li> </ul> <ul> <li>Balance at the end of the swing, is very important (see the Relachement Rotation Rythm Equilibre principles from Leadbetter) and starts at the address.  </li> <li>Grip not in life line, be more in diagonal in the palm of the hand</li> </ul>"},{"location":"golf/golf/#module-2","title":"Module 2","text":"<ul> <li>Shallow practice for better ball contact</li> <li>Open club face, like in bunker, to have higher ball, but same swing and club face vertical when body is facing the target</li> <li>In case of socket, may be put the ball 2 cm farther. The club hits closer to the shaft. </li> <li>Use counterweight and 7 second in the finish.</li> </ul> <ul> <li>Top came because of holding the club at impact, do dry swing to feel the inertia.</li> <li>The left arm is straight on back swing but not thighten, no tension, inertie brings the arm straight.  </li> </ul>"},{"location":"golf/golf/#module-3-long-iron-and-draw","title":"Module 3: Long iron and draw","text":""},{"location":"golf/golf/#module-4","title":"Module 4","text":"<p>Create space between club and body on back swing. </p> <p>Topping the ball is coming from a hand twist at impact or a small body movement upward.  To avoid top or thin ball, use the under swing image, by dropping the club with gravity, keep the right shoulder low. Gather the ball.</p>"},{"location":"golf/golf/#module-5-wood-and-driver","title":"Module 5: Wood and driver","text":"<ul> <li>Be lightly on the left, left shoulder and right hip pointing higher in the sky.</li> <li>Use shallow swing to arrive flat and from bottom to up</li> <li>Keep club low when starting backswing</li> <li>To find the fluid transition and best inertia, without any pression in the hands: Double finishes swing to feel the inertia of the club: to improve rotation, let the gravity play.</li> <li>Start by doing short swing with driver, like it is an approach. To learn to get lower.</li> <li>Speed comes from the swing arc.</li> <li>Work on finish in pure balance.</li> </ul>"},{"location":"golf/golf/#course-management","title":"Course management","text":"<p>Do not think the next shot will be your best, so add one or more club as most likley you will be shorter. Smooth take away is the most important (Leadbetter, Laura Davies,..) thing in the golf swing. Smooth means we start by moving the body, quick it means we move the hands and arms before. Routine to work on the trajectory with 8 iron and higher: one straight, one fade, one draw, one high (move ball forward in stance), one low (ball at the beginning of the stance, keep hands low after impact). Follow the feet direction for the take away.</p>"},{"location":"golf/golf/#structural-training","title":"Structural training","text":""},{"location":"golf/golf/#core-principles","title":"Core principles","text":"<ul> <li>We could not be in confidence on shots we do not master: need to train in all golf shots. When not in confidence we are back to the swing mechanic and not playing golf.</li> <li>Improve the swing, increase the good shots. To improve score we need to remove bad shots, which is not just the swing improvement. (1 lost ball = 2 shots, 1 top = 1 shot lost, 3 puts = 1 shot lost)</li> <li>Learn how to play with our current swing technic. Holistic Training</li> </ul>"},{"location":"golf/golf/#the-training","title":"The training","text":"<p>Start by contact only drill, no focus on direction.</p> <p>The most common training is taking a bucket of 40 balls, hit around 30 with 7 iron, few with driver and then 3 with wedge. So 3 shots on 4 balls. The structural training is focusing on the shot type. Use 36 balls around, 6 balls for 6 exercises, focusing on trajectory. The goal is to work on all part of the game. Privilege the ball contact.</p> <p>Warm by relaxing the back spine, the back and arms, using breath.</p> <ol> <li> <p>5 - 6 meters from the green. 6 balls for 6 different flight: immediate green entrance, 2 meters, close to the pin (higher flight). x2. Use pitching wedge.</p> <ul> <li>short fly - long roll: body weight on left foot. ball more right foot - closed club face. Light arm to get better contact. Interior to exterior club trajectory. Position feet like for a draw.</li> <li>higher fly: open club face, ball in front of sternum, open to the left feet position like for a slice.</li> </ul> </li> <li> <p>Divot exercise: Sand wedge - goal is ball contact. no divot, small one, bigger one. 40 meters to the flag.</p> <ul> <li>no divot: club hits ball at the lower point of the swing arc: ball in front of sternum, shoulder parallel to the ground</li> <li>small divot - grass only: ball on right of sternum - left shoulder on the left - weight on left. all moderate.</li> <li>bigger divot - with dirt : ball more on right, more weight on left.</li> </ul> </li> <li> <p>Short game with iron 7 - on tee. Same trajectory for 6 shots. Not full swing. short stance. Focus on ball impact and relaxation.  goal: is rescue shot the 70+ meter or get out of a under tree... work on the routine with 1 minute per ball, always use a \"coup d'essai\".</p> </li> <li>Longer than previous ball: play on different factors:  club speed, swing length, club face. Use smaller stance, ball on right foot. Focus on follow through.  </li> <li>Driver contact: one ball with right arm get the feeling. second ball with 2 hands on the same feeling. No focus on distance and trajectory.</li> <li>Putt: distance control: balls arrive at a club distance of the gree border. Focus on dosage.</li> </ol>"},{"location":"golf/golf/#other-sources","title":"Other sources","text":"<ul> <li>Paul Runyan: the shortest way to lower the score.</li> </ul>"},{"location":"java/","title":"Java studies","text":"<p>Java studies repository with a bench of code</p>"},{"location":"java/#18-new-features","title":"1.8 new features","text":"<p>Java 8 tutorials</p>"},{"location":"java/#functional-programming","title":"Functional programming","text":"<p>A functional programming function is like a mathematical function, which produces an output that typically depends only on its arguments. Functions exhibit referential transparency, which means you could replace a function call with its resulting value without changing the computation's meaning. With function Java becomes declarative while with object it is imperative.</p> <p><pre><code>// compute the length of a string. \nFunction&lt;String, Integer&gt; func = x -&gt; x.length();\nAssertions.assertEquals(6,func.apply(\"jerome\"));\n// can chain functions\nFunction&lt;Integer, Integer&gt; func2 = x -&gt; x * 2;\nAssertions.assertEquals(12,func.andThen(func2).apply(\"jerome\"));\n</code></pre> They favor immutability, which means the state cannot change. Function doe not support variable assignments.</p> <p>Function examples</p>"},{"location":"java/#lambdas","title":"Lambdas","text":"<ul> <li>single method classes that represent behavior</li> <li>can be assigned to a variable</li> <li>can be pass as argument to a method</li> <li>The type of any lambda is a functional interface: A functional interface is a special interface with one and only one abstract method.</li> </ul> <pre><code>@FunctionalInterface\npublic interface Function&lt;T, R&gt; {\n      R apply(T t);\n}\n</code></pre> <pre><code>* It has one and only one abstract method.\n* It can be decorated with an optional @FunctionalInterface\n</code></pre>"},{"location":"java/#new-date","title":"New date","text":"<p>Java 8 created a series of new date and time APIs in java.time package</p>"},{"location":"java/#streams","title":"Streams","text":"<p>A Stream is a sequence of elements from a source that supports aggregate operations. Streams API lets you process data in a declarative way. Aggregate functions are SQL-like operations and common operations from functional programing languages, such as <code>filter, map, reduce, find, match, sorted</code>.</p> <p>Streams can leverage multi-core architectures without you having to write a single line of multithread code.</p> <pre><code>List&lt;String&gt; lines = Arrays.asList(\"spring\", \"node\", \"quarkus\");\nList&lt;String&gt; result = lines.stream()                // convert list to stream\n             .filter(line -&gt; !\"spring\".equals(line))     // we dont like spring\n             .collect(Collectors.toList());              // collect the output and convert streams to a List\n\nresult.forEach(System.out::println);                \n</code></pre> <p>Search within a list <pre><code>Person result1 = persons.stream()                        \n                .filter(x -&gt; \"jack\".equals(x.getName()))        // we want \"jack\" only\n                .findAny()                                      // If 'findAny' then return found\n                .orElse(null); \n</code></pre></p> <p>Many stream operations return a stream themselves. This allows operations to be chained to form a larger pipeline, which can be seen as forming a query on the data.</p> <pre><code>List&lt;Integer&gt; transactionsIds = \n    transactions.stream()\n                .filter(t -&gt; t.getType() == Transaction.GROCERY)\n                .sorted(comparing(Transaction::getValue).reversed())\n                .map(Transaction::getId)\n                .collect(toList());\n</code></pre> <p><code>map()</code> is used to extract information. No work is actually done until collect is invoked. The collect operation will start processing the pipeline to return a result. <code>toList()</code> describes a recipe for converting a Stream into a List.</p> <ul> <li>Stream operations that can be connected are called intermediate operations. </li> <li>Operations that close a stream pipeline are called terminal operations.</li> </ul> <p>See streams tests in</p> More readings <ul> <li>Streams filter examples</li> <li>Processing Data with Java SE 8 Streams</li> <li>java.util .stream.Stream</li> </ul>"},{"location":"java/#enterprise-context","title":"Enterprise Context","text":"<p>The <code>Context</code> refers to the interface used to interact with your runtime environment. In java we have Servlet's ServletContext, JSF's FacesContext, Spring's ApplicationContext, Android's Context, JNDI's InitialContext,...</p> <p>Note</p> <pre><code>Recall that Java EE servers host several application component types that correspond to the tiers in a multi-tiers application. The Java EE server provides services to these components in the form of a container.\n\n* The web container is the interface between web components and the web server. The container manages the component\u2019s life cycle, dispatches requests to application components, and provides interfaces to context data, such as information about the current request. \n* EJB container is the interface between enterprise beans, which provide the business logic in a Java EE application.\n* The application client container runs on the client machine and is the gateway between the client application and the Java EE server components.\n</code></pre> <p>javax.enterprise.context is a set of annotation APIs to define component scope. See ApplicationScoped.</p>"},{"location":"java/#java-11-updates","title":"Java 11 updates","text":"<p>Baeldung Java 11 new features</p> <ul> <li>OpenJDK as Hotspot is licenced now</li> <li>New String methods like isBlank, lines, repeat, strip...</li> </ul> <pre><code>String multilineString = \"Baeldung helps \\n \\n developers \\n explore Java.\";\nList&lt;String&gt; lines = multilineString.lines()\n  .filter(line -&gt; !line.isBlank())\n  .map(String::strip)\n  .collect(Collectors.toList());\nassertThat(lines).containsExactly(\"Baeldung helps\", \"developers\", \"explore Java.\");\n</code></pre> <ul> <li>Add stream capability, Not Predicate </li> <li>local variable in lambda</li> </ul> <p><code>java    sampleList.stream().map((@Nonnull var x) -&gt; x.toUpperCase())</code></p> <ul> <li>HTTP client: improves overall performance and provides support for both HTTP/1.1 and HTTP/2</li> <li>we can directly run the file using the java command, no more need to explicitly call javac</li> <li>Improved Aarch64 Intrinsics to leverage CPU capabilities</li> <li>Java Flight Recorder (JFR) is open source and used for jvm profiling.</li> </ul>"},{"location":"java/#web-sites-with-good-content","title":"Web sites with good content","text":"<p>Good web sites I like to go back to for excellent source of knowledge</p> <ul> <li>Baeldung</li> <li>Open liberty</li> <li>Quarkus</li> <li>Microprofile guides</li> <li>Vert.x</li> </ul>"},{"location":"java/#sdkmanio","title":"sdkman.io","text":"<p>sdkman is a tool for managing parallel versions of multiple Software Development Kits like java.</p> <pre><code># update a project to set environment to create a .sdkmanrc file\nsdk env init\n# list existing java installable version\nsdk list java\n\n# install a specific version\nsdk install java 8.0.265.hs-adpt \nsdk use java 11.0.8.j9-adpt\nor\nsdk use java 8.0.265.hs-adpt\n# then do your maven project with older 1.8 compile.\n</code></pre>"},{"location":"java/#maven","title":"Maven","text":"<p>Update maven cli: download it from http://maven.apache.org/download.cgi, unzip to ~/Tools. modify $PATH.</p>"},{"location":"java/#create-a-java-project-with-maven","title":"Create a java project with maven","text":"<pre><code>mvn archetype:generate -DgroupId=jbcodeforce -DartifactId=apicurio-client -DarchetypeArtifactId=maven-archetype-quickstart\n</code></pre>"},{"location":"java/#create-executable-java","title":"Create executable Java","text":"<ul> <li>Need to generate the manifest from the class, for that we use the following plugin</li> </ul> <pre><code>  &lt;plugin&gt;\n        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n        &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;\n        &lt;version&gt;2.4&lt;/version&gt;\n        &lt;configuration&gt;\n          &lt;archive&gt;\n            &lt;manifest&gt;\n              &lt;addClasspath&gt;true&lt;/addClasspath&gt;\n              &lt;mainClass&gt;ibm.swat.KafkaClientV35&lt;/mainClass&gt;\n              &lt;classpathPrefix&gt;dependency-jars/&lt;/classpathPrefix&gt;\n            &lt;/manifest&gt;\n          &lt;/archive&gt;\n        &lt;/configuration&gt;\n      &lt;/plugin&gt;\n</code></pre> <ul> <li>it also needs to get dependencies</li> </ul> <pre><code> &lt;plugin&gt;\n        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n        &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;\n        &lt;version&gt;2.5.1&lt;/version&gt;\n        &lt;executions&gt;\n          &lt;execution&gt;\n            &lt;id&gt;make-assembly&lt;/id&gt;\n            &lt;phase&gt;package&lt;/phase&gt;\n            &lt;goals&gt;\n              &lt;goal&gt;copy-dependencies&lt;/goal&gt;\n            &lt;/goals&gt;\n            &lt;configuration&gt;\n              &lt;outputDirectory&gt;\n                ${project.build.directory}/dependency-jars/\n              &lt;/outputDirectory&gt;\n            &lt;/configuration&gt;\n          &lt;/execution&gt;\n        &lt;/executions&gt;\n\n      &lt;/plugin&gt;\n</code></pre> <p>See tech academy tech jam pom file.</p>"},{"location":"java/#multi-module-maven","title":"Multi-module maven","text":"<p>This project includes different modules to test different subjects related to the java last language features and other topics on Java, like reactive messaging, kafka, ... See this tutorial to understand maven multi modules.</p> <ul> <li>Define an aggregator POM</li> </ul> <pre><code>mvn archetype:generate -DgroupId=jbcodeforce -DartifactId=Java-studies\n</code></pre> <ul> <li>Change the <code>&lt;packaging&gt;</code> element to <code>pom</code></li> <li>Define each module in separate child folder with its own pom using the command:</li> </ul> <pre><code>mvn archetype:generate -DgroupId=jbcodeforce  -DartifactId=AlgoPlay\n</code></pre> <p>By building the project through the aggregator POM, each project that has packaging type different than pom will result in a built archive file</p> <p>By setting packaging to pom type, we're declaring that project will serve as a parent or an aggregator for other sub module project. A module element is added to the parent project.</p> <p>When running <code>mvn package</code> command in the parent project directory, Maven will build and test all the dependent modules. </p> <p>To bypass integration test: <code>mvn install -DskipITs</code></p> <p>Note</p> <pre><code>Recall that maven `profile` helps to define specific environments for maven execution. It is useful, to set some different runtime properties. Useful for testing and production packaging.\n`mvn package -P test`. See some examples [here](https://www.mkyong.com/maven/maven-profiles-example/)\n</code></pre>"},{"location":"java/#maven-building-jars-with-dependency","title":"Maven building jars with dependency","text":"<p>First you need to declare to use the assembly plugin</p> <pre><code>&lt;build&gt;\n    &lt;pluginManagement&gt;\n     &lt;plugins&gt;\n        &lt;plugin&gt;\n          &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;\n          &lt;version&gt;3.0.0&lt;/version&gt;\n          &lt;configuration&gt;\n            &lt;phase&gt;package&lt;/phase&gt;\n            &lt;archive&gt;\n              &lt;manifest&gt;\n                &lt;mainClass&gt;${fully.qualified.main.class}&lt;/mainClass&gt;\n              &lt;/manifest&gt;\n            &lt;/archive&gt;\n            &lt;descriptorRefs&gt;\n              &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;\n            &lt;/descriptorRefs&gt;\n          &lt;/configuration&gt;\n        &lt;/plugin&gt;\n      &lt;/plugins&gt;\n    &lt;/pluginManagement&gt;\n&lt;/build&gt;\n</code></pre> <p>Then in the plugins section add the execution stanza to specify when to run the assembly.</p> <pre><code> &lt;plugin&gt;\n        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;\n        &lt;executions&gt;\n          &lt;execution&gt;\n            &lt;id&gt;make-assembly&lt;/id&gt;\n            &lt;phase&gt;package&lt;/phase&gt;\n            &lt;goals&gt;\n              &lt;goal&gt;single&lt;/goal&gt;\n            &lt;/goals&gt;\n          &lt;/execution&gt;\n        &lt;/executions&gt;\n      &lt;/plugin&gt;\n</code></pre> <p>There is a better alternate is to use maven shade plugin to build a fat or uber jar. </p>"},{"location":"java/#maven-exec","title":"maven exec","text":"<p><code>mvn  exec:java -Dexec.mainClass=</code></p> <p><code>mvn exec:java -Dexec.mainClass=\"jb.Main\" -Dexec.args=\"arg0 arg1 arg2\"</code></p> <p>Get the available parameters: <code>mvn exec:help -Ddetail=true -Dgoal=java</code></p>"},{"location":"java/#maven-profiles","title":"Maven profiles","text":"<p>Profile helps to use different configuration for different environment.  They modify the POM at build time, and are meant to be used in complementary sets to give equivalent-but-different parameters for a set of target environments. Profiles can be explicitly specified using the <code>-P profilename</code>, or in the activeProfiles in the settings.xml file.</p>"},{"location":"java/#using-maven-with-docker","title":"Using maven with docker","text":"<p>Typical dockerfile</p> <pre><code>FROM maven:3.6.3-jdk13\nCOPY pom.xml workspace/\nCOPY src workspace/src/\nWORKDIR /workspace\nCMD [ \"mvn\" \"compile\" \"quarkus:dev\" ]\n</code></pre> <p>Run and reuse existing m2 to avoid downloading each time</p> <pre><code>docker run -ti --network kafkanet -p 8080:8080 -v ~/.m2:/root/.m2 tmp-builder\n</code></pre>"},{"location":"java/#algo-play","title":"Algo play","text":"<p>The folder Algo play has its own pom, and the following problems are implemented:</p> <ul> <li>Searching in a graph using DPS and Transversal in a graph. See nice article here</li> </ul>"},{"location":"java/#jakarta-ee-8","title":"Jakarta EE 8","text":"<p>Jakarta EE comprises many technologies such as:</p> <ul> <li>Java Server Pages or JSP,</li> <li>Java Server Faces or JSF,</li> <li>Serverlet,</li> <li>JSTL,</li> <li>JDBC,</li> <li>Struts,</li> <li>Java Persistence API or JPA</li> <li>Hibernate ORM</li> </ul>"},{"location":"java/#servlet-40","title":"Servlet 4.0","text":"<p>The typical life cycle:</p> <ul> <li>First the HTTP requests coming to the server are delegated to the servlet container.</li> <li>The servlet container loads the servlet and call <code>init()</code>, before invoking the <code>service()</code> method.</li> <li>Then the servlet container handles multiple requests by spawning multiple threads, each thread executing the service() method of a single instance of the servlet.</li> <li>The servlet is terminated by calling the destroy() method.</li> <li>Finally, servlet is garbage collected by the garbage collector of the JVM</li> </ul> <p>Use the Open Liberty guide from here to create a web app. Below is a quick summary of what was done for JEEPlay project.</p> <ul> <li>Define a server.xml file in src/main/liberty/config using the <code>servlet-4.0</code> feature. Clone this open liberty guide as source.</li> <li>Add open liberty maven app parent artifact</li> <li>Add war as project packaging</li> <li>Add the properties for the app server used in server.xml and integration tests (See pom.xml in JEEPlay folder)</li> <li>Add dependencies on servlet 4.01 api and http client. The dependency is set to provided, which means that the API is in the server runtime and doesn\u2019t need to be packaged by the application</li> <li>Add the Liberty Maven plugin, which gives details of the name, version, and file type of Open Liberty runtime package which it will download from the public Maven repository. The <code>&lt;bootstrapProperties/&gt;</code> section provides the set of variables that the <code>server.xml</code> references. The <code>&lt;stripVersion/&gt;</code> field removes the version number from the name of the application.</li> <li>Maven is configured to run the integration test using the maven-failsafe-plugin. The <code>&lt;systemPropertyVariables/&gt;</code> section defines some variables that the test class uses. The test code needs to know where to find the application that it is testing. While the port number and context root information can be hardcoded in the test class, it is better to specify it in a single place like the Maven pom.xml file because this information is also used by other files in the project.</li> <li> <p>mvn install and then start liberty: </p> <pre><code>mvn liberty:start-server\n</code></pre> </li> </ul> <p>See above steps materialized in code in JEEPlay/... HelloServlet.java and ServletEndPointIT.java.</p> <p>The <code>javax.servlet.annotation.WebServlet</code> helps to replace the web.xml file and servlet section to define name, urlPattern, load on startup... * For integration tests or connecting to remote end point, use the Apache commons client..</p> <p>See also this tutorial on servlet.</p>"},{"location":"java/#jax-rs-resource","title":"JAX-RS resource","text":"<p>In JAX-RS, a single class should represent a single resource, or a group of resources of the same type.</p> <p>The <code>@Path</code> annotation on the resource class indicates that this resource responds to the properties path in the JAX-RS application. The <code>@ApplicationPath</code> annotation in the SystemApplication class together with the <code>@Path</code> annotation in the resource class indicates the URL the resource is available at.</p> <p>JAX-RS maps the HTTP methods on the URL to the methods on the class. The method to call is determined by the annotations that are specified on the methods.</p> <p>The <code>@Produces</code> annotation indicates the format of the content that will be returned.</p>"},{"location":"java/#jax-rs-client-faq","title":"JAX-RS client FAQ","text":""},{"location":"java/#create-a-client","title":"Create a client","text":"<pre><code>import javax.ws.rs.client.Client;\nimport javax.ws.rs.client.ClientBuilder;\nimport javax.ws.rs.client.WebTarget;\nimport javax.ws.rs.core.Response;\n\nClient client = ClientBuilder.newClient();\n\nWebTarget target = client.target(url);\nResponse response = target.request().get();\nresponse.getStatus();\nString json = response.readEntity(String.class);\n// transform json string\nresponse.close();\n</code></pre>"},{"location":"java/#create-a-client-using-header","title":"Create a client using header","text":""},{"location":"java/#client-for-https-connection","title":"Client for HTTPs connection","text":""},{"location":"java/#reactor","title":"Reactor","text":"<p>Reactor is an implementation of the Reactive Programming paradigm, which can be summed up as: it is an asynchronous programming concerned with data streams and the propagation of change. It provides few additional aspects:</p> <ul> <li>Composability and readability</li> <li>Data as a flow manipulated with a rich vocabulary of operators</li> <li>Nothing happens until you subscribe</li> <li>Back pressure or the ability for the consumer to signal the producer that the rate of emission is too high</li> <li>High level but high value abstraction that is concurrency-agnostic</li> </ul> <p>See the simplest examples of Flux code in ReactorSimpleTest.java. Then tests with the StepVerifier. StepVerifier wraps source of stream and express expectations about the next signals to occur ReactorStepVerifier.java.</p> <p>See Project Reactor</p>"},{"location":"java/#kafka","title":"Kafka","text":"<p>See this dedicated studies for how to do end to end event driven microservice with kafka.</p> <p>See also the eda-quickstart project for tool and project template.</p>"},{"location":"java/#reactive-messaging","title":"Reactive messaging","text":"<p>See the extensive doc from SmallRye and my note quarkus</p>"},{"location":"java/#mq","title":"MQ","text":"<p>This is the implementation of the MQ developer tutorial and supports the Reseller class outlined in the red rectangle below: </p> <p>Code to subscribe to topic, and put, get from Queue. It includes a Docker image with MQ embedded more explanation here.</p>"},{"location":"java/#testcontainer-for-integration-test","title":"TestContainer for integration test","text":"<p>junit based Test containers and a simple quickstart.</p> <p>Projects where I used it: </p> <ul> <li>JMSMQClient</li> <li>Person manager with couchdb</li> </ul> <pre><code>@Container\n    public static GenericContainer container = new GenericContainer(\"couchdb\").withExposedPorts(5984)\n            .withEnv(\"COUCHDB_USER\", \"admin\").withEnv(\"COUCHDB_PASSWORD\", \"password\");\n    public static  PersonRepository repo = new PersonRepository();\n</code></pre>"},{"location":"java/#enterprise-inject","title":"Enterprise Inject","text":"<p><code>javax.enterprise.inject.Produces</code>: Identifies a producer method or field. CDI specification. A producer method acts as a source of objects to be injected, where:</p> <ul> <li>the objects to be injected are not required to be instances of beans, or</li> <li>the concrete type of the objects to be injected may vary at runtime, or</li> <li>the objects require some custom initialization that is not performed by the bean constructor.</li> </ul> <p>The container is responsible for processing all methods and fields marked with a @Produces annotation, and will normally do this when your application is deployed. The processed methods and fields will then be used as part of the injection point resolution for managed beans, as needed</p>"},{"location":"java/FAQ/","title":"FAQ","text":""},{"location":"java/FAQ/#list-current-jdk-installed","title":"List current JDK installed","text":"<pre><code>/usr/libexec/java_home -V\n</code></pre>"},{"location":"java/FAQ/#get-uuid","title":"Get UUID","text":"<pre><code>    UUID uuid = UUID.randomUUID();\n    order.setOrderID(uuid.toString());\n</code></pre>"},{"location":"java/FAQ/#java-reflection","title":"Java Reflection","text":"<p>The baeldung tutorial, with an example of getting attributes of a class. </p> <pre><code>Object person = new Person();\nField[] fields = person.getClass().getDeclaredFields();\n</code></pre> <ul> <li>Create a class instance: ClusterRepository is an interface and we can have different implementation as specified by an application properties</li> </ul> <pre><code> @ConfigParameter(\"cluster.repository.class\")\n String repositoryClassToUse;\n Class&lt;?&gt; clazz = Class.forName(repositoryClassToUse);\n</code></pre>"},{"location":"java/FAQ/#java-se","title":"Java SE","text":""},{"location":"java/FAQ/#read-from-a-properties-file","title":"Read from a properties file","text":"<pre><code>Properties props = new Properties();\n try {\n        props.load(  new FileInputStream(filename));\n } catch (FileNotFoundException e){\n         e.printStackTrace();\n  } catch (IOException e) {\n        e.printStackTrace();\n }\n</code></pre>"},{"location":"java/FAQ/#how-to-read-json-file","title":"How to read json file","text":"<p>Using google parser. For the maven dependency</p> <pre><code>FileReader input= new FileReader(\"src/test/resources/testpumps.json\");\nGson parser = new Gson();\nAssetEvent[] assets = parser.fromJson(new JsonReader(input), AssetEvent[].class);\n</code></pre> <p>Using jsonb (quarkus)</p> <pre><code>Book book = new Book(\"Java 11\", LocalDate.now(), 1, false, \"Duke\", new BigDecimal(44.444));\nJsonb jsonb = JsonbBuilder.create();\nString resultJson = jsonb.toJson(book);\nBook serializedBook = jsonb.fromJson(resultJson, Book.class);\n</code></pre> <p>Using jackson to serialize an object to a String</p> <pre><code>import com.fasterxml.jackson.databind.ObjectMapper;\nstatic ObjectMapper mapper = new ObjectMapper();\n try {\n    this.order = mapper.writeValueAsString(order);\n} catch (JsonProcessingException e) {\n    e.printStackTrace();\n    this.order = \"\";\n}\n</code></pre>"},{"location":"java/FAQ/#json-binding-jsr-367","title":"Json binding JSR 367","text":"<p>JSR 367 is the API for JSON processing, and implemented with JSON-B. The maven with quarkus is</p> <pre><code>    &lt;dependency&gt;\n      &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n      &lt;artifactId&gt;quarkus-resteasy-jsonb&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n</code></pre> <p>The main API features</p> <p>Example of sending a java object as json string with rest-assured:</p> <pre><code>Jsonb jsonb = JsonbBuilder.create();\n\ngiven()\n    .contentType(ContentType.JSON)\n    .body(jsonb.toJson(order)).post(url).then();\n</code></pre>"},{"location":"java/FAQ/#data-server-with-socket","title":"Data server with socket","text":"<p>The code read data from a csv file, listen socket connection, and once connected sends record line by line.  See the class as example in flink project.</p>"},{"location":"java/FAQ/#kafka-stream-and-json-b-serdes","title":"Kafka stream and json-b serdes","text":"<p>How to use a pojo as value in the stream? use the io.quarkus.kafka.client.serialization.JsonbSerde class.</p> <pre><code>import io.quarkus.kafka.client.serialization.JsonbSerde;\n\nJsonbSerde&lt;WeatherStation&gt; weatherStationSerde = new JsonbSerde&lt;&gt;(\n                WeatherStation.class);\nJsonbSerde&lt;Aggregation&gt; aggregationSerde = new JsonbSerde&lt;&gt;(Aggregation.class);\n\n// then use the serdes...\nGlobalKTable&lt;Integer, WeatherStation&gt; stations = builder.globalTable( \n                WEATHER_STATIONS_TOPIC,\n                Consumed.with(Serdes.Integer(), weatherStationSerde));\n</code></pre>"},{"location":"java/FAQ/#jaxrs","title":"JAXRS","text":""},{"location":"java/FAQ/#get-startup-and-destroy-event","title":"Get startup and destroy event","text":"<pre><code>@ApplicationScoped\npublic class StartupListener {\n\n    public void init(@Observes\n                     @Initialized(ApplicationScoped.class) ServletContext context) {\n        // Perform action during application's startup\n    }\n\n    public void destroy(@Observes\n                        @Destroyed(ApplicationScoped.class) ServletContext context) {\n        // Perform action during application's shutdown\n    }\n</code></pre>"},{"location":"java/FAQ/#unit-test-when-there-is-cdi","title":"Unit test when there is CDI","text":"<p>Add a constructor with the injectable bean as parameter:</p> <pre><code>    @Inject\n    private OrderRepository repository;\n\n    public OrderService(OrderRepository repository) {\n        this.repository = repository;\n    }\n</code></pre>"},{"location":"java/FAQ/#securing-communication-with-ssl","title":"Securing communication with SSL","text":"<p>To secure a service exposing end point with TLS. In Java a client can have a client key, it saves is a keystore, and then consume server certificate that needs to match a certificate saved in is truststore. </p> <p>Suppose the app expose a SSL end point, so it will have to define a server certificate, and then it will consume other service so it needs to import the other service end point to its truststore so during theSSL handshake, the connection can be estalished. </p>"},{"location":"java/FAQ/#good-articles","title":"Good articles","text":"<ul> <li>https://www.baeldung.com/java-ssl</li> <li>oracle doc</li> </ul>"},{"location":"java/FAQ/#concept-summary","title":"Concept summary","text":"<p>SSL is necessary to support the three main information security principles:</p> <ul> <li>Encryption: protect data transmissions between parties</li> <li>Authentication: ensure the server we connect to is indeed the proper server</li> <li>Data integrity: guarantee that the requested data is what is effectively delivered</li> </ul> <p>A self-signed certificate is one that you create for your server, in the server's KeyStore.</p> <p>The truststore is the file containing trusted certificates that Java uses to validate secured connections.</p> <p>add the public certificate of the server to the default cacerts truststore used by Java. while initiating the SSL connection. Set the javax.net.ssl.trustStore environment variable to point to the truststore file so that the application can pick up that file which contains the public certificate of the server we are connecting to. The steps to install a new certificate into the Java default truststore are:</p> <p>An error like unable to find valid certification path to requested target while establishing the SSL connection, it indicates that we don't have the public certificate of the server which we're trying to connect in the Java truststore.</p>"},{"location":"java/FAQ/#summary-of-what-needs-to-be-done","title":"Summary of what needs to be done","text":"<ul> <li> <p>create a self signed certificate to be the server of your end point</p> </li> <li> <p>Import the ca certificate of the server you want to connect to.</p> <ul> <li>Extract cert from server: <code>openssl s_client -connect server:443</code></li> <li>Import certificate into truststore using keytool: <code>keytool -import -alias alias.server.com -keystore $JAVA_HOME/jre/lib/security/cacerts</code> or to a truststore file you can use in a dockerfile, or upload as a truststore and then mount to a pod.</li> </ul> </li> </ul>"},{"location":"java/concurrent/","title":"Concurrency","text":""},{"location":"java/concurrent/#executorservice","title":"ExecutorService","text":"<p>The Java ExecutorService interface, java.util.concurrent.ExecutorService, represents an asynchronous execution mechanism which is capable of executing tasks concurrently in the background. Once the thread has delegated the task to the ExecutorService, the thread continues its own execution independent of the execution of that task. The ExecutorService then executes the task concurrently, independently of the thread that submitted the task.</p> <p>The api offers a way to specify the pool size, this is to avoid loading the app server common thread pool.</p> <pre><code>ExecutorService executor = Executors.newFixedThreadPool(2);\norderEventRunner = new OrderEventRunner();\nexecutor.execute(orderEventRunner);\n</code></pre> <p><code>Execute</code> method execute a Runnable asynchronously. The Java ExecutorService <code>submit(Runnable)</code> method also takes a Runnable implementation, but returns a <code>Future</code> object. This Future object can be used to check if the Runnable has finished executing. The second flavor for submit is using a Callable as argument. A callable is the same as a Runnable, excepts it returns an object and can generate exception.</p> <pre><code>public interface Callable{\n    public Object call() throws Exception;\n}\n</code></pre> <pre><code>Future future = executorService.submit(new Callable(){\n    public Object call() throws Exception {\n        System.out.println(\"Asynchronous Callable\");\n        return \"Callable Result\";\n    }\n});\n\nSystem.out.println(\"future.get() = \" + future.get());\n</code></pre>"},{"location":"java/funqy/","title":"Funqy","text":"<p>Quarkus Funqy is part of Quarkus\u2019s serverless strategy and aims to provide a portable Java API to write functions deployable to various FaaS environments like AWS Lambda, Azure Functions, Knative serving, and Knative Eventing (Cloud Events)</p> <p>See basic quarkus guide</p> <p>See FUNQY KNATIVE EVENTS BINDING. </p>"},{"location":"java/java-vscode/","title":"Java programming with VScode","text":""},{"location":"java/java-vscode/#some-editor-tricks","title":"Some Editor tricks","text":"<p>The access to the command palette shift -&gt; cmd -&gt; P</p> <p>palette and access to preview a file like a csv.</p>"},{"location":"java/java-vscode/#autocompletion","title":"Autocompletion","text":"<ul> <li>main to get public void main.... </li> <li>sysout for System.out...</li> <li>F2 to refactor name</li> </ul>"},{"location":"java/java-vscode/#navigate","title":"Navigate","text":"<ul> <li>cmd -&gt; click to navigate to the source of a class</li> <li>opt -&gt; Shift -&gt; o to organize import</li> <li>cmd -&gt; shift -&gt; O get the code outline</li> <li>cmd -&gt;  P navigate project file</li> </ul>"},{"location":"java/java-vscode/#run-a-java-main","title":"Run a java main","text":"<p>Create a configuration with the name of the class and arguments. See example here.. Something like:</p> <pre><code> \"configurations\": [\n        {\n            \"type\": \"java\",\n            \"name\": \"Debug (Launch)\",\n            \"request\": \"launch\",\n            \"mainClass\": \"jbcodeforce.p1.WordCountMain\",\n            \"args\": \"wc.txt\"\n        }\n ]\n</code></pre>"},{"location":"java/java-vscode/#vscode-tricks","title":"VSCode tricks","text":"<ul> <li>Control the file exposed (like removing eclipse related file) -&gt; code -&gt; Preferences -&gt; Settings -&gt; your-existing-projectname -&gt; Commonly used, file exclude and then enter a pattern like (<code>**/.classpath</code>)</li> <li>Java Dependencies view helps to get the code of the used dependencies</li> <li>Run maven goals from the maven projects view in the Explorer</li> <li>Avoid file closed when not edited: double clicks on the file header so it changes from italic to regular.</li> <li>Control the number of columns in editor: <code>Settings &gt; Editor: Word Wrap &gt; wordWrapColumn, Settings &gt; Editor: Word Wrap Column = 100</code></li> </ul>"},{"location":"java/java-vscode/#debugging","title":"Debugging","text":"<p>Codelens debugging</p>"},{"location":"java/java-vscode/#set-environment-variables","title":"Set environment variables","text":"<p>select Debug &gt; Open Configurations then you should see a set of launch configurations for debugging your code. You can then add to it an env property with a dictionary of string:string.</p> <pre><code>            \"env\": {\n                \"BOOTSTRAP_SERVERS\": \"localhost:9092\",\n                \"TOPIC_NAME\": \"orders\"\n            }\n</code></pre>"},{"location":"java/java-vscode/#error","title":"Error","text":"<ul> <li><code>Failed to launch debugger in terminal. Reason: Failed to launch debuggee in terminal. Reason: java.util.concurrent.TimeoutException: timeout</code>: this was done by unknown localhost resolution due to some DNS setting on mac.</li> </ul>"},{"location":"java/java-vscode/#quarkus-support","title":"Quarkus support","text":"<p>Create a new projet: shift -&gt; cmd -&gt; P:  <code>Quarkus: Generate a Maven project</code> Start debugger:  shift -&gt; cmd -&gt; P: <code>Quarkus:  Debug current Quarkus Project</code> to create a configuration.</p> <p>Add code for a resource: Add a java file and use <code>qrc</code> in editor.</p> <p>To add an extension to current project: shift -&gt; cmd -&gt; P: <code>Quarkus:add an extension to current project</code>.</p> <p>In the outline view, we can get a tree view for application.properties</p>"},{"location":"java/jpa-hibernate/","title":"Hibernate ORM for JPA","text":"<p>Capabilities:</p> <ul> <li>object-relational mapping</li> <li>manage entities life cycle</li> <li>uses persistence context as a \"cache\" for entities read or saved to a database</li> <li>session is a logical transaction, defined at the method level with @Transaction or using entity Manager</li> <li>Entities are in one of <code>transient, persistent, detached</code> states</li> </ul> <p>Practices:</p> <ul> <li>always have a single instance of entity for every database record during the session</li> </ul>"},{"location":"java/jpa-hibernate/#dev-solution","title":"Dev solution","text":""},{"location":"java/jpa-hibernate/#specify-column-property","title":"Specify column property","text":"<pre><code>@Column(length = 255)\npublic String name;\n</code></pre>"},{"location":"java/jpa-hibernate/#error-solution","title":"Error - solution","text":"<p>org.hibernate.PersistentObjectException: detached entity passed to persist</p>"},{"location":"java/microprofile/","title":"Microprofile 3.x notes","text":"<p>microprofile is the java framework to develop microservice architecture. MicroProfile APIs establish an optimal foundation for developing microservices-based applications.</p> <p>This openliberty website includes very good interactive tutorials to learn more on microprofile.</p> <p>Two way to implement the communication between services:</p> <ul> <li>Choreography defines business workflow as a sequence of domain events and service actions. Each involved service signals a successful or faulty fulfilment of its part of the workflow via a corresponding domain event. Other services may listen to this domain event and start working on their part of the workflow after receiving it.</li> <li>Orchestration  defines one orchestrating service which sends commands to the different services and receives their responses. This could be done synchronously via RESTful calls or asynchronously with the help of queues or topics.</li> </ul>"},{"location":"java/microprofile/#create-a-microprofile-app","title":"Create a microprofile app","text":"<p>This section summarizes some getting started stuff on microprofile.</p>"},{"location":"java/microprofile/#with-maven-liberty","title":"With maven Liberty","text":"<p>Use the Generate tool at start.microprofile.io.</p> <p>Run with <code>mvn liberty:dev</code> to listen to file changes.</p>"},{"location":"java/microprofile/#run-with-docker","title":"Run with docker","text":"<p>Once the application is created and built, we can also define a docker file and run it. From this openliberty guides here is a quick summary:</p> <ul> <li>The open liberty maven plugin add an execution to create the server, install features, and deploy the app as part of the <code>mvn package</code> command.</li> <li>docker build and then docker run it by mounting the war file:</li> </ul> <pre><code>docker run -ti --network kafkanet -p 9081:9081 -v $(pwd)/target/PerfConsumerApp.war:/config/dropins/PerfConsumerApp.war ibmcase/perfconsumerapp \n</code></pre>"},{"location":"java/microprofile/#cross-cutting-concerns","title":"Cross cutting concerns","text":"<p>Cross-cutting concerns are common to all microservice and include logging, monitoring of service health and metrics, fault tolerance, configuration, and security.</p> <ul> <li>Support distributed logging and tracing cross microservices, that can be merged and analyzed as a whole.</li> <li>Without a single point of control, each microservice needs to know if it is healthy and perform as expected.</li> <li>Microservices architecture needs to be resilient and fault tolerant by design. This means we must not only be able to detect but also to handle any issue automatically.</li> <li>Microservice accesses configurations from multiple sources in a homogeneous and transparent way</li> <li>Support a mechanism to handle distributed authentication and authorization</li> <li>Offer security context propagation</li> <li>Make sure that the original service call is not forged in the authentication process.</li> </ul>"},{"location":"java/microprofile/#programming-model","title":"Programming model","text":"<p>MicroProfile utilizes a very small subset of Java EE APIs to define its core programming model:</p> <ul> <li>CDI 2.0 for Dependency Injection</li> <li>JAX-RS 2.1 for REST APIs</li> <li>JSON P 1.1 for JSON Parsing</li> <li>JSON-B 1.0 for JSON Binding</li> </ul> <p> As illustrated in the figure above Microprofile 3.0 bundles a set of features:</p>"},{"location":"java/microprofile/#config","title":"Config","text":"<p>Config: externalizes configuration and obtains config via injection from config files, environment variables, system properties, or custom resource. Those configurations are static, they cannot be modified while the server is running. MicroProfile Config uses Contexts and Dependency Injection (CDI) to inject configuration property values directly into an application without requiring user code to retrieve them.</p> <pre><code>```java\n// inject property with default value\n@Inject @ConfigProperty(name = \"topicName\")\nString topicName;\n```\n</code></pre> <p>MicroProfile Config combines configuration properties from multiple sources, each known as a ConfigSource. The config file is named <code>META-INF/microprofile-config.properties</code></p> <p>The <code>@Inject</code> annotation injects the topic name directly, the injection value is static and fixed on application starting.</p> <p>It is possible to have dynamic configuration by implementing the <code>org.eclipse.microprofile.config.spi.ConfigSource</code> interface and using the java.util.ServiceLoader mechanism with configuration file in <code>META-INF/services/org.eclipse.microprofile.config.spi.ConfigSource</code> to specifies the class name to use for loading configuration. The property to load dynamically has a Provider type:</p> <pre><code>  @Inject\n  @ConfigProperty(name = \"io_openliberty_guides_inventory_inMaintenance\")\n  private Provider&lt;Boolean&gt; inMaintenance;\n</code></pre> <p>It forces the service to retrieve the inMaintenance value just in time. This retrieval of the value just in time makes the config injection dynamic and able to change without having to restart the application.</p> <p>To enable this capability, we need <code>mpConfig-1.3</code> or <code>microprofile-3.0</code> feature.</p> <p>See configuration for microprofile git repo. And this tutorial.</p>"},{"location":"java/microprofile/#cdi","title":"CDI","text":"<p>CDI Contexts and Dependency Injection (CDI) to manage scopes and inject dependencies</p>"},{"location":"java/microprofile/#fault-tolerance","title":"Fault tolerance","text":"<p>Fault Tolerance enables us to build resilient microservices by separating the execution logic from business logic. Key aspects of the Fault Tolerance API includes well known resilience patterns like TimeOut, RetryPolicy, Fallback, Bulkhead (isolate failure), and Circuit Breaker (fail fast) processing.</p> <pre><code>```java\n@GET\n@Timeout(500)\n@Retry(delay = 200, maxRetries = 2, jitter = 100, retryOn = IOException.class)\n@Fallback(fallbackMethod = \u201cgetBestsellersFallback\u201d)\npublic Response getPersonalRecommendations() throws InterruptedException {\n    // retrieve personal recommendations by delegating the call to\n    // Personal Recommendations Microservice\n}\npublic Response getBestsellersFallback() {\n    // retrieve bestsellers instead of personal recommendations as fallback\n    // by delegating the call to Bestsellers Service\n}\n```\n\nThe `@Timeout` annotation specifies the time in milliseconds allowed for the request to finish. This is to avoid the user interface to wait forever.\nThe `@Retry` helps to recover from network or remote microservice transient failures. The retryOn specifies the exception to trigger the retry. `delay` specifies the amount of time to wait before retrying a request. `jitter` specifies a variation to apply to the delay interval between retries.\n</code></pre>"},{"location":"java/microprofile/#health-check","title":"Health Check","text":"<ul> <li> <p>Health Check lets developers define and expose a domain-specific microservices health status (\u201cUP\u201d or \u201cDOWN\u201d) so unhealthy services can be restarted by the underlying environment. Health checks are used to determine both the liveness and readiness of a service. Determining the state of a service can be composed by a set of verification procedures. Multiple domain-specific health checks can easily be added to a microservice by implementing the corresponding HealthCheck interface.</p> <pre><code>@Readiness\n@ApplicationScoped\npublic class ConnectionPoolCheck implements HealthCheck {\n    @Override\n    public HealthCheckResponse call() {\n        if (isConnectionPoolHealthy()) {\n        return HealthCheckResponse(\u201ccustomer-cp\u201d)\n        .up()\n        .build();\n        } else {\n        return HealthCheckResponse(\u201ccustomer-cp\u201d)\n        .down()\n        .build();\n        }\n    }\n</code></pre> </li> </ul> <p>The <code>@Liveness</code> annotation is used to assess if the microservice is running.</p> <p>The <code>@Readiness</code> annotation indicates that this particular bean is a readiness health check procedure. By pairing this annotation with the ApplicationScoped context from the Contexts and Dependency Injections API, the bean is discovered automatically when the <code>http://&lt;&gt;/health</code> endpoint receives a request</p> <ul> <li> <p>Metrics delivers details about the microservices runtime behavior by providing a unified way for MicroProfile servers to export monitoring data to management agents. Metrics also provides a common Java API for exposing their telemetry data. Metrics serves to pinpoint issues, providing long-term trend data for capacity planning, and pro-active discovery of issues (e.g. disk usage growing without bounds) via a simple to use RESTful API. Metrics can also help scheduling-systems decide when to scale the application to run on more or fewer machines.</p> <pre><code>@POST\n@Produces(MediaType.APPLICATION _ JSON)\n@Timed(absolute = true,\n    name = \"microprofile.ecommerce.checkout\",\n    displayName = \"check-out time\",\n    description = \"time of check-out process in ns\",\n    unit = MetricUnits.NANOSECONDS)\npublic Response checkOut(...) {\n    // do some check-out specific business logic\n    return Response.ok()\u2026 build();\n}\n</code></pre> </li> </ul> <p>The <code>@Timed</code> annotation tracks how frequently the method is invoked and how long it takes for each invocation of the method to complete. The <code>@Counted</code> annotation to a method to count how many times the matching URL is accessed monotonically, which is counting up sequentially. The <code>@Gauge</code> annotation to a method helps to track the number of items that are in the returned collection.</p> <p>In OpenLiberty the quickStartSecurity and keyStore configuration elements provide basic security to secure the server. When you visit the /metrics endpoint, use the credentials defined in the server configuration to log in and view the data.</p> <ul> <li> <p>Open API specification provides a set of Java interfaces and programming models that allow developers to natively produce OpenAPI v3 documents. Use JAX-RS anotations with OpenAPI annotations or start from a yml file.</p> </li> <li> <p>Rest Client simplifies building REST Clients by providing a type-safe approach for invoking RESTful services over HTTP. It uses JXRS 2.1.</p> </li> <li> <p>JWT Authentication defines a format of JSON Web Token (JWT) used as the basis for interoperable authentication and authorization by providing role-based access control (RBAC) microservice endpoints using OpenID Connect. One of the main strategies to propagate the security state from clients to services, or even from services to services, involves the use of security tokens.</p> </li> <li> <p>OpenTracing enables services to easily participate in a distributed tracing environment by defining an API. To accomplish distributed tracing, each service must be instrumented to log messages with a correlation id that may have been propagated from an upstream service.</p> </li> </ul>"},{"location":"java/microprofile/#sandbox-projects","title":"Sandbox projects","text":"<ul> <li>The proposal for Long Running Actions introduces APIs for services to coordinate activities. The main thrust of the proposal introduces an API for loosely coupled services to coordinate long running activities in such a way as to guarantee a globally consistent outcome without the need to take locks on data.</li> <li>The Reactive Streams Operators specification propose a set of operators for Reactive Streams. By mapping Java Streams API but for Reactive Streams, it provide a natural API to deal with stream of data, enforcing error propagation, completion signals, and back-pressure.</li> <li>The Reactive Messaging proposal explores the question \u201ewhat if Java offered a new API for handling streams of messages - either point to point or from a message broker - based on the JDK 9 Flow API or alternatively on the JDK8 compatible Reactive Streams API - that was lighter weight and easier to use than JMS/MDBs\u201c</li> </ul>"},{"location":"java/microprofile/#getting-started","title":"Getting started","text":"<p>MicroProfile Starter helps to generate microprofile maven project with sample code. The folders microprofile30/service-a and service-b have the generated code. From this code the app under JEEPlay jbcodeforce.microprofile.app defines a basic template for a REST based microservice. What need to be done is:</p> <ul> <li>JAX-RS has two key concepts for creating REST APIs. The resource itself, which is modelled as a class, and a JAX-RS application, which groups all exposed resources under a common path. See the class BasicRestApp.</li> <li>The @ApplicationPath annotation has a value that indicates the path within the WAR that the JAX-RS application accepts requests from.</li> <li>Clear have one Resource class for the same resource type. The @Path annotation on the class indicates that this resource responds to specified path of the JAX-rS application. JAX-RS maps the HTTP methods on the URL to the methods on the class. SystemPropertiesResource</li> <li>The JAX-RS 2.1 specification mandates JSON-Binding (JSON-B) and JAX-B.</li> <li>Example of results : http://localhost:9080/JEEPlay/System/properties</li> </ul>"},{"location":"java/microprofile/#concepts","title":"Concepts","text":""},{"location":"java/microprofile/#cdi_1","title":"CDI","text":"<p>Use Contexts and Dependency Injection (CDI) to manage scopes and inject dependencies into microservices. The most fundamental services that are provided by CDI are contexts that bind the lifecycle of stateful components to well-defined contexts, and dependency injection that is the ability to inject components into an application in a typesafe way. With CDI, the container does all the daunting work of instantiating dependencies, and controlling exactly when and how these components are instantiated and destroyed.</p> <p>Scopes are defined by using CDI annotations.</p> <p>When a bean needs to be persistent between all of the clients (singleton), use the <code>@ApplicationScoped</code> annotation. This annotation indicates that this particular bean is to be initialized once per application. By making it application-scoped, the container ensures that the same instance of the bean is used whenever it is injected into the application.</p> <p>Add the <code>@RequestScoped</code> annotation on the class to indicate that this bean is to be initialized once for every request. Request scope is short-lived and is therefore ideal for HTTP requests.</p> <p>The <code>@Inject</code> annotation indicates a dependency injection for application concept beans.</p> <p>Important to note that access to the injected bean could not be done in the contructor of the class using this injected bean. There is a problem of life cycle and so the host bean needs to be constructed before getting the injection.</p>"},{"location":"java/microprofile/#ssl-client-connection","title":"SSL client connection","text":"<p>When an application deployed in your app server, like Open Liberty, needs to access to a server end point over TLS (SSL), we need to make the server public certificate available to the Java client JVM. The certificate is persisted in a Truststore.</p> <p>In open liberty the server.xml define where to find the truststore:</p>"},{"location":"java/mqChallenge/","title":"MQ Challenges","text":""},{"location":"java/mqChallenge/#getting-started-with-mq-docker-image","title":"Getting started with MQ docker image","text":"<p>See the docker compose file in the Java Studies mqChallenges folder. Once the mqserver is running, open a bash:  <code>docker exec -ti mqserver bash</code></p> <p>See this last mq connect app to qm in container article</p> <ul> <li>Display the MQ server information by exec in the container and use command <code>dspmqver</code>.</li> </ul> <pre><code>bash-4.4$ dspmqver\nName:        IBM MQ\nVersion:     9.1.5.0\nLevel:       p915-ifix-L200325.DE\nBuildType:   IKAP - (Production)\nPlatform:    IBM MQ for Linux (x86-64 platform)\nMode:        64-bit\nO/S:         Linux 4.19.76-linuxkit\nO/S Details: Red Hat Enterprise Linux 8.1 (Ootpa)\nInstName:    Installation1\nInstDesc:    IBM MQ V9.1.5.0 (Unzipped)\nPrimary:     N/A\nInstPath:    /opt/mqm\nDataPath:    /mnt/mqm/data\nMaxCmdLevel: 915\nLicenseType: Developer\n</code></pre> <ul> <li>Display your running queue managers: <code>dspmq</code></li> </ul> <pre><code>QMNAME(QM1)                                               STATUS(Running)\n</code></pre> <p>The default configuration is:</p> <pre><code>Queue manager QM1\nQueue DEV.QUEUE.1\nChannel DEV.APP.SVRCONN\nListener DEV.LISTENER.TCP on port 1414\n</code></pre> <p>The queue that you will be using, DEV.QUEUE.1, \u201clives\u201d on the queue manager QM1. The queue manager also has a listener that  listens for incoming connections, for example, on port 1414. Client applications can connect to the queue manager  and can open, put, and get messages, and close the queue.</p> <p>Applications use an MQ channel to connect to the queue manager. Access to these three objects is restricted in different ways.  For example, user \u201capp\u201d, who is a member of the group \u201cmqclient\u201d is permitted to use the channel DEV.APP.SVRCONN to connect  to the queue manager QM1 and is authorized to put and get messages to and from the queue DEV.QUEUE.1.</p>"},{"location":"java/mqChallenge/#demo-app","title":"Demo app","text":"<p>Client applications connect to the queue manager and can open, put, and get messages, and close the queue. Applications use an MQ channel to connect to the queue manager. The following demo  opens a queue, puts messages to it and then gets messages from the same queue. The app demonstrates point-to-point style of messaging.</p> <pre><code>curl https://raw.githubusercontent.com/ibm-messaging/mq-dev-samples/master/gettingStarted/mq-demo-client-application/Dockerfile\n# Build\ndocker build -t mq-demo .\n# Run\ndocker run -ti mq-demo\n</code></pre> <p>Then answer the questions, using the name of <code>mqserver</code> for the hostname, as it is the name defined in the docker compose, and the password is the one defined in <code>MQ_APP_PASSWORD</code>.</p> <pre><code>|  \\/  |/ _ \\  |   \\ ___ _ __  ___\n| |\\/| | (_) | | |) / -_) '  \\/ _ \\\n|_|  |_|\\__\\_\\ |___/\\___|_|_|_\\___/\n\nWelcome to this demo application for IBM MQ!\nThe app will allow you to connect to a queue manager and start sending and receiving messages.\n\nType the name of your queue manager (leave blank for 'QM1'):\nType the host name or IP address for your queue manager:\nmqserver\nType the listener port of your queue manager (leave blank for '1414'):\nType the name of the client connection channel (leave blank for 'DEV.APP.SVRCONN'):\nType the name of the queue (leave blank for 'DEV.QUEUE.1'):\nType the application user name (leave blank for 'app'):\n\nConnecting to queue manager 'QM1'\nat address 'mqserver(1414)'\nthrough channel 'DEV.APP.SVRCONN'\n...\nConnected!\n\n*** YOU ARE NOW CONNECTED TO THE QUEUE MANAGER! ***\n\nType PUT [number] to put a given number of messages to the queue\nType GET [number] to get a given number of messages from the queue\nType AUTO [seconds] to put and get messages automatically for a given number of seconds\nType EXIT to quit\nType HELP to display this message\n</code></pre> <p>Play with PUT and GET.</p>"},{"location":"java/mqChallenge/#access-to-mq-console","title":"Access to MQ Console","text":"<p><code>https://localhost:9443/ibmmq/console</code>  admin / passw0rd</p> <p></p>"},{"location":"java/mqChallenge/#mq-concepts","title":"MQ concepts","text":"<p>See my notes</p> <p>Configure MQ Qmgr with config map. See this real time inventory gitops repo</p> <p>Example of config map</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mq-mqsc-config\ndata:\n  example.mqsc: |\n    DEFINE QLOCAL('ITEMS') REPLACE\n    DEFINE CHANNEL('DEV.ADMIN.SVRCONN') CHLTYPE(SVRCONN) REPLACE\n    DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE\n    ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE')\n    DEFINE CHANNEL(DEV.APP.SVRCONN) CHLTYPE(SVRCONN) \n    ALTER QMGR CHLAUTH (DISABLED)\n    REFRESH SECURITY TYPE(CONNAUTH)\n</code></pre> Tutorials <ul> <li>Developer.ibm</li> </ul>"},{"location":"java/mqChallenge/#basic-jms-20-client","title":"Basic JMS 2.0 client","text":"<p>The code under JMSMQclient is an adaption of the develop MQ client tutorial. The differences are:</p> <ul> <li>use quarkus to package as webapp.</li> <li>Offer API to start the sending of n messages.</li> <li>Offer API to get the consumed message.</li> <li>Inject configuration from properties file.</li> <li>Expose API via swagger.</li> </ul>"},{"location":"java/mqChallenge/#send-text-message-and-use-correlationid-to-be-used-as-key","title":"Send Text message and use correlationID to be used as key","text":"<p>See the code in the store simulator MQItemGenerator class:</p> <pre><code>private void sendToMQ(Item item) {\n      try { \n        String msg = parser.toJson(item);\n        TextMessage message = jmsContext.createTextMessage(msg);\n        message.setJMSCorrelationID(item.storeName);\n        producer.send(destination, message);\n        logger.info(\"sent to MQ:\" + msg);\n      } catch( Exception e) {\n        e.printStackTrace();\n      }   \n}\n</code></pre> <p>And the corresponding kafka connector configuration in mq-source.properties:</p> <pre><code> mq.connection.mode=client\n key.converter=org.apache.kafka.connect.storage.StringConverter\n mq.message.body.jms=true\n value.converter=org.apache.kafka.connect.storage.StringConverter\n mq.record.builder=com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder\n mq.record.builder.key.header=JMSCorrelationID\n</code></pre>"},{"location":"java/mqChallenge/#publisher-subscribe-on-mq","title":"Publisher / Subscribe on MQ","text":"<p>This is the implementation of the MQ developer tutorial and supports the implementation of the following solution:</p> <p></p> <p>The conference event system and reseller applications are loosely coupled. Asynchronous messaging allows us to integrate these components and build in a buffer, or shock absorber. Should either component lose connectivity, fail or experience fluctuations in throughput, the messaging layer will deal with any instability.</p>"},{"location":"java/mqChallenge/#the-event-booking","title":"The Event Booking","text":"<p>This is the implementation of the MQ developer tutorial and supports the implementation of the following solution:</p> <p></p> <p>The conference event system and reseller applications are loosely coupled. Asynchronous messaging allows us to integrate  these components and build in a buffer, or shock absorber. Should either component lose connectivity, fail or experience   fluctuations in throughput, the messaging layer will deal with any instability.</p> <p>The event booking system runs in a container and holds an event publisher, MQ server hosting newTickets topic and purchase  and confirmation queues. The code is in <code>java-studies/mqChallenges/MQTicketService</code> folder.</p> <p><code>newTicket</code> messages are sent every 30 s to the <code>newTickets</code> topic. The message payload contains an EventId and the number of available tickets. The Reseller application uses the received payload to construct a request message to buy some tickets.</p> <p>The application uses JMS to send to queue or topic.</p> <p>See instructions in the [readme] to build and run the solution locally with docker.</p> <p>For any app we need the JMS api jar and the MQ all client code. Here are the two curl to use</p> <pre><code>curl -o com.ibm.mq.allclient-9.2.0.0.jar https://repo1.maven.org/maven2/com/ibm/mq/com.ibm.mq.allclient/9.2.0.0/com.ibm.mq.allclient-9.2.0.0.jar\ncurl -o javax.jms-api-2.0.1.jar https://repo1.maven.org/maven2/javax/jms/javax.jms-api/2.0.1/javax.jms-api-2.0.1.jar\n</code></pre>"},{"location":"java/mqChallenge/#the-reseller-app","title":"The reseller app","text":"<p>The Reseller application provides the prompt to ask the user how many of the available tickets they want to purchase. The conference event booking system will process the request message and provide a response.  The Reseller application will print the outcome to stdout.</p> <p>Build with the shell <code>./TicketSeller/build.sh</code> (be sure to use jdk 1.8) and run it with:</p> <pre><code># use jdk 1.8\nsdk use java 8.0.292.j9-adpt\n# build\n./build.sh\n# run it\n./runReseller.sh\n# And buy ticket as they are published\n</code></pre>"},{"location":"java/openliberty/","title":"OpenLiberty notes","text":"<p>OpenLiberty.io</p>"},{"location":"java/openliberty/#create-openliberty-webapp","title":"Create OpenLiberty webapp","text":""},{"location":"java/openliberty/#from-starting-repo","title":"From starting repo","text":"<pre><code>git clone https://github.com/openliberty/guide-getting-started.git\n# or\ngit clone https://github.com/OpenLiberty/sample-getting-started.git\ncd sample-getting-started\nmvn liberty:dev\n</code></pre>"},{"location":"java/openliberty/#maven-commands","title":"maven commands","text":"<pre><code># dev mode: The Open Liberty server automatically reloads the configuration without restarting\nmvn liberty:dev\n# dev mode with docker\n#builds a Docker image, mounts the required directories, binds the required ports, and then runs the application inside of a container\nmvn liberty:devc \n# or control the start, stop...\nmvn liberty:run\nmvn liberty:start\nmvn liberty:stop\n</code></pre> <p>These messages are also logged to the <code>target/liberty/wlp/usr/servers/defaultServer/logs/console.log</code>. You can find the error logs in the ffdc directory and the tracing logs in the trace.log file.</p> <p>To set properties, they can be set in the server.xml</p> <pre><code>  &lt;variable name=\"app.inMaintenance\" value=\"false\"/&gt;\n</code></pre>"},{"location":"java/openliberty/#build-test-package","title":"Build . test . package","text":"<pre><code>mvn package\n# will create a war file with the &lt;artifactId&gt; name\n</code></pre> <p>Instead of creating a server package, you can generate a runnable JAR file that contains the application along with a server runtime. As a result, the generated JAR file is only about 50 MB.</p> <pre><code>mvn liberty:package -Dinclude=runnable\n# execute\njava -jar yourapp.jar\n</code></pre>"},{"location":"java/openliberty/#features","title":"Features","text":"<ul> <li>mpHealth-4.0\" health Check and Readiness classes</li> </ul>"},{"location":"java/openliberty/#cdi","title":"CDI","text":"<p>When bean must be persistent between all of the clients, which means multiple clients need to share the same instance. Simply add the <code>@ApplicationScoped</code> annotation onto the class.</p> <p>With the <code>@RequestScoped</code> annotation, the bean is instantiated when the request is received and destroyed when a response is sent back to the client. A request scope is short-lived.</p> <p>The <code>@Inject</code> annotation indicates a dependency injection</p>"},{"location":"java/openliberty/#openliberty-operator","title":"OpenLiberty Operator","text":"<p>OpenLiberty Operator helps to deploy and manage applications running on Open Liberty into kubernetes or OpenShift clusters.</p> <p>To build a openliberty app docker image follow guidelines here.</p> <p>Install it using the Operator Hub. Once installed the following commands </p>"},{"location":"java/openliberty/#open-liberty-links","title":"Open liberty links","text":"<ul> <li>Open Liberty developer tool for eclipse</li> <li>Super guides</li> <li>Understanding the liberty maven plugin</li> </ul>"},{"location":"java/openliberty/#debug-openliberty-app","title":"Debug Openliberty app","text":"<p>Start <code>mvn liberty:debug</code>, the console should display the port number, it listens to (7777). Then in Eclipse define a debud configuration for a <code>remote java application</code>, use localhost and the matching port number. Any breakpoint in the code should be reachable.</p> <p>For VS code:</p>"},{"location":"java/quarkus/","title":"Quarkus Summary and trick","text":"<p>Best source of knowledge is reading the guides and the workshop.</p> <p>Info</p> <p>Updated 08/23/2023</p>"},{"location":"java/quarkus/#value-propositions","title":"Value Propositions","text":"<ul> <li>Designed to run java for microservice in container and OpenShift: reduce start time to microseconds, and reduce memory footprint.</li> <li>Run native in linux based image, so most of the java processing is done at build time.</li> <li>Extensible components, very easy to add dependencies and hide plumbing code</li> <li>Easy to develop integration tests at API level</li> <li>Quarkus implements reactive programming with Vert.x</li> <li> <p>50% reduction in resources demand</p> </li> <li>No need to retrain Java developer to nodejs</li> <li>Hot reload made developer flow faster: 20 microservices built per week.</li> </ul> <p>Quarkus HTTP support is based on a non-blocking and reactive engine (Eclipse Vert.x and Netty). All the HTTP requests your application receives, are handled by event loops (IO Thread) and then are routed towards the code that manages the request.</p>"},{"location":"java/quarkus/#getting-started","title":"Getting Started","text":""},{"location":"java/quarkus/#quarkus-cli","title":"Quarkus CLI","text":"<p>Quarkus CLI lets you create projects, manage  extensions and do essential build and dev commands using the underlying project\u2019s build  tool. It replaces the maven plugin. The CLI does not work on Java 1.8 so use <code>sdk</code> to change the SDK version.</p> <p>Always use <code>quarkus --help</code> and <code>quarkus --version</code> to get the last updated CLI.</p> <p>Here are some common commands:</p> <pre><code># Create a project with groupId=com.foo, artifactId=bar, and version=1.0.0-SNAPSHOT\nquarkus create app com.foo:bar\n\n# List extension\nquarkus ext ls\n\n# Add extensions\nquarkus ext add openshift qpid-jms resteasy-reactive smallrye-openapi smallrye-health quarkus-resteasy-reactive-jackson\n# Build\nquarkus build\n\nquarkus dev\n</code></pre> <p>If the project could not be build because of missing maven wrapper use the following command:</p> <pre><code>mvn -N io.takari:maven:wrapper\n</code></pre>"},{"location":"java/quarkus/#create-a-project","title":"Create a project","text":"<pre><code># new way with cli\nquarkus create app  -x openapi,metrics,openshift,resteasy-reactive,resteasy-reactive-jackson ibm.gtm.dba:app-name:1.0.0\n# old way\nmvn io.quarkus:quarkus-maven-plugin:2.2.3.Final:create \\\n    -DprojectGroupId=ibm.gse.eda \\\n    -DprojectArtifactId=app-name \\\n    -Dpath=\"/greeting\"\ncd app-name\n</code></pre>"},{"location":"java/quarkus/#package-run","title":"Package &amp; run","text":"<p>Run with automatic compilation </p> <pre><code>quarkus dev\n# build and  package\nquarkus build\n# older way\n./mvnw compile quarkus:dev\n# packaging\n./mvnw clean package\n# or for native\n./mvnw clean package -Pnative\n#  build with a docker build image\n./mvnw package -Pnative -Dquarkus.native.container-build=true  -Dquarkus.container-image.build=true\n</code></pre> <p>For native build see QUARKUS - TIPS FOR WRITING NATIVE APPLICATIONS</p> <p>Start and override properties at runtime:</p> <pre><code>java -Dquarkus.datasource.password=youshallnotpass -jar target/myapp-runner.jar\n</code></pre> <p>for a native executable: </p> <pre><code>./target/myapp-runner -Dquarkus.datasource.password=youshallnotpass\n</code></pre> <p>See the Building a native executable guide to develop with graalvm:</p> <ul> <li>Use Mandrel for java 11+ app, for linux OS</li> <li>Use Graalvm community edition for development JDK 1.8 Apps.</li> <li>Install native image with: <code>${GRAALVM_HOME}/bin/gu install native-image</code></li> <li>To generate debug symbols, add <code>-Dquarkus.native.debug.enabled=true</code> flag when generating the native executable.</li> <li>Run the tests against a native executable that has already been built: <code>./mvnw test-compile failsafe:integration-test</code> </li> <li>Build and deploy on OpenShift: add OpenShift plugin and do these steps.</li> </ul> <p>Can also use environment variables: Environment variables names are following the conversion rules of Eclipse MicroProfile.</p>"},{"location":"java/quarkus/#debug-within-vscode","title":"Debug within VSCode","text":"<p>Start debugger:  shift -&gt; cmd -&gt; P: <code>Quarkus:  Debug current Quarkus Project</code> to create a configuration.</p>"},{"location":"java/quarkus/#other-maven-quarkus-cli","title":"Other Maven Quarkus CLI","text":"<p>See Maven tooling guide</p> <pre><code># Run integration tests on native app\n./mvnw verify -Pnative\n# Generate configuration for the application\n./mvnw quarkus:generate-config\n</code></pre>"},{"location":"java/quarkus/#add-capabilities","title":"Add capabilities","text":"<p>Useful capabilities:</p> <pre><code>quarkus ext add health,smallrye-openapi,kafka\n</code></pre> <p>older way</p> <ul> <li>Heath for liveness and readiness: <code>./mvnw quarkus:add-extension -Dextensions=\"smallrye-health\"</code></li> <li>Metrics for application monitoring: <code>./mvnw quarkus:add-extension -Dextensions=\"smallrye-metrics\"</code></li> <li>Use API over HTTP in the JSON format: <code>./mvnw quarkus:add-extension -Dextensions=\"resteasy-jsonb\"</code>.</li> <li>Openapi and swagger-ui <code>./mvnw quarkus:add-extension -Dextensions=\"quarkus-smallrye-openapi\"</code>. Also to get the swagger-ui visible in \"production\" set <code>quarkus.swagger-ui.always-include=true</code> in the application.properties.</li> <li>Kafka client: <code>./mvnw quarkus:add-extension -Dextensions=\"kafka\"</code></li> <li>Kubernetes to get the deployment yaml file generated</li> </ul> <pre><code>./mvnw quarkus:add-extension -Dextensions=\"kubernetes,kubernetes-config\"\n</code></pre> <ul> <li>Deploy to OpenShift using source to image <code>./mvnw quarkus:add-extension -Dextensions=\"openshift\"</code>.  See guide QUARKUS - DEPLOYING ON OPENSHIFT</li> <li><code>./mvnw quarkus:add-extension -Dextensions=\"container-image-docker\"</code></li> <li> <p>Postgres hibernate reactive: <code>./mvnw quarkus:add-extension -Dextensions=reactive-pg-client,resteasy-mutiny</code> Then see the REACTIVE SQL CLIENTS guide and the maas_backend project</p> </li> <li> <p>vert.x: <code>./mvnw quarkus:add-extension -Dextensions=\"vertx\"</code></p> </li> <li>jib: to do container image build. See note here <code>./mvnw quarkus:add-extension -Dextensions=\"container-image-jib\"</code></li> <li>Kogito:  <code>./mvnw quarkus:add-extension -Dextensions=\"kogito\"</code></li> </ul>"},{"location":"java/quarkus/#docker-build","title":"Docker build","text":"<pre><code># image name will be the name of the project in pom. build only\n./mvnw clean package -Dnative -Dquarkus.container-image.build=true -Dquarkus.container-image.group=ibmcase -Dquarkus.container-image.tag=1.0.0\n# and push it to repository: \n./mvnw clean package -Dquarkus.container-image.build=true -Dquarkus.container-image.push=true\n</code></pre> <p>To avoid downloading all the maven jars while using a multistage Dockerfile and to keep the current executable started with <code>quarkus:dev</code> running on the same docker network as other dependent components, use a simple docker file for development that has java and maven:</p> <pre><code>FROM maven:3.6.3-jdk-11\n\nCOPY pom.xml /home/\nCOPY src /home/src/\n\nWORKDIR /home\n\nCMD [\"mvn\", \"compile\", \"quarkus:dev\"]\n</code></pre> <p>And then start it, by mounting the .m2 maven repository.</p> <pre><code>docker build -f Dockerfile-dev -t tmp-builder .\ndocker run --rm -p 8080:8080 -ti  -v ~/.m2:/root/.m2 tmp-builder\n</code></pre> <p>We can also combine this into a docker-compose file like in the eda item inventory project.</p> <pre><code>  maven:\n    image: maven\n    volumes:\n      - \"./:/app\"\n      - \"~/.m2:/root/.m2\"\n    depends_on:\n      - kafka\n    hostname: aggregator\n    environment:\n      - BOOTSTRAP_SERVERS=kafka:9092\n      - QUARKUS_KAFKA_STREAMS_BOOTSTRAP_SERVERS=kafka:29092\n      - QUARKUS_PROFILE=dev\n    ports:\n      - \"8080:8080\"\n      - \"5005:5005\"\n    working_dir: /app\n    command: \"mvn quarkus:dev\"\n</code></pre>"},{"location":"java/quarkus/#openapi","title":"OpenAPI","text":"<p>Add the declaration of the Application api in properties. See this open API guide for configuration details.</p> <pre><code>%dev.mp.openapi.extensions.smallrye.info.title=Product Microservice API (development)\nmp.openapi.extensions.smallrye.info.title=Product Microservice API\nmp.openapi.extensions.smallrye.info.version=1.0.0\nmp.openapi.extensions.smallrye.info.description=Just an example of kafka consumer and service\nmp.openapi.extensions.smallrye.info.termsOfService=Demonstration purpose\nmp.openapi.extensions.smallrye.info.contact.name=IBM Technical Asset and Architecture team \nmp.openapi.extensions.smallrye.info.contact.url=https://ibm-cloud-architecture/refarch-eda\nmp.openapi.extensions.smallrye.info.license.name=Apache 2.0\nmp.openapi.extensions.smallrye.info.license.url=http://www.apache.org/licenses/LICENSE-2.0.html\n</code></pre>"},{"location":"java/quarkus/#running-on-openshift","title":"Running on OpenShift","text":"<p>The guide is here and the main points are:</p> <ul> <li>The OpenShift extension is actually a wrapper extension that brings together the kubernetes and container-image-s2i extensions with defaults so that it\u2019s easier for the user to get started with Quarkus on OpenShift</li> <li>Build is done by source 2 image binary build: <code>./mvnw clean package -Dquarkus.container-image.build=true</code>. The output of the build is an ImageStream that is configured to automatically trigger a deployment</li> <li> <p>The first time, build and deployment are done with the command: <code>./mvnw clean package -Dquarkus.kubernetes.deploy=true</code>, after that, any build will trigger redeployment. Here are the steps done:</p> <ul> <li>Build thin jar</li> <li>Contact kubernetes API server</li> <li>Perform s2i binary build with jar on the server. This adds a BuildConfig on the connected project.</li> <li>Send source code from local folder to OpenShift build container.</li> <li>Write manifest and signatures </li> <li>Generate Dockerfile and build the image on server</li> <li>Push image to private registry</li> <li>Apply the manifests: service account, service, image stream, build config, and deployment config as defined by the generated <code>openshift.yaml</code></li> </ul> </li> <li> <p>Three pods are visible: build, deploy and running app.</p> </li> </ul> <pre><code># Remote build on OCP\nmvn clean package -Dquarkus.container-image.build=true -Dquarkus.kubernetes.deploy=true -DskipTests\n# example from store simulator with UI\nmvn clean package -Dui.deps -Dui.dev -Dquarkus.container-image.build=true -Dquarkus.container-image.group=ibmcase -Dquarkus.container-image.tag=1.0.0 -Dquarkus.kubernetes.deploy=true -DskipTests\n# Get image streams\noc get is\n# From the created image\n# Create an application in openshift- ex:\noc new-app --name=credit-application-mgr jb-credit-app/credit-application-mgr:1.0.0 \n</code></pre> <p>See all the configuration parameters here.</p>"},{"location":"java/quarkus/#play-with-environment-variables","title":"Play with environment variables","text":"<ul> <li> <p>To define environment variables, use config map:</p> <ul> <li>Add a config map with the variable name in the data filed as key. It follows the environment variable naming convention to overwrite quarkus' property. The config map can have multiple variables.</li> </ul> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: message-cm\ndata:\n  GREETING_MESSAGE: salut\n</code></pre> <ul> <li>To inject environment variables from config map add a property with the name of the config map.</li> </ul> <pre><code>greeting.message=bonjour\nquarkus.openshift.env.configmaps=message-cm\n</code></pre> </li> <li> <p>Redeploy: <code>./mvnw clean package -Dquarkus.kubernetes.deploy=true</code></p> </li> </ul> <p>This will add the following declaration to the deploymentConfig:</p> <pre><code>      envFrom:\n            - configMapRef:\n                name: message-cm\n</code></pre> <ul> <li>To add config map, secrets, we need the <code>kubernetes-config</code> extension. See this guide, then declare properties in the following formats:</li> </ul> <pre><code>quarkus.openshift.env.configmaps=vaccine-order-ms-cm\nquarkus.openshift.env.secrets=vaccine-order-secrets\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret=light-es-cluster-ca-cert\nquarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key=ca.password\nquarkus.openshift.mounts.es-cert.path=/deployments/certs/server\nquarkus.openshift.secret-volumes.es-cert.secret-name=light-es-cluster-ca-cert\n</code></pre> <p>That will generate the following spec:</p> <pre><code>   envFrom:\n     - configMapRef:\n       name: vaccine-order-ms-cm\n     - secretRef:\n       name: vaccine-order-secrets\n   volumeMounts:\n   - mountPath: /deployments/certs/server\n     name: es-cert\n     readOnly: false\n     subPath: \"\"\n volumes:\n     - name: es-cert\n       secret:\n         defaultMode: 384\n         optional: false\n         secretName: sandbox-rp-cluster-ca-cert\n</code></pre> <p>See OpenShift options</p> <p>To change the value of a specific property in the application properties, we can use environment  variables: The convention is to convert the name of the property to uppercase and replace  every dot (.) with an underscore (_). So define a config map to define those environment  variables in <code>src/main/kubernetes</code> folder.</p>"},{"location":"java/quarkus/#some-reusable-configuration","title":"Some reusable configuration","text":"<ul> <li>To expose a route add:</li> </ul> <pre><code>quarkus.openshift.expose.route=true\n</code></pre> <ul> <li>Set logging level</li> </ul> <pre><code>quarkus.log.console.enable=true\nquarkus.log.console.format=%d{HH:mm:ss} %-5p [%c{2.}] (%t) %s%e%n\nquarkus.log.console.level=DEBUG\nquarkus.log.console.color=true\n</code></pre>"},{"location":"java/quarkus/#remote-dev-mode","title":"Remote dev mode","text":"<p>For running Quarkus app on OpenShift while developing locally so change done on code, pom.xml, properties are sent to the remote quarkus use the following settings:</p> <p>See this guide section.</p> <ul> <li>Add in application.properties:</li> </ul> <pre><code>quarkus.package.type=mutable-jar\nquarkus.live-reload.password=changeit\n</code></pre> <ul> <li>set the environment variable QUARKUS_LAUNCH_DEVMODE=true</li> <li>start with <code>./mvnw quarkus:remote-dev -Dquarkus.live-reload.url=http://my-remote-host:8080</code></li> </ul> <p>This is done via a HTTP based long polling transport, that will synchronize your local workspace and the remote application via HTTP calls.</p>"},{"location":"java/quarkus/#testing-with-quarkus","title":"Testing with Quarkus","text":"<p>Quarkus uses junit 5, and QuarkusTest to access to CDI and other Quarkus goodies.  <code>quarkus dev</code> enables continuous testing, and understands what tests are affected by code changes.</p> <p>See the test guide here. </p> <p>With the dev console (<code>/q/dev</code>) offers access to the test UI. To test via HTTP, and rest-assured. </p> <p>Here is an example for post testing:</p> <pre><code>TrainSearchRequest req = new TrainSearchRequest(); //...\nResponse resp = with().headers(\"Content-Type\", ContentType.JSON, \"Accept\", ContentType.JSON)\n          .body(req)\n          .when().post(\"/consolidatorA\")\n          .then()\n             .statusCode(200)\n             .contentType(ContentType.JSON)\n        .extract()\n        .response();\n        TrainSearchResponse searchResp= resp.body().as(TrainSearchResponse.class);\n</code></pre> <p>Other example on a Get</p> <pre><code>Response rep = given()\n          .when().get(basicURL + \"/names\")\n          .then()\n             .statusCode(200)\n             .contentType(ContentType.JSON)\n        .extract()\n        .response();\n        String[] names = rep.body().as(String[].class);\n        Assertions.assertNotNull(names);\n        Assertions.assertEquals(3,names.length);\n</code></pre> <p>Application configuration will be used in any active profile. The built-in profiles in Quarkus are: <code>dev, prod and test</code>. The <code>test</code> profile will be used every time you run the <code>@QuarkusTest</code>.</p> <p><code>@QuarkusTest</code> helps to get the CDI working. But there is still an issue on inject properties  that may not be loaded due to proxy instance creation. So in test class   the properties need to be accessed via getter: For example to be sure the hostname is loaded from the <code>application.properties</code> do:</p> <pre><code>@ApplicationScoped\npublic class RabbitMQItemGenerator {\n\n  @ConfigProperty(name = \"amqp.host\")\n  public String hostname;\n...\n\n@QuarkusTest\npublic class TestRabbitGenerator {\n\n    @Inject\n    RabbitMQItemGenerator generator;\n\n    ...\n    Assertions.assertNotNull(generator.getHost());\n</code></pre> <p>Things to do:</p> <ul> <li>in the test class add @QuarkusTest</li> <li>inject the bean under test</li> <li>be sure to use the good version of maven-surefire</li> <li>set the java.util.logging system property to make sure tests will use the correct log manager:</li> </ul> <pre><code>&lt;systemPropertyVariables&gt;\n    &lt;java.util.logging.manager&gt;org.jboss.logmanager.LogManager&lt;/java.util.logging.manager&gt;\n&lt;/systemPropertyVariables&gt;\n</code></pre> <ul> <li>Exposed port for tests are on port 8081 (can be changed via <code>quarkus.http.test-port=8083</code>)</li> </ul> <p>Integration test uses Rest-assured with API doc.</p> <p>For testing body content, use the Hamcrest APIs. Example of not and containsString operators.</p> <pre><code>public void shouldNotHaveStore_7_fromGetStoreNames(){\n        given().when().get(\"/names\").then().statusCode(200).body(not(containsString(\"Store_7\")));\n    }\n</code></pre>"},{"location":"java/quarkus/#maven-profile-and-quarkus-test","title":"Maven profile and quarkus test","text":"<p>The properties can be prefixed with <code>%staging</code> or <code>%prod</code> or <code>%dev</code> to set those properties according to the deployment. If we use a custom prefix like <code>staging</code> then we need to run quarkus with a specific profile. For that set <code>export QUARKUS_PROFILE=staging</code> in your <code>.env</code> file. </p> <p>By default, Quarkus tests in JVM mode are run using the test configuration profile</p>"},{"location":"java/quarkus/#vue-app-with-quarkus-app","title":"Vue app with quarkus app","text":"<p>See this separate note.</p>"},{"location":"java/quarkus/#reactive-crud-app-with-postgres","title":"Reactive CRUD app with Postgres","text":"<ul> <li>Add following to pom.xml</li> </ul> <pre><code>&lt;dependency&gt;\n      &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n      &lt;artifactId&gt;quarkus-resteasy-mutiny&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n      &lt;artifactId&gt;quarkus-reactive-pg-client&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n      &lt;artifactId&gt;quarkus-hibernate-reactive&lt;/artifactId&gt;\n  &lt;/dependency&gt;\n</code></pre> <ul> <li>Annotate the entity with table and query</li> </ul> <pre><code>@Entity\n@Table(name = \"kafka_clusters\")\n@NamedQuery(name = \"ClusterDetail.findAll\", query = \"SELECT k FROM kafka_clusters k ORDER BY k.name\")\npublic class ClusterDetail {\n\n    @Id\n    @SequenceGenerator(name = \"fruitsSequence\", sequenceName = \"known_fruits_id_seq\", allocationSize = 1, initialValue = 10)\n    @GeneratedValue(generator = \"fruitsSequence\")\n    private Integer id;\n</code></pre>"},{"location":"java/quarkus/#command-mode-application","title":"Command mode application","text":"<p>Develop command mode applications that run and then exit, like a java main method.</p> <p>For that implement QuarkusApplication and have Quarkus run this method automatically</p> <pre><code>@QuarkusMain\npublic class CommandModeApp implements QuarkusApplication {\n\n    @Override\n    public int run(String... args) throws Exception {\n        System.out.println(\"Hello from mars\");\n        return 10;\n    }\n</code></pre> <p>A @QuarkusMain instance is an application scoped bean by default.</p> <p>For a Java main app</p> <pre><code>@QuarkusMain(name: \"kbMain\")\npublic class JavaMain {\n\n    public static void main(String... args) {\n        Quarkus.run(HelloWorldMain.class, args);\n    }\n}\n</code></pre> <p>Dev mode works the same.</p> <p>Then <code>mvn package</code> and run it with <code>java -jar target/....-runner.jar</code></p> <p>When running a command mode application the basic lifecycle is as follows:</p> <ul> <li>Start Quarkus</li> <li>Run the QuarkusApplication main method</li> <li>Shut down Quarkus and exit the JVM after the main method returns</li> </ul> <p>It is possible to have multiple main methods in an application, and select between them at build time. The @QuarkusMain annotation takes an optional 'name' parameter and the name is used with <code>-Dquarkus.package.main-class=\"kbMain\"</code>. If the app needs arguments, pass them with <code>-Dquarkus.args=</code>.</p>"},{"location":"java/quarkus/#development-practices","title":"Development practices","text":""},{"location":"java/quarkus/#configuration","title":"Configuration","text":"<p><code>application.properties</code> content is injected with code like:</p> <pre><code>    @ConfigProperty(name=\"temperature.threshold\", defaultValue=\"2.5\")\n    public double temperatureThreshold;\n</code></pre> <p>Quarkus does much of its configuration and bootstrap at build time. But some properties are defined at run time from system properties, environment variables (in uppercase, . transformed as _ : QUARKUS_DATASOURCE_PASSWORD), using <code>.env</code> file, and then <code>application.property</code> in a <code>$(pwd)/target/config</code> folder for development test.</p> <pre><code>// does fail as it return -\nAssertions.assertEquals(2.0, assessor.temperatureThreshold);\n// while this works!\nAssertions.assertEquals(2.0, assessor.getTemperatureThreshold());\n</code></pre> <p>The content can be combined with environment variables See this section in quarkus guide</p> <p>Also you can access the configuration programmatically. It can be handy to achieve dynamic lookup, or retrieve configured values from classes that are neither CDI beans or JAX-RS resources. See this paragraph</p> <pre><code>String databaseName = ConfigProvider.getConfig().getValue(\"database.name\", String.class);\nOptional&lt;String&gt; maybeDatabaseName = ConfigProvider.getConfig().getOptionalValue(\"database.name\", String.class);\n</code></pre> <p>Quarkus supports the notion of configuration profiles. These allow you to have multiple configuration in the same file and select between them via a profile name.</p> <p>See also Using Property Expressions</p>"},{"location":"java/quarkus/#get-access-to-start-and-stop-application-events","title":"Get access to start and stop application events","text":"<p>This is an important practice when we want to start a listener on Kafka topic for example or a kafka stream topology.</p> <pre><code>import javax.enterprise.event.Observes;\n...\n void onStart(@Observes StartupEvent ev){}\n\n void onStop(@Observes ShutdownEvent ev) { }\n</code></pre>"},{"location":"java/quarkus/#testing-with-moquito","title":"Testing with Moquito","text":"<p>Declare the beans to Injected in the Test class. Then a static method to create and moquito them. See this quarkus blog</p> <p>Mockito API</p> <pre><code> JMSQueueWriter mockJMSWriter = Mockito.mock(JMSQueueWriter.class);  \n        Mockito.when(mockJMSWriter.sendMessage(any(),anyString())).thenReturn(\"Success\");\n        QuarkusMock.installMockForType(mockJMSWriter, JMSQueueWriter.class);  \n</code></pre>"},{"location":"java/quarkus/#reactive-with-mutiny","title":"Reactive with Mutiny","text":"<p>Mutiny is a reactive programming library to offer  a more guided API than traditional reactive framework and API. It supports asynchronous,  non-blocking programming, streams, event driven, back-pressure and data flows and only two types to manage asynchronous actions (Uni and Multi).</p> <p>Quarkus uses an event-loop to implement the reactive mode, based on Vert.X as reactive engine. To program with reactive  or imperative, we use different APIs. Mutiny is such API.</p> <p>To asynchronously handle HTTP requests, the endpoint method must return a java.util.concurrent.CompletionStage  or an <code>io.smallrye.mutiny.Uni</code>  or <code>io.smallrye.mutiny.Multi</code>(requires the quarkus-resteasy-mutiny extension).</p> <p>With Mutiny both <code>Uni</code> and <code>Multi</code> expose event-driven APIs: you express what you want to do upon a given event (success, failure, etc.).  These APIs are divided into groups (types of operations) to make it more expressive and avoid having 100s of methods attached to a single class.</p> <p>This section of the product documentation goes over some examples on how to use Uni/ Multi.</p> <p>Here are some basic examples:</p> <ul> <li>Support asynchronous REST end point</li> </ul> <pre><code>    @GET\n    @Produces(MediaType.APPLICATION_JSON)\n    @Path(\"/json/{id}\")\n  // producing generic json object\n  public Uni&lt;JsonObject&gt; getJsonObject(@PathParam String id){\n        JsonObject order = new JsonObject(\"{\\\"name\\\": \\\"hello you\\\", \\\"id\\\": \\\"\" + id + \"\\\"}\");\n        return Uni.createFrom().item( order);\n    }\n\n    // Producing a bean\n    @GET\n    @Produces(MediaType.APPLICATION_JSON)\n    @Path(\"/order/{id}\")\n    public Uni&lt;Order&gt; getOrderById(@PathParam String id){\n        Order o = new Order();\n        o.deliveryLocation=\"Paris/France\";\n        o.id=id;\n        return Uni.createFrom().item(o);\n    }\n</code></pre> <ul> <li> <p>Calling an external service</p> <ol> <li>Create an interface about this service</li> <li>Call the service and use the different event to process success and failure </li> </ol> <pre><code>  interface GreetingService {\n    Uni&lt;String&gt; greeting(String name);\n  }\n\n//somewhere in a class\n greetingService.greeting(\"Luke\")\n            .subscribe().with(\n            item -&gt; System.out.println(item),\n            failure -&gt; System.out.println(\"Oh no! \" + failure.getMessage())\n    );\n</code></pre> </li> <li> <p>Processing data as stream:</p> <pre><code> Multi&lt;String&gt; stream = Multi.createFrom().items(\"a\", \"b\", \"c\", \"d\");\n    stream\n            .subscribe().with(\n                item -&gt; System.out.println(\"Received an item: \" + item),\n                failure -&gt; System.out.println(\"Oh no! Received a failure: \" + failure.getMessage()),\n                () -&gt; System.out.println(\"Received the completion signal\")\n    );\n</code></pre> </li> <li> <p>Create a multi from a list and send to kafka</p> </li> </ul> <pre><code>  @Inject\n  @Channel(\"items\")\n  Emitter&lt;Item&gt; emitter;\n\n Multi.createFrom().items(buildItems(numberOfRecords).stream())\n                .subscribe().with( item -&gt; {\n                    logger.warning(\"send \" + item.toString());\n                    //KafkaRecord&lt;String, Item&gt; record = KafkaRecord.of(item.sku,item);\n                    CompletionStage&lt;Void&gt; acked = emitter.send(item);\n                    acked.toCompletableFuture().join();\n                    },\n                    failure -&gt; System.out.println(\"Failed with \" + failure.getMessage()));\n</code></pre> <p>Events won\u2019t begin flowing through the data streams until a subscriber requests them.</p>"},{"location":"java/quarkus/#reactive-messaging","title":"Reactive messaging","text":"<p>Content built from Quarkus guide and Clement Escoffier's book.</p> <p>For a quick review of a reactive messaging guide search <code>reactive messaging</code> Quarkus tutorial  is here</p> <p>See also this article for reactive programming and system summary.</p> <p>Quick summary:</p> <ul> <li>Reactive Messaging integrates with various messaging technologies, such as Apache Kafka, AMQP, and others...</li> <li>Add extension: <code>quarkus ext add smallrye-reactive-messaging-kafka</code></li> <li>No need to start a Kafka broker when using the dev mode or for tests. Quarkus starts redpanda container and apicurio if using schema</li> <li>Define an application scoped bean for your service. See code template in eda-quickstarts/quarkus-reactive-kafka-producer</li> </ul> <pre><code>@ApplicationScoped\npublic class OrderService {\n    @Channel(\"orders\")\n    public Emitter&lt;OrderEvent&gt; eventProducer ;\n\n    public OrderEntity createOrder(OrderEntity order){\n      OrderEvent orderEvent = OrderEvent.from(order);\n      eventProducer.send(orderEvent);\n    }\n</code></pre> <ul> <li>define channel properties in <code>application.properties</code>.</li> </ul> <pre><code>mp.messaging.outgoing.orders.connector=smallrye-kafka\nmp.messaging.outgoing.orders.topic=orders\nmp.messaging.outgoing.orders.value.serializer=io.quarkus.kafka.client.serialization.ObjectMapperSerializer\n# automatically register the schema with the registry, if not present\nmp.messaging.outgoing.orders.apicurio.registry.auto-register=true\n</code></pre> <ul> <li> <p>On the consumer side, use <code>@Incoming</code> annotation with channel name on top of function to process the message from kafka</p> </li> <li> <p>Use Kafka connector: <code>mp.messaging.incoming.[channel-name].connector=smallrye-kafka</code></p> </li> <li> <p>Implement Deserializer using Jsonb or Jackson. See this section. </p> </li> <li> <p>When using Avro and Apicurio schema registry then the deserializer needs to be <code>io.apicurio.registry.utils.serde.AvroKafkaDeserializer</code> But with Quarkus 2.5 the deserializer and serializer are set by default.</p> </li> </ul> <pre><code>mp.messaging.incoming.shipments.value.deserializer=io.apicurio.registry.utils.serde.AvroKafkaDeserializer\nmp.messaging.incoming.shipments.apicurio.registry.url=${SCHEMA_REGISTRY_URL}\nmp.messaging.incoming.shipments.specific.avro.reader=true\nmp.messaging.incoming.shipments.apicurio.registry.avro-datum-provider=io.apicurio.registry.utils.serde.avro.ReflectAvroDatumProvider\nmp.messaging.incoming.shipments.apicurio.registry.as-confluent=true\n</code></pre> <p>Nice cheat sheet to combine Mutiny, Reactive messaging.</p> <ul> <li>It is possible to combine imperative and reactive: so on a POST api, the code emits event to Kafka. We just need to inject an emitter as below:</li> </ul> <pre><code>  @Inject @Channel(\"items\") Emitter&lt;KafkaRecord&lt;String, Item&gt;&gt; emitter;\n</code></pre> <p>Emitting Kafka Records will duplicate the payload.</p> <pre><code>{\"headers\":{},\n\"key\":\"Item_4\",\n\"metadata\":{},\"partition\":-1,\n\"payload\":{\"id\":296,\"price\":72.9,\"quantity\":7,\"sku\":\"Item_4\",\"storeName\":\"Store_2\"}}\n</code></pre> <p>To send directly the payload with a key, use a Message instance.</p> <p>Apache Kafka specific for reactive messaging</p> <p>The Kafka Connector is based on the Vert.x Kafka Client.</p>"},{"location":"java/quarkus/#a-case-for-kafka-to-sse","title":"A case for Kafka to SSE","text":"<p>A nice capability is to open a Server Side Event end point (see code in freezer mgr project):</p> <p>In the API resource class:</p> <pre><code>    @Inject\n    @Channel(\"internal-alert-stream\")\n    Publisher&lt;ReeferAlert&gt; alerts;\n\n    @GET\n    @Path(\"/alerts\")\n    @Produces(MediaType.SERVER_SENT_EVENTS)\n    @SseElementType(MediaType.APPLICATION_JSON)\n    public Publisher&lt;ReeferAlert&gt; streamAlerts(){\n        return alerts;\n    }\n</code></pre> <p>In the service class, the incoming channel is connected to Kafka and the outgoing is an internal channel to broadcast message to it.</p> <pre><code>    @Incoming(\"reefer-alerts\")                                     \n    @Outgoing(\"internal-alert-stream\")                             \n    @Broadcast                                              \n    @Acknowledgment(Acknowledgment.Strategy.PRE_PROCESSING)\n    public ReeferAlert process(ReeferAlert inAlert){\n        // process the alert and generate the message to broadcast to SSE\n        return inAlert;\n    }\n</code></pre>"},{"location":"java/quarkus/#rest-client","title":"REST Client","text":"<ul> <li>Install client jars</li> </ul> <pre><code>quarkus ext  add rest-client-reactive-jackson\n</code></pre> <ul> <li>Add an interface</li> </ul>"},{"location":"java/quarkus/#adopting-vertx","title":"Adopting Vertx","text":"<p>Quarkus is based on Vert.x, and almost all network-related features rely on Vert.x / Netty. Every IO interaction passes through the non-blocking and reactive Vert.x engine. The (Vert.x) HTTP server receives the request and then routes it to the application. If the request targets a JAX-RS resource, the routing layer invokes the resource method in a worker thread and writes the response when the data is available. But if the HTTP request targets a reactive (non-blocking) route, the routing layer invokes the route on the IO thread giving lots of benefits such as higher concurrency and performance.</p> <p></p> <p>To fully benefit from this model, the application code should be written in a non-blocking manner.</p> <p>Add the extension: <code>./mvnw quarkus:add-extension -Dextensions=\"vertx\"</code>. Get access to Vert.x via injection: </p> <pre><code>@Inject Vertx vertx;\n</code></pre> <p>When using the Mutiny API to program in reactive approach, then the Vert.x package is <code>io.vertx.mutiny.core.Vertx</code>.</p>"},{"location":"java/quarkus/#typical-problems","title":"Typical problems","text":""},{"location":"java/quarkus/#running-cloud-native","title":"Running cloud native","text":"<p>By default, when building a native executable, GraalVM will not include any of the resources that are on the classpath into the native executable it creates. Resources that are meant to be part of the native executable need to be configured explicitly.</p> <p>See writing native app tips</p>"},{"location":"java/quarkus/#run-quarkus-test-with-external-components-started-with-docker-compose","title":"Run quarkus test with external components started with docker compose","text":"<p>The best approach is to avoid using docker-compose for development and use TestContainer.</p>"},{"location":"java/quarkus/#cheat-sheet","title":"Cheat-Sheet","text":"<ul> <li>Official</li> </ul>"},{"location":"java/quarkus/#to-read","title":"To read","text":"<ul> <li>Using Kubernetes ConfigMaps to define your Quarkus application\u2019s properties</li> <li>Reactive Programming with Quarkus - postgresql reactive with mutiny</li> </ul>"},{"location":"java/restassured/","title":"Rest Assured","text":"<p>REST Assured is used to test REST services in java. See some examples here and Baeldung post</p> <p>Test a GET</p> <pre><code>import io.restassured.http.ContentType;\nimport io.restassured.response.Response;\nimport static io.restassured.RestAssured.when;\n@Test public void\nlotto_resource_returns_200_with_expected_id_and_winners() {\n\n    when().\n            get(\"/lotto/{id}\", 5).\n    then().\n            statusCode(200).\n            body(\"lotto.lottoId\", equalTo(5),\n                 \"lotto.winners.winnerId\", hasItems(23, 54));\n\n}\n</code></pre> <p>Other examples</p> <ul> <li>InventoryResourceIT:</li> </ul> <pre><code>public void shouldGetOneInventory(){\n        Response r = given().headers(\"Content-Type\", ContentType.JSON, \"Accept\", ContentType.JSON)\n        .when()\n        .get(\"/inventory/Store-1/Item-1\")\n        .then()\n        .statusCode(200)\n        .contentType(ContentType.JSON)\n        .extract()\n        .response();\n\n        System.out.println(r.jsonPath());\n    }\n</code></pre> <p>Test a POST</p> <pre><code>import static io.restassured.RestAssured.given;\n\ngiven().pathParam(\"numberRecords\", 3)\n          .when().post(\"http://localhost:8080/start/{numberRecords}\")\n          .then()\n             .statusCode(200)\n             .body(is(\"started\"));\n</code></pre> <p>Another POST with taking the response body to a bean</p> <pre><code>import io.restassured.http.ContentType;\nimport io.restassured.response.Response;\nimport static io.restassured.RestAssured.with;\n\nClusterBasicInfo clusterInfo = new ClusterBasicInfo();\n        clusterInfo.name = \"localKafka\";\n        clusterInfo.adminURL = kafkaContainerForTest.getBootstrapServers();\n        clusterInfo.bootstrapURL = clusterInfo.adminURL;\n        Response resp = with().headers(\"Content-Type\", ContentType.JSON, \"Accept\", ContentType.JSON)\n          .body(clusterInfo)\n          .when().post(basicURL)\n          .then()\n             .statusCode(200)\n             .contentType(ContentType.JSON)\n        .extract()\n        .response();\n\n        ClusterDetail updateDetails = resp.body().as(ClusterDetail.class);\n</code></pre> <p>And array of beans:</p> <pre><code>Response rep = given()\n          .headers(\"Content-Type\", ContentType.JSON, \"Accept\", ContentType.JSON)\n          .when().get(basicURL)\n          .then()\n             .statusCode(200)\n             .contentType(ContentType.JSON)\n            .extract()\n            .response();\n            System.out.println(rep.jsonPath().prettyPrint());\n            OrderDTO[] orders = rep.body().as(OrderDTO[].class);\n            Assertions.assertTrue(orders.length &gt;= 2);\n</code></pre>"},{"location":"java/springcloud/","title":"Spring Cloud and Spring Boot","text":"<p>This note is to summarize the techno, from an older springframework (2003) dev. </p>"},{"location":"java/springcloud/#spring-boot","title":"Spring Boot","text":"<p>To get started with Spring Boot see this REST service app guide.  Use Spring initialzr tool to get the maven project starting code. The starter guide use Spring Web.</p> <p>When creating app to be deployed as container think to add Actuator to get health, metrics, sessions, ...</p> <p>To build: <code>./mvnw clean package</code>. To run the main executable: <code>java -jar target/demo-0.0.1-SNAPSHOT.jar</code></p> <p><code>@SpringBootApplication</code> is a convenience annotation that adds: <code>@Configuration</code>, <code>@EnableAutoConfiguration</code>  (Tells Spring Boot to start adding beans based on classpath settings, other beans, and various property settings), <code>@ComponentScan</code>. (look for other components, configurations, and services in package, letting it find the controllers.</p> <p>REST resources are <code>@RESTController</code> and the methods are annoted with <code>@GetMapping</code>, <code>@PostMapping</code>... To use test driven development add the dependency</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n    &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre> <p>And then test classes like:</p> <pre><code>import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get;\nimport static org.springframework.test.web.servlet.result.MockMvcResultHandlers.print;\nimport static org.springframework.test.web.servlet.result.MockMvcResultMatchers.jsonPath;\nimport static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;\n\n@SpringBootTest\n@AutoConfigureMockMvc\npublic class GreetingControllerTests {\n\n    @Autowired\n    private MockMvc mockMvc;\n\n    @Test\n    public void noParamGreetingShouldReturnDefaultMessage() throws Exception {\n\n        this.mockMvc.perform(get(\"/greeting\")).andDo(print()).andExpect(status().isOk())\n                .andExpect(jsonPath(\"$.content\").value(\"Hello, World!\"));\n    }\n}\n</code></pre> <p>One of the most complete app is KC container service</p>"},{"location":"java/springcloud/#to-remember","title":"To Remember","text":"<ul> <li>Spring boot app is a main with <code>@SpringBootApplication</code> annotation to let the injection and bean management working</li> <li>Bean is the IoC component of your application managed by Spring. A function can become a bean using <code>@Bean</code>.</li> <li>@Configuration indicates that a class declares one or more @Bean methods and may be processed by the Spring container to generate bean definitions.</li> </ul>"},{"location":"java/springcloud/#some-how-to","title":"Some how to","text":"<ul> <li>Define the app-root for url: add properties: <code>server.servlet.context-path: orderms</code></li> <li>Get the VScode to recognize <code>org.springframework</code> package: use <code>Update Project</code> on the pom.xml</li> <li> <p>Copy a java bean properties into another one: use BeanUtils from spring: The bean needs to have getters and setters</p> <pre><code>public static Order from(OrderDTO dto){\n\n    Order mappedOrder = new Order();\n    BeanUtils.copyProperties(dto, mappedOrder);\n    return mappedOrder;\n}\n</code></pre> </li> </ul>"},{"location":"java/springcloud/#add-swagger-ui","title":"Add swagger UI","text":"<ul> <li>Add springfox dependency    </li> <li>Add swagger configuration via Docket bean</li> </ul>"},{"location":"java/springcloud/#spring-cloud","title":"Spring Cloud","text":"<p>&lt; re integrate tvp work&gt;</p>"},{"location":"java/vertx/","title":"Vert.x","text":"<p>A polyglot library to develop event driven non blocking apps. The main website vertx.io/ has a lot of examples to get started.</p>"},{"location":"java/vertx/#concepts","title":"Concepts","text":"<ul> <li>Vert.x is not a restrictive framework, and doesn't force the correct way to write an application.</li> <li>It uses an event loop thread to process requests with non blocking IO, and dispatch to event handlers. </li> <li>Verticles are pieces of code that Vert.x engine executes</li> <li>An application would typically be composed of multiple verticles running in the same Vert.x instance and communicate with each other using events via the event bus</li> <li>In Quarkus every IO interaction passes through the non-blocking and reactive Vert.x engine</li> <li>Verticles remain dormant until they receive a message or event.</li> <li>Message handling is ideally asynchronous, messages are queued to the event bus, and control is returned to the sender</li> <li>Vert.x includes an  event bus  allowing the different components of an application to interact using  messages. Messages are sent to  addresses and have a set of  headers  and a  body.</li> <li>By starting a Vert.x application in cluster  mode, nodes are connected to enable shared data structure, hard-stop failure detection, and load-balancing group communication. Vert.x uses Hazelcast by default to support the cluster management.</li> </ul>"},{"location":"java/vertx/#get-started","title":"Get started","text":"<p>Use the app generator to get starting code and pom.xml or use Quarkus app generator as it has a Vert.x engine embedded inside it.</p> <p>See all the vert.x samples here.</p> <ul> <li>Simple http server with vertx router. To run it: <code>mvn  exec:java -Dexec.mainClass=io.vertx.example.openshift.grting.MyGreetingApp</code></li> <li>A verticle to call a service via http</li> <li>Clustered http server to deploy on openshift</li> <li>Event bus point to point</li> <li>Event bus pub/sub</li> <li>Circuit breaker</li> <li>Kafka consumer and producer</li> </ul> <p>vertx has a CLI to run verticle, by bypassing maven or graddle.</p> <ul> <li>Start a http server with vertx: <code>vertx run io.vertx.examples.openshift.MyHttpVerticle -cp target/clustered-application-http-3.9.0.jar</code></li> </ul>"},{"location":"java/vertx/#event-bus","title":"Event bus","text":"<p>Event bus\u00a0is used by different verticles to communicate through asynchronous message passing  (JSON). </p> <p>A receiver on a point to point channel</p> <pre><code>public void start() throws Exception {\n\n    EventBus eb = vertx.eventBus();\n\n    eb.consumer(\"ping-address\", message -&gt; {\n\n      System.out.println(\"Received message: \" + message.body());\n      // Now send back reply\n      message.reply(\"pong!\");\n    });\n\n    System.out.println(\"Receiver ready!\");\n  }\n</code></pre> <p>Start with cluster option so Hazelcast will manage the verticles and netty will support the TCP connection</p> <p>A sender looks like:</p> <pre><code>public void start() throws Exception {\n    EventBus eb = vertx.eventBus();\n\n    // Send a message every second\n    vertx.setPeriodic(1000, v -&gt; {\n\n      eb.request(\"ping-address\", \"ping!\", reply -&gt; {\n        if (reply.succeeded()) {\n          System.out.println(\"Received reply \" + reply.result().body());\n        } else {\n          System.out.println(\"No reply\");\n        }\n      });\n\n    });\n  }\n</code></pre>"},{"location":"java/vertx/#run-with-docker","title":"Run with docker","text":"<p>Using vert.x with docker to get a Java environment with the Vert.x jars ready to go. </p> <p>To run a Verticle:</p> <pre><code>docker run -i -t -p 8080:8080 \\\n    -v $PWD:/verticles vertx/vertx3-exec \\\n    run io.vertx.sample.RandomGeneratorVerticle \\\n    -cp /verticles/MY_VERTICLE.jar\n</code></pre>"},{"location":"java/vertx/#deploy-existing-application-with-a-dockerfile","title":"Deploy existing application with a dockerfile","text":"<pre><code>oc new-build --binary --name=vertx-greeting-application -l app=vertx-greeting-application\nmvn dependency:copy-dependencies compile\noc start-build vertx-greeting-application --from-dir=. --follow\noc new-app vertx-greeting-application -l app=vertx-greeting-application\noc expose service vertx-greeting-application\n</code></pre> <p>Access to external url and call one endpoint</p> <pre><code>export URL=http://$(oc get routes vertx-greeting-application -o  yaml| yq .spec.host |sed 's/\"//g')\ncurl $URL\n</code></pre>"},{"location":"java/vertx/#circuit-breaker","title":"Circuit breaker","text":"<p>To use the circuit breaker you need to:</p> <ul> <li>Create a circuit breaker, with the configuration you want (timeout, number of failure before opening the circuit)</li> <li>Execute some code using the breaker</li> </ul> <p><code>vertx run io.vertx.example.circuit.breaker.Client -cp target/circuit-breaker-exales-3.9.0.jar</code></p> <p>Operations guarded by a circuit breaker are intended to be non-blocking and asynchronous, in order to benefit from the Vert.x execution mode</p>"},{"location":"java/vertx/#kafka-and-vertx","title":"Kafka and vert.x","text":"<p>The Kafka consumer and producer sample is an interesting example where a kafka broker (from debezium) is started within a main verticle. Start the broker, producers and consumer to see the real time updated dashboard: <code>vertx run io.vertx.example.kafka.dashboard.MainVerticle -cp target/kafka-examples-3.9.0-fat.jar</code></p> <p>For Producer the code is a simple AbstractVerticle</p> <pre><code>import io.vertx.core.json.JsonObject;\nimport io.vertx.kafka.client.producer.KafkaWriteStream;\nimport org.apache.kafka.clients.producer.ProducerRecord;\n\n  private KafkaWriteStream&lt;String, JsonObject&gt; producer;\n\n  public void start() throws Exception {\n    // Get the kafka producer config\n    JsonObject config = config();\n    // Create the producer, with key as string and value as jsonObject\n    producer = KafkaWriteStream.create(vertx, config.getMap(), String.class, JsonObject.class);\n    // get messages and write:\n    producer.write(new ProducerRecord&lt;&gt;(\"the_topic\", new JsonObject().put(key, payload)));\n  }\n\n   public void stop() throws Exception {\n    if (producer != null) {\n      producer.close();\n    }\n  }\n</code></pre> <p>On the consumer side the same approach is used, get the kafka config and then create consumer:</p> <pre><code>JsonObject config = config();\n\nKafkaReadStream&lt;String, JsonObject&gt; consumer = KafkaReadStream.create(vertx, config.getMap(), String.class, JsonObject.class);\n\n    consumer.handler(record -&gt; {\n      JsonObject obj = record.value();\n      // process the received record \n      });\n\n    // Subscribe to Kafka\n    consumer.subscribe(Collections.singleton(\"the_topic\"));\n</code></pre>"},{"location":"java/vertx/#quarkus-vertx-reactive-app","title":"Quarkus Vert.x reactive app","text":"<p>Quarkus HTTP support is based on a non-blocking and reactive engine (Eclipse Vert.x and Netty). All the HTTP requests, your application receives, are handled by\u00a0event loops\u00a0(IO Thread) and then are routed towards the code that manages the request.  Depending on the destination, it can invoke the code managing the request on a worker thread (Servlet, Jax-RS) or use the IO Thread (reactive route). Quarkus also integrates smoothly with the Vert.x event bus (to enable asynchronous messaging passing between application components) and some reactive clients.</p> <p>See this guide to get started with quarkus and vert.x.</p> <p>Use the following extensions: </p> <pre><code>mvn io.quarkus:quarkus-maven-plugin:1.9.1.Final:create \\\n    -DprojectGroupId=jbcodeforce \\\n    -DprojectArtifactId=getting-started-reactive \\\n    -DclassName=\"jbcodeforce.gs.ReactiveGreetingResource\" \\\n    -Dpath=\"/hello\" \\\n    -Dextensions=\"resteasy-mutiny, vertx, resteasy-jsonb\"\n# Or using the add extension\nmvn quarkus:add-extension -Dextension=resteasy-mutiny \n</code></pre> <p></p> <p>Get access to Vert.x via injection in any beans (different Vertx classes are available depending of the api used for reactive programming): </p> <pre><code>@Inject io.vertx.mutiny.core.Vertx vertx;\n</code></pre> <p>To asynchronously handle the HTTP request, the endpoint method must return a <code>java.util.concurrent.CompletionStage</code> or an <code>io.smallrye.mutiny.Uni</code> or <code>Multi</code> from the mutiny framework:</p> <pre><code>@GET\n@Produces(MediaType.TEXT_PLAIN)\npublic Uni&lt;String&gt; doSomethingAsync() {\n    // Mimic an asynchronous computation.\n    return Uni.createFrom()\n            .item(() -&gt; \"Hello!\")\n            .onItem().delayIt().by(Duration.ofMillis(10));\n    }\n</code></pre> <p>You can deploy verticles in Quarkus. It supports:</p> <ul> <li>bare verticle - Java classes extending <code>io.vertx.core.AbstractVerticle</code></li> <li>Mutiny verticle - Java classes extending <code>io.smallrye.mutiny.vertx.core.AbstractVerticle</code></li> </ul> <p>Verticles are not beans, but adding @ApplicationScoped, can be used to access injection.</p> <p>Here is an example on how to deploy a verticle from another application scoped bean, when the application starts. </p> <pre><code>public void init(@Observes StartupEvent e, Vertx vertx, MyBeanVerticle verticle) {\n         vertx.deployVerticle(verticle).await().indefinitely();\n    }\n</code></pre> <p>For reactive routes, we need to add vertx-web. And then add  @Route annotation on method to indicate it is a reactive route. The method gets a <code>RoutingContext</code> as a parameter from which we can get the HTTP request and response objects. You can also register your route directly on the HTTP routing layer by registering routes directly on the Vert.x Router object. A reactive route must be non-blocking.</p> <pre><code>\n</code></pre> <p>Reactive route and JaxRS resource can be combined. Every operations which may take long processing time can be asynchronous with callback.</p>"},{"location":"java/vertx/#vertx-reactive-with-database","title":"Vertx reactive with database","text":"<p>Get a service loading entity from postgresql using reactive client.</p> <pre><code>public static Uni&lt;Fruit&gt; findById(PgPool client, Long id) {\n        return client.preparedQuery(\"SELECT id, name FROM fruits WHERE id = $1\").execute(Tuple.of(id))\n                .onItem().apply(RowSet::iterator)\n                .onItem().apply(iterator -&gt; iterator.hasNext() ? from(iterator.next()) : null);\n    }\n</code></pre>"},{"location":"java/vertx/#vertx-body-of-knowledge","title":"Vert.x body of knowledge","text":"<ul> <li>Grace Jansen's Getting started with Reactive Systems</li> <li>[Building reactive microservice in java - Clement Escoffier]</li> <li>baeldung Vertx</li> <li>Quarkus using Eclipse Vert.x</li> <li>Hazelcast and Vert.x</li> </ul>"},{"location":"leadership/7_habits/","title":"The 7 Habits of Highly Effective People","text":"<p>If you want to achieve your highest aspirations and overcome your greatest challenges, identify and apply the principles or natural laws that govern the results you seek.</p>"},{"location":"leadership/first-90-days/","title":"The First 90 Days,","text":"<p>From book of Watkins, Michael. </p>"},{"location":"leadership/first-90-days/#accelerate-your-learning","title":"Accelerate your learning","text":"<p>The first task in making a successful transition is to accelerate your learning:</p> <ol> <li>Adapt to roadblocks: learn about the organization\u2019s culture and politics to socialize and customize your approach.</li> <li>Managing Learning as an Investment Process: select actionable insights (knowledge that enables you to make better decisions earlier and so helps you quickly reach the break-even point in personal value creation) from all the learning. </li> <li>Define your learning agenda: Start by generating questions about the past, the present, and the future. See template: Organization learning template</li> <li>Identifying the Best Sources of Insight: listen to key people both inside and outside the organization. Interview: customer, staff\u2026 See template: Organization learning - template</li> </ol>"},{"location":"leadership/first-90-days/#new-organization-diagnostic","title":"New organization diagnostic:","text":"<p>One to one meeting with reportes. Ask 5 questions:</p> <ul> <li>What are the biggest challenges the organization is facing (or will face in the near future)? </li> <li>Why is the organization facing (or going to face) these challenges?</li> <li>What are the most promising unexploited opportunities for growth? </li> <li>What would need to happen for the organization to exploit the potential of these opportunities? </li> <li>If you were me, what would you focus attention on?</li> </ul> <p>How effective are you at learning about new organizations?  Do you sometimes fall prey to the action imperative? To coming in with \u201cthe\u201d answer? If so, how will you avoid doing this? </p> <p>What is your learning agenda? Based on what you know now, compose a list of questions to guide your early inquiries. If you have begun to form hypotheses about what is going on, what are they, and how will you test them? </p> <p>Given the questions you want to answer, who is likely to provide you with the most useful insights? </p> <p>How might you increase the efficiency of your learning process? What are some structured ways you might extract more insight for your investment of time and energy? </p> <p>What support is available to accelerate your learning, and how might you best leverage it? </p> <p>Given your answers to the previous questions, start to create your learning plan.</p>"},{"location":"leadership/first-90-days/#match-strategy-to-situation","title":"Match strategy to situation","text":"<p>To take charge successfully, you must have a clear understanding of the situation you are facing and the implications for what you need to do and how you need to do it.</p> <p>Two fundamental questions:</p> <ol> <li> <p>What kind of change am I being called upon to lead? </p> <ul> <li>Only by answering this question will you know how to match your strategy to the situation.</li> </ul> </li> <li> <p>What kind of change leader am I? </p> <ul> <li>the answer has implications for how you should adjust your leadership style.</li> </ul> </li> </ol> <p>The STARS Model defines a framework of situation assessment.</p> <p></p> <p>You\u2019re unlikely to encounter a pure of a STARS situation. At a high level your situation may fit into one of these categories. But as soon as you drill down, you will almost certainly discover that you\u2019re managing a portfolio\u2014of products, projects, processes, or people\u2014that represents a mix of STARS situations.</p> <p>Identify the elements (process, product, project, perhaps even complete businesses) in your new responsibilities fall into the various STARS situations .  You do not need to have something in every category. Everything may be in turnaround, or it may be a mix of two or three types. Then use the third column to estimate the percentage of your effort that should be allocated to each category in the next 90 days. See template: </p> STARS evaluation Elements % Assignement Startup Turnaround Accelerate Growth Realignement Sustain Success <p>In turnarounds, leaders must move people out of a state of despair. Early win may come from raising people\u2019s awareness of the need for change, by providing facts and figures.</p>"},{"location":"leadership/first-90-days/#negotiating-success","title":"Negotiating success","text":"<p>Negotiating success means proactively engaging with your new boss to shape the game so that you have a fighting chance of achieving desired goals.</p>"},{"location":"leadership/first-90-days/#how-do-you-build-a-productive-relationship-with-a-new-boss","title":"How do you build a productive relationship with a new boss?","text":"<ul> <li>Don\u2019t stay away. If you have a boss who doesn\u2019t reach out to you, or with whom you have uncomfortable interactions, you will have to reach out yourself.</li> <li>Get on your boss\u2019s calendar regularly. </li> <li>Be sure your boss is aware of the issues you face and that you are aware of her expectations, especially whether and how they\u2019re shifting.</li> <li>Don\u2019t surprise your boss. It\u2019s no fun bringing your boss bad news. However, most bosses consider it a far greater sin not to report emerging problems early enough. Worst of all is for your boss to learn about a problem from someone else.</li> <li>Don\u2019t approach your boss only with problems. Ask your reports:  \u201cDon\u2019t just bring me problems, bring me plans for how we can begin to address them.\u201d</li> <li>Don\u2019t run down your checklist to your boss. Use only three things you need to work on and need help.</li> <li>Don't expect your boss to change. it\u2019s your responsibility to adapt to your boss\u2019s style; you need to adapt your approach to work with your boss\u2019s preferences.</li> </ul>"},{"location":"leadership/first-90-days/#clarify-expectations-early-and-often","title":"Clarify expectations early and often.","text":"<p>Begin managing expectations from the moment you consider taking a new role. Focus on expectations during the interview process. You are in trouble if your boss expects you to fix things fast when you know the business has serious structural problems. It\u2019s wise to get bad news on the table early and to lower unrealistic expectations. Then check in regularly to make sure your boss\u2019s expectations have not shifted. Revisiting expectations is especially important if you\u2019re onboarding from the outside and don\u2019t have a deep understanding of the culture and politics. </p>"},{"location":"leadership/first-90-days/#take-100-percent-responsibility-for-making-the-relationship-work","title":"Take 100 percent responsibility for making the relationship work.","text":"<p>This is the flip side of \u201cDon\u2019t stay away.\u201d Don\u2019t expect your boss to reach out or to offer you the time and support you need. It\u2019s best to begin by assuming that it\u2019s on your shoulders to make the relationship work. If your boss meets you partway, it will be a welcome surprise.</p>"},{"location":"leadership/first-90-days/#negotiate-time-lines-for-diagnosis-and-action-planning","title":"Negotiate time lines for diagnosis and action planning.","text":"<ul> <li>Don't start firefighting</li> <li>Diagnose the new organization and come up with an action plan. </li> <li>Aim for early wins in areas important to the boss. What are his priorities and goals, and how do your actions fit into this picture? </li> <li>focus on three important things to your boss and discuss what you\u2019re doing about them every time you interact. In that way, your boss will feel ownership of your success.</li> </ul>"},{"location":"leadership/first-90-days/#pursue-good-marks-from-those-whose-opinions-your-boss-respects","title":"Pursue good marks from those whose opinions your boss respects.","text":"<ul> <li>Simply be alert to the multiple channels through which information and opinion about you will reach your boss.</li> </ul>"},{"location":"leadership/first-90-days/#plan-for-five-conversations-within-the-90-days","title":"Plan for five conversations within the 90 days.","text":"<ul> <li>The situational diagnostic conversation to assess the STARS as seen by the boss. Build the stars map before meeting, have a clear learning plan and work effort % per element, bullet list how the boss can help</li> <li> <p>The expectation conversation. Assess expectations, short and medium terms. What will constitute success? Critically, how will your performance be measured? When?</p> <ul> <li>Closely align your expectations with your shared assessment of the situation. </li> <li>Aim for Early Wins in Areas Important to Your Boss.</li> <li>Know the things own by your boss that are untouchables.</li> <li>Underpromise and Overdeliver to get credibility. </li> <li>don\u2019t let key issues remain ambiguous. Ambiguity about goals and expectations is dangerous.</li> </ul> </li> <li> <p>The resource conversation. What do you need to be successful? What do you need your boss to do? Key here is to focus your boss on the benefits and costs of what you can accomplish with different amounts of resources.</p> <ul> <li>you must have agreement with your boss on your STARS portfolio and associated goals and expectations.</li> <li>back them up with as much hard data as you can get, and prepare to explain exactly why you see certain resources as essential.</li> <li>Better to push hard then to bleed deeply</li> </ul> </li> <li> <p>Apply the Technics of effective negotiation</p> <ul> <li>Focus on underlying interests.  understand the agendas of your boss and any others from whom you need to secure resources.</li> <li>Look for mutually beneficial exchanges. Seek resources that both support your boss\u2019s resource owner agenda and advance your own.</li> <li>Link resource to results.</li> </ul> </li> <li> <p>The style conversation is about how you and your new boss can best interact on an ongoing basis. </p> <ul> <li>What forms of communication does he prefer, and for what? Face-to-face? Voice, electronic? </li> <li>How often?</li> <li>What kinds of decisions does he want to be consulted on, and when can you make the call on your own?</li> <li>How do your styles differ, and what are the implications for the ways you should interact?</li> <li>understand what it takes to build a productive working relationship.</li> <li>Scope Out the Dimensions of Your decision box. This is the confort zone of the boss to take decisions. What sorts of decisions does she want you to make on your own but tell her about? Are you free, for example, to make key personnel decisions? When does she want to be consulted before you decide?</li> <li>Adapt to boss style. If there is a problem on style difference, asses if you can change, or address the issue with a meeting. One proven strategy is to focus your early conversations on goals and results instead of how you achieve them.</li> </ul> </li> <li> <p>The personal development conversation. Once you\u2019re a few months into your new role, you can begin to discuss how you\u2019re doing and what your developmental priorities should be.</p> <ul> <li>Where are you doing well?</li> <li>In what areas do you need to improve or do things differently? </li> <li>Are there projects or special assignments you could undertake (without sacrificing focus)?</li> </ul> </li> <li> <p>How might you use the five conversations framework to accelerate the development of your team? Where are you in terms of having the key conversations with each of your direct reports? What kinds of guidance and support do you give them? </p> </li> </ul>"},{"location":"leadership/first-90-days/#secure-early-win","title":"Secure early win","text":"<p>Executives plan and implement change in distinct waves: early learning, transition (with early wins), immersion, reshaping, consolidation.  Your early wins should advance longer-term goals. Each wave should consist of distinct phases: learning, designing the changes, building support, implementing the changes, and observing results.</p> <ul> <li> <p>What do you need to do during your transition to create momentum for achieving them?</p> <ul> <li>new leader tailors early initiatives to build personal credibility, establish key relationships, and identify and harvest low-hanging fruit\u2014the highest-potential opportunities for short-term improvements in organizational performance.</li> <li>Focus on business priorities.</li> </ul> </li> <li> <p>How do people need to behave differently to achieve these goals?</p> <ul> <li>Identify and support behavioral changes  See template: Problematic behavior patterns</li> <li>Describe as vividly as you can the behaviors you need to encourage and those you need to discourage. </li> </ul> </li> <li> <p>How do you plan to connect yourself to your new organization? </p> <ul> <li>Who are your key audiences, and what messages would you like to convey to them? </li> <li>What are the best modes of engagement? </li> </ul> </li> <li> <p>What are the most promising focal points to get some early improvements in performance and start the process of behavior change?</p> </li> <li>What projects do you need to launch, and who will lead them?</li> <li>What predictable surprises could take you off track?</li> </ul>"},{"location":"methodology/lean-startup/","title":"Lean Startup Summary (Eric Ries)","text":""},{"location":"methodology/lean-startup/#vision","title":"VISION","text":"<p>Entrepreneurship requires a managerial discipline to harness the entrepreneurial opportunity we have been given. </p> <p>Lean thinking is radically altering the way supply chains and production systems are run. Among its tenets are drawing on the knowledge and creativity of individual workers, the shrinking of batch sizes, just-in-time production and inventory control, and an acceleration of cycle times. The Lean Startup adapts these ideas to the context of entrepreneurship, proposing that entrepreneurs judge their progress differently from the way other kinds of ventures do. </p> <p>The goal of a startup is to figure out the right thing to build, the thing customers want and will pay for, as quickly as possible. In other words, the Lean Startup is a new way of looking at the development of innovative new products that emphasizes fast iteration and customer insight, a huge vision, and great ambition, all at the same time. </p> <p>Every new version of a product, every new feature, and every new marketing program is an attempt to improve the startup engine of growth. </p> <p>Many startup business plan prescribes the steps to take and the results to expect in excruciating detail, and as in planning to launch a rocket, they are set up in such a way that even tiny errors in assumptions can lead to catastrophic outcomes. </p> <p>The Lean Startup method is designed to make constant adjustments with the Build-Measure-Learn feedback loop. We can learn when and if it\u2019s time to make a sharp turn called a pivot or whether we should persevere along our current path. </p> <p>Startups have a destination in mind: creating a thriving and world-changing business. It is a startup\u2019s vision. To achieve that vision, startups employ a strategy, which includes a business model, a product road map, a point of view about partners and competitors , and ideas about who the customer will be. The product is the end result of this strategy. </p> <p>In  general management, a failure to deliver results is due to either a failure to plan adequately or a failure to execute properly. </p>"},{"location":"methodology/lean-startup/#1-define","title":"1-DEFINE","text":"<p>The entrepreneurial prerequisites are \u2014 proper team structure, good personnel, a strong vision for the future, and an appetite for risk taking. Anyone who is creating a new product or business under conditions of extreme uncertainty is an entrepreneur whether he or she knows it or not and whether working in a government agency, a venture-backed company, a nonprofit, or a decidedly for-profit company with financial investors. Other source: https://www.cloudave.com/1171/what-makes-an-entrepreneur-four-letters-jfdi/ </p> <p>A startup is a human institution designed to create a new product or service under conditions of extreme uncertainty. The fact that a startup\u2019s product or service is a new innovation is also an essential part of the definition and a tricky part too. Product encompasses any source of value for the people who become customers. </p> <p>Innovation: Startups use many kinds of innovation: novel scientific discoveries, repurposing an existing technology for a new use, devising a new business model that unlocks value that was hidden, or simply bringing a product or service to a new location or a previously under-served set of customers. </p> <p>Startups are designed to confront situations of extreme uncertainty. To open up a new business that is an exact clone of an existing business all the way down to the business model, pricing, target customer, and product may be an attractive economic investment, but it is not a startup because its success depends only on execution. </p>"},{"location":"methodology/lean-startup/#2-learn","title":"2- LEARN","text":"<p>Measuring progress by making sure our work proceeded according to plan, with high quality, and cost about what we had projected, lead to failure if it builds a product that nobody wants. </p> <p>\u201clearning\u201d is the oldest excuse for a failure of execution. Yet if the fundamental goal of entrepreneurship is to engage in organization building under conditions of extreme uncertainty, its most vital function is learning. We must learn the truth about which elements of our strategy are working to realize our vision and which are just crazy. We must learn what customers really want, not what they say they want or what we think they should want. We must discover whether we are on a path that will lead to growing a sustainable business. Validated learning is the process of demonstrating empirically that a team has discovered valuable truths about a startup\u2019s present and future business prospects. </p> <p>At the beginning of start-up life we want to answer: What should we build and for whom? What market could we enter and dominate? How could we build durable value that would not be subject to erosion by competition? </p> Metcalfe\u2019s law <p>The value of a network as a whole is proportional to the square of the number of participants. In other words, the more people in the network, the more valuable the network. </p> <p>Could we have learned lessons earlier if we hadn\u2019t been so focused on making the product \u201cbetter\u201d by adding features and fixing bugs? which of our efforts are value-creating and which are wasteful? This question is at the heart of the lean manufacturing revolution. </p> <p>Lean thinking defines value as providing benefit to the customer; anything else is waste.</p> <p>The effort that is not absolutely necessary for learning what customers want can be eliminated. It is called \"validated learning\" because it is always demonstrated by positive improvements in the startup\u2019s core metrics. We adopted the view that our job was to find a synthesis between our vision and what customers would accept; it wasn\u2019t to capitulate to what customers thought they wanted or to tell customers what they ought to want.</p>"},{"location":"methodology/lean-startup/#3-experiment","title":"3- EXPERIMENT","text":"<p>If you cannot fail, you cannot learn. True experiment follows the scientific method. It begins with a clear hypothesis that makes predictions about what is supposed to happen. It then tests those predictions empirically. The two most important assumptions entrepreneurs make are the \"value hypothesis\" and the \"growth hypothesis\". </p> <ul> <li>The value hypothesis tests whether a product or service really delivers value to customers once they are using it. </li> <li>For the growth hypothesis, which tests how new customers will discover a product or service, we can do a similar analysis. Once the program is up and running, how will it spread among the employees, from initial early adopters to mass adoption throughout the company? </li> </ul> <p>Find early adopters: the customers who feel the need for the product most acutely . Those customers tend to be more forgiving of mistakes and are especially eager to give feedback. </p> <p>In the Lean Startup model, an experiment is more than just a theoretical inquiry; it is also a first product. </p>"},{"location":"methodology/lean-startup/#4-steering","title":"4- STEERING","text":"<p>This Build-Measure-Learn feedback loop is at the core of the Lean Startup model. The essence of steering a startup is to focus our energies on minimizing the total time through this feedback loop. The Minimum Value Product is that version of the product that enables a full turn of the Build -Measure-Learn loop with a minimum amount of effort and the least amount of development time. Upon completing the Build-Measure-Learn loop , we confront the most difficult question any entrepreneur faces: whether to pivot the original strategy or persevere. If we\u2019ve discovered that one of our hypotheses is false, it is time to make a major change to a new strategic hypothesis. </p> <p>Planning really works in the reverse order: we figure out what we need to learn, use innovation accounting to figure out what we need to measure to know if we are gaining validated learning, and then figure out what product we need to build to run that experiment. </p>"},{"location":"methodology/lean-startup/#5-leap","title":"5- LEAP","text":"<p>Every business plan begins with a set of assumptions. It lays out a strategy that takes those assumptions as a given and proceeds to show how to achieve the company\u2019s vision. The goal is to test them as quickly as possible. </p> <p>The first challenge for an entrepreneur is to build an organization that can test these assumptions systematically. The second challenge, as in all entrepreneurial situations, is to perform that rigorous testing without losing sight of the company\u2019s overall vision. </p> <p>Many assumptions in a typical business plan are unexceptional. These are well established facts drawn from past industry experience or straightforward deductions. The others are more risky and do not have clear cut answers. A strategy can be built by assessing analog: things or questions answered by another company, and assessing antilogs which are somethings companies are doing that are in opposition with one of the assumptions. In the iPod business, antilog was Naptser with free download and so the leaps of faith was that people would pay for music they download. </p> <p>What differentiates the success stories from the failures is that the successful entrepreneurs had the foresight, the ability, and the tools to discover which parts of their plans were working brilliantly and which were misguided, and adapt their strategies accordingly. The first step in understanding a new product or service is to figure out if it is fundamentally value-creating or value-destroying. A strong lean practice: genchi gembutsu: \"go and see for yourself\u201d so that business decisions can be based on deep firsthand knowledge. </p> <p>All successful sales models depend on breaking down the monolithic view of organizations into the disparate people that make them up. </p> <p>Startups need extensive contact with potential customers to understand them, so get out of your chair and get to know them. </p> <p>The first step in this process is to confirm that your leap-of-faith questions are based in reality, that the customer has a significant problem worth solving. </p> <p>Design and the Customer Archetype: The goal of such early contact with customers is not to gain definitive answers. Instead, it is to clarify at a basic, coarse level that we understand our potential customer and what problems they have. </p> <p>With that understanding, we can craft a customer archetype, a brief document that seeks to humanize the proposed target customer. This archetype is an essential guide for product development and ensures that the daily prioritization decisions that every product team must make are aligned with the customer to whom the company aims to appeal. </p> <p>No amount of design can anticipate the many complexities of bringing a product to life in the real world. </p>"},{"location":"methodology/lean-startup/#6-test","title":"6- TEST","text":"<p>A minimum viable product (MVP) helps entrepreneurs start the process of learning as quickly as possible. It is not necessarily the smallest product imaginable, though; it is simply the fastest way to get through the Build-Measure-Learn feedback loop with the minimum amount of effort. The goal of the MVP is to begin the process of learning, not end it. Unlike a prototype or concept test, an MVP is designed not just to answer product design or technical questions. Its goal is to test fundamental business hypotheses. </p> <p>Before new products can be sold successfully to the mass market, they have to be sold to early adopters. They accept, in fact prefer, an 80 percent solution; you don\u2019t need a perfect solution to capture their interest. Early adopters use their imagination to fill in what a product is missing. </p> <p>In enterprise products, it\u2019s often about gaining a competitive advantage by taking a risk with something new that competitors don\u2019t have yet. </p> <p>Additional features or polish beyond what early adopters demand is a form of wasted resources and time. Minimum viable products range in complexity from extremely simple smoke tests (little more than an advertisement) to actual early prototypes complete with problems and missing features. Deciding exactly how complex an MVP needs to be cannot be done formulaically. It requires judgment.</p>"},{"location":"methodology/lean-startup/#the-role-of-quality-and-design-in-an-mvp","title":"THE ROLE OF QUALITY AND DESIGN IN AN MVP","text":"<p>One of the most vexing aspects of the minimum viable product is the challenge it poses to traditional notions of quality. Thus, for startups the following quality principle applies: If we do not know who the customer is, we do not know what quality is.</p> <p>Even a \u201clow-quality\u201d MVP can act in service of building a great high-quality product. Yes, MVPs sometimes are perceived as low-quality by customers. If so, we should use this as an opportunity to learn what attributes customers care about. </p> <p>As you consider building your own minimum viable product, let this simple rule suffice: remove any feature, process, or effort that does not contribute directly to the learning you seek. </p> <p>IP protection may be needed before releasing a MVP or as early as possible after. Most of the time competitors are overwhelmed and no time to chase after your idea. If a competitor can out execute a startup once the idea is known, the startup is doomed anyway. </p> <p>The reason to build a new team to pursue an idea is that you believe you can accelerate through the Build-Measure-Learn feedback loop faster than anyone else can. </p>"},{"location":"methodology/lean-startup/#7-measure","title":"7- MEASURE","text":"<p>Innovation accounting enables startups to prove objectively that they are learning how to grow a sustainable business. Innovation accounting begins by turning the leap-of-faith assumptions    into a quantitative financial model. For established companies the rate of growth depends primarily on three things:</p> <ul> <li>the profitability of each customer,</li> <li>the cost of acquiring new customers,</li> <li>the repeat purchase rate of existing customers.</li> </ul>"},{"location":"methodology/lean-startup/#how-innovation-accounting-works","title":"HOW INNOVATION ACCOUNTING WORKS","text":"<p>Innovation accounting works in three steps:</p> <ul> <li>First, use a minimum viable product to establish real data on where the company is right now</li> <li>Second, startups must attempt to tune the engine from the baseline toward the ideal. This may take many attempts.</li> <li>Pivot or preserve</li> </ul> <p>Establish the Baseline: </p> <p>A startup might create a complete prototype of its product and offer to sell it to real customers through its main marketing channel. This MVP validates all major assumptions in one run. Or develop a MVP for each assumption at a time. Before developing the full product the startup can perform a smoke test: customers are given the opportunity to pre-order a product that has not yet been built. </p> <p>A smoke test measures only one thing: </p> <p>Whether customers are interested in trying a product. An MVP allows a startup to fill in real baseline data in its growth model\u2014conversion rates, sign-up and trial rates, customer lifetime value. All being foundation for learning. When selecting among multiple assumptions, it makes sense to test the riskiest assumptions first. If you can\u2019t find a way to mitigate these risks toward the ideal that is required for a sustainable business, there is no point in testing the others.</p> <ul> <li>Agile development with sprint, user stories, backlog, small sized story, etc make a team disciplined to build product with good feedbacks. But it is important to understand if the prioritization decisions is making sense?</li> <li>Agile does not answer the startup problems: How do we know which features to prioritize? How can we get more customers to sign up and pay? How can we get out the word about our product?</li> <li>The grand bargain of agile development : engineers agree to adapt the product to the business\u2019s constantly changing requirements but are not responsible for the quality of those business decisions.</li> <li>Adopting vanity metrics such as the total number of customers and the total number of questions answered, gives the team the sensation of forward motion even though the company was making little progress.</li> <li>Changed the metrics  to evaluate success in two ways, cohort-based metrics, and a true split-test experiment. A split-test (A/B testing) experiment is one in which different versions of a product are offered to customers at the same time. By observing the changes in behavior between the two groups , one can make inferences about the impact of the different variations.</li> <li>It requires extra accounting and metrics to keep track of each variation, it almost always saves tremendous amounts of time in the long run by eliminating work that doesn\u2019t matter to customers.</li> </ul> <p>The lean manufacturing principle of Kanban, or capacity constraint, can be used to control product development:</p> <ul> <li>Under the new system, user stories were not considered complete until they led to validated learning.</li> <li>Thus, stories could be cataloged as being in one of four states of development: in the product backlog, actively being built, done (feature complete from a technical point of view), or in the process of being validated. Validated was defined as \u201cknowing whether the story was a good idea to have been done in the first place</li> <li>The kanban rule permitted only a limited number of stories in each of the four states. As stories flow from one state to the other, the buckets fill up. Once a bucket becomes full, it cannot accept more stories.</li> <li>The only way to start work on new features is to investigate some of the stories that are done but haven\u2019t been validated.</li> <li>Every feature should be split-tested. Always ask why build a new feature that is not part of a split-test experiment?</li> <li>Most important, teams working in this system begin to measure their productivity according to validated learning, not in terms of the production of new features.</li> </ul> <p>Three A\u2019s of metrics: actionable , accessible, and auditable.</p> <ul> <li>Actionable: For a report to be considered actionable, it must demonstrate clear cause and effect. Otherwise , it is a vanity metric.</li> <li>Accessible: First, make the reports as simple as possible so that everyone understands them. Remember the saying \u201cMetrics are people, too.\u201d The easiest way to make reports comprehensible is to use tangible, concrete units. The report deals with people and their actions, which are far more useful than piles of data points.</li> <li>Auditable: We must ensure that the data is credible to employees. Most data reporting systems are built by business managers and they lack a way to test if the data is consistent with reality. To solve this problem first use the report to talking to customers. This is the only way to be able to check if the reports contain true facts. Second, whenever possible, reports should be drawn directly from the master data, rather than from an intermediate system, which reduces opportunities for error.</li> </ul>"},{"location":"methodology/lean-startup/#8-pivot","title":"8-PIVOT","text":"<p>A pivot is a structured course correction designed to test a new fundamental hypothesis about the product, strategy, and engine of growth. there is no bigger destroyer of creative potential than the misguided decision to persevere. </p> <p>Startup productivity is  about aligning our efforts with a business and product that are working to create value and drive growth. In other words, successful pivots put us on a path toward growing a sustainable business. </p> <p>Deciding to pivot or persevere is one of the hardest decisions entrepreneurs face. The goal of creating learning milestones is not to make the decision easy; it is to make sure that there is relevant data in the room when it comes time to decide. The more money, time, and creative energy that has been sunk into an idea, the harder it is to pivot. Failure is a prerequisite to learning. The problem with the notion of shipping a product and then seeing what happens is that you are guaranteed to succeed, at seeing what happens. But then what ? A pivot requires that we keep one foot rooted in what we\u2019ve learned so far, while making a fundamental change in strategy in order to seek even greater validated learning. The process to deliver new MvP accelerates even if much of the product had to be discarded between pivots. Worse, the product that remained was classified as a legacy product, one that was no longer suited to the goals of the company. But still the startup accelerated its MVP process because it was learning critical things about its customers, market, and strategy. </p> <p>Runway is defined as the remaining cash in the bank divided by the monthly burn rate. Two ways to extends the Runway: get more money or cut costs. But the true measure of runway is how many pivots a startup has left: the number of opportunities it has to make a fundamental change to its business strategy. Which suggests another way to extend that runway: get to each pivot faster. </p> <p>Pivot needs courage. Three reasons that slows pivot decisions:</p> <ul> <li>Vanity metrics can allow entrepreneurs to form false conclusions and live in their own private reality</li> <li>When an entrepreneur has an unclear hypothesis, it\u2019s almost impossible to experience complete failure, and without failure there is usually no impetus to embark on the radical change a pivot requires.</li> <li>Entrepreneurs\u2019 biggest fear, is that the vision might be deemed wrong without having been given a real chance to prove itself. This fear drives much of the resistance to the minimum viable product, split testing, and other techniques to test hypotheses. Ironically, this fear drives up the risk because testing doesn\u2019t occur until the vision is fully represented. However, by that time it is often too late to pivot because funding is running out.</li> </ul> <p>Every startup have a regular \u201cpivot or persevere \u201d meeting. It requires the participation of both the product development and business leadership teams. </p> <p>When efforts at tuning the engine were reaching diminishing returns, it is the classic sign of the need to pivot. </p> <p>Different type of pivot:</p> <ul> <li>Platform pivot: for example changing the payment model from license to subscription / credit card.</li> <li>Customer segment pivot: In this pivot, the company realizes that the product it\u2019s building solves a real problem for real customers but that they are not the customers it originally designed to serve.</li> <li>Zoom-in Pivot: In this case, what previously was considered a single feature in a product becomes the whole product.</li> <li>Zoom-out Pivot: what was considered the whole product becomes a single feature of a much larger product.</li> <li>Customer Need Pivot: the product hypothesis is partially confirmed; the target customer has a problem worth solving, just not the one that was originally anticipated.</li> <li>Business architecture pivot: companies generally follow one of two major business architectures: high margin, low volume (complex systems model) or low margin, high volume (volume operations model). In a business architecture pivot, a startup switches architectures.</li> <li>Value Capture Pivot: capturing value (revenue model) is an intrinsic part of the product hypothesis. Often, changes to the way a company captures value can have far-reaching consequences for the rest of the business, product, and marketing strategies.</li> <li>Engine of growth pivot: there are three primary engines of growth that power startups : the viral, sticky, and paid growth models. In this type of pivot, a company changes its growth strategy to seek faster or more profitable growth. Commonly but not always, the engine of growth also requires a change in the way value is captured.</li> <li>Channel Pivot: the requirements of the channel determine the price, features, and competitive landscape of a product. A channel pivot is a recognition that the same basic solution could be delivered through a different channel with greater effectiveness.</li> <li>Technology pivot: company discovers a way to achieve the same solution by using a completely different technology. Technology pivots are much more common in established businesses.</li> </ul>"},{"location":"methodology/lean-startup/#accelerates","title":"Accelerates","text":""},{"location":"methodology/lean-startup/#batch","title":"Batch","text":"<ul> <li>When we do work that proceeds in stages, the \u201cbatch size\u201d refers to how much work moves from one stage to the next at a time.</li> <li>Toyota discovered that small batches made their factories more efficient.</li> <li>The biggest advantage of working in small batches is that quality problems can be identified much sooner.</li> <li>in the Lean Startup the goal is not to produce more stuff efficiently. It is to\u2014 as quickly as possible\u2014learn how to build a sustainable business.</li> <li>Working in small batches ensures that a startup can minimize the expenditure of time, money, and effort that ultimately turns out to have been wasted.</li> <li>Instead of working in separate departments, engineers and designers would work together side by side on one feature at a time. Whenever that feature was ready to be tested with customers, they immediately would release a new version of the product, which would go live on our website for a relatively small number of people. The team would be able immediately to assess the impact of their work , evaluate its effect on customers, and decide what to do next.</li> <li>IMVU makes about fifty changes to its product (on average) every single day.</li> <li>The key to being able to operate this quickly is to check for defects immediately, thus preventing bigger problems later.</li> <li>Analogously to the Toyota \"andon\" cord, IMVU used an elaborate set of defense mechanisms that prevented engineers from accidentally breaking something important.</li> <li>Continuously monitored the health of our business itself so that mistakes were found and removed automatically.</li> <li>By reducing batch size, we can get through the Build-Measure-Learn feedback loop more quickly.</li> </ul>"},{"location":"methodology/lean-startup/#growth","title":"Growth","text":"<p>The engine of growth is the mechanism that startups use to achieve sustainable growth. Sustainable growth is characterized by one simple rule: New customers come from the actions of past customers.</p> <p>There are four primary ways past customers drive sustainable growth:</p> <ol> <li>Word of mouth.</li> <li>As a side effect of product usage.</li> <li>Funded advertising: marginal cost &lt; marginal revenue</li> <li>Through repeat purchase or use.</li> </ol> <p>Three engine of growth:</p> <ul> <li>Sticky engine product: The rules that govern the sticky engine of growth are pretty simple: if the rate of new customer acquisition exceeds the churn rate, the product will grow. The way to find growth is to focus on existing customers for the product even more engaging to them. </li> <li>The Viral Engine of Growth: the viral engine is powered by a feedback loop that can be quantified. It is called the viral loop, and its speed is determined by a single mathematical term called the viral coefficient. The viral coefficient measures how many new customers will use a product as a consequence of each new customer who signs up. Companies that rely on the viral engine of growth must focus on increasing the viral coefficient more than anything else, because even tiny changes in this number will cause dramatic changes in their future prospects. In the viral engine of growth, monetary exchange does not drive new growth; it is useful only as an indicator that customers value the product enough to pay for it. By investing their time and attention in the product, they make the product valuable to advertisers.</li> <li>The Paid Engine of Growth: evaluate the cost of acquiring new customer. The engine increases the revenue from each customer or drive down the cost of acquiring a new customer. The margin between the LTV (lifetime value) and the CPA (cost per acquisition) determines how fast the paid engine of growth will turn.</li> </ul> <p>Technically, more than one engine of growth can operate in a business at a time. But successful startups usually focus on just one engine of growth. </p> <p>Only after pursuing one engine thoroughly should a startup consider a pivot to one of the others. </p> <ul> <li>Product/market fit: in a great market\u2014 a market with lots of real potential customers\u2014 the market pulls product out of the startup. A startup can evaluate whether it is getting closer to product/ market fit as it tunes its engine by evaluating each trip through the Build-Measure-Learn feedback loop using innovation accounting. What really matters is not the raw numbers or vanity metrics but the direction and degree of progress.</li> </ul>"},{"location":"methodology/lean-startup/#five-whys","title":"Five whys","text":"<p>Adaptive processes force you to slow down and invest in preventing the kinds of problems that are currently wasting time. As those preventive efforts pay off, you naturally speed up again.</p> <p>The core idea of Five Whys is to tie investments directly to the prevention of the most problematic symptoms. The system takes its name from the investigative method of asking the question \u201cWhy?\u201d five times to understand what has happened (the root cause).</p> <p>At the root of every seemingly technical problem is a human problem. Five Whys provides an opportunity to discover what that human problem might be.</p>"},{"location":"methodology/lean-startup/#how-to-use-five-whys-analysis-to-build-an-adaptive-organization","title":"How to use Five Whys analysis to build an adaptive organization","text":"<p>Consistently make a proportional investment at each of the five levels of the hierarchy.</p> <p>If the outage is a minor glitch, it\u2019s essential that we make only a minor investment in fixing it.</p> <p>The Five Whys approach acts as a natural speed regulator. The more problems you have, the more you invest in solutions to those problems. As the investments in infrastructure or process pay off, the severity and number of crises are reduced and the team speeds up again.</p> <p>Make sure that any person impacted by the problem is in the room during the analysis of the root cause. For the Five Whys to work properly, there are rules that must be followed:</p> <ul> <li>It requires an environment of mutual trust and empowerment.</li> <li>Be tolerant of all mistakes the first time.</li> <li>Never allow the same mistake to be made twice.</li> <li>Be ready to face unpleasant truth.</li> <li>Start small and be specific: starting with a narrowly targeted class of symptoms. The more specific the symptoms are , the easier it will be for everyone to recognize when it\u2019s time to schedule a Five Whys meeting.</li> <li>Appoint a five whys master: he is the moderator for each Five Whys meeting, making decisions about which prevention steps to take, and assigning the follow-up work from that meeting.</li> </ul>"},{"location":"methodology/lean-startup/#innovation","title":"Innovation","text":"<p>Startup teams require three structural attributes: </p> <ul> <li>scarce but secure resources, </li> <li>independent authority to develop their business, </li> <li>a personal stake in the outcome.</li> </ul> <p>Whenever possible, the innovation team should be cross-functional and have a clear team leader. Those who look to adopt the Lean Startup as a defined set of steps or tactics will not succeed.</p>"},{"location":"methodology/narrative/","title":"Narratives","text":"<p>Writing narratives has proven to be  tool that drives positive business outcomes. It includes 2 parts the narrative and the appendices (documents to support the narratives). </p> <p>Narratives part is to define, describe the details of a strategy, a problem, the solution or initiative. They are part o the decision making process at Amazon.</p> <p>when to use it: </p> <ul> <li>to present a project / problem and action plan</li> <li>sharing an idea for sign-off</li> <li>Dive deep into a topic</li> </ul> <p>The purpose is to clarify your thinking and inform others. Or to ask for support or funding. Decide whether to prioritize / take action on a problem. Manage action items. Documenting for posterity.</p> <p>Two formats: one pagers or 6 pagers. Make as short as possible and as long as necessary. Reader spent 5 minutes per page.</p> <ul> <li>one pagers: this is a brief narrative to communicate high-level goals, tenets, and design of a project. Be very crisp about the value of a project. Reader understand the project, evaluate its benefits and risks and makes high-level decisions</li> <li>six pagers: 6 pages but unlimited appendices. Be concise and actionable. Includes group or project tenets,  Document accurate information in order to make good decisions.</li> </ul> <p>Address</p> <ul> <li>Why reader should care about this doc?</li> <li>What does reader need to know to make decision?</li> <li>Why should reader already know and not know?</li> </ul> <p>Narratives tell a story.</p>"},{"location":"methodology/narrative/#purpose","title":"Purpose","text":"<p>What is the objective of your document? What is the \"bottom line\"? What do you want from the reader?</p> <p>Executive summary is a more extensive version of the purpose or objective. ( may be found in 6-pagers)</p>"},{"location":"methodology/narrative/#background-context","title":"Background context","text":"<p>The length depends of what the audience already knows. Senior leaders need more context. </p> <p>Look at:</p> <ul> <li>How closer the reader to your customer? Do they know your customer as well as you do?</li> <li>What background information do they need to know to understand the problem and want to support your solutions?</li> <li>How familiar is your team and its charter to your reader? </li> </ul> <p>Get details about what the current process is, or what the team is doing, vision, mission and goals.  Give performance year to date with goals status. </p>"},{"location":"methodology/narrative/#problem-and-opportunity","title":"Problem and opportunity","text":"<p>We need to spend a lot of time understanding what the customer problem is. </p> <p>Be sure to tell the reader why they should care. If they do not believe there is a problem, you're going to struggle to sell your recommendations. </p> <ul> <li>Define who is the customer</li> <li>What problems customer is facing</li> <li>The impact the problem is having</li> <li>The cause of the problem</li> <li>Describe why the process needs to change. </li> <li>List the problems identified and how they are impacting customers now, and how they will continue to impact customers if not resolved.</li> <li>Use Data to quantify the problem.</li> </ul>"},{"location":"methodology/narrative/#recommendations-and-strategy","title":"Recommendations and strategy","text":"<p>Think Big and invent and simplify. Do your best to brainstorm ideas, measuring how well they solve the problem and making some form of recommendations.</p> <p>Be sure to include what will be implemented, what would be the predicted output, what long term change, outcome will happen. </p> <p>Recommendations will vary depending on the complexity of the problem and the availability of the resources to solve problems.</p> <p>Give multiple solutions and make a recommendation for which would work best and why. Includes the recommendations disregarded and reasons why. </p> <p>For each problem outline the recommendation with pros and cons. </p> <p>During narrative review, keep the discussion on the recommendations, decide together on which would work best. This meeting may not reach conclusion so define items and owner. </p>"},{"location":"methodology/narrative/#next-steps","title":"Next steps","text":"<p>Outline what the next steps are for implementing the recommendation and requests for support. List actions to be taken by whom. List any conflicting dates or events which impact implementation. Define how success will be measured and tracked.  List any blockers to remove.</p>"},{"location":"methodology/narrative/#conclusion","title":"Conclusion","text":"<p>Similar to the executive summary. What is expected of the reader now they've read the doc. Approval, advice, alignment...</p> <ul> <li>A summary of the problem</li> <li>A summary of the recommendation(s)</li> <li>A summary of the next steps</li> </ul>"},{"location":"methodology/narrative/#appendices","title":"Appendices","text":"<ul> <li>list what supporting data, visuals, to support statement done in the narrative</li> <li>predict some hard questions from the reader. </li> <li>FAQs are useful when proposing controversial recommendation.</li> <li>Put them in the order, they are referred to in the main document</li> </ul>"},{"location":"methodology/pm/","title":"Product management role","text":"<p>A product manager is the CEO of a product: set the vision, the product strategy, and is at the intersection of product, customer and business. It is a business function.</p>"},{"location":"methodology/pm/#producer-managers-work","title":"Producer Manager's work","text":"<ol> <li>Talk to customers. Survey, problems discovery, validate ideas with prototyping.</li> <li>Understand the market, research market. Understand the competitors.</li> <li>Create stories and product requirements.</li> <li>To the team, PM is the voice of the customers, but not the users. Define roadmap and feature backlog.</li> <li>Address what needs to be built, why and when it will be delivered. Create roadmaps, feature backlogs with prioritization.</li> <li>Work cross-functionally with development and business.</li> <li>Cost / business case - justification. Define success criterias. Define metrics of product usage.</li> <li>Story teller.</li> </ol>"},{"location":"methodology/pm/#project-manager-product-manager","title":"Project manager / Product manager","text":"<ul> <li> <p>Project manager</p> <ul> <li>Manage requirements</li> <li>Create project plans, develop schedules</li> <li>Estimate time and costs</li> <li>Monitor project progress and control</li> <li>Analyze and manage risks</li> <li>Document project vision, stakeholders, assumptions, outcome...</li> <li>Communicate with project stakeholders</li> <li>Close of and sign off.</li> </ul> </li> <li> <p>Product Manager (See above)</p> </li> </ul>"},{"location":"methodology/pm/#getting-start","title":"Getting Start","text":"<ul> <li> <p>Discover and ideation</p> <ul> <li>User research: talk to users and confirm hypothesis</li> <li>Define the product vision or new vision</li> <li> <p>Define</p> </li> <li> <p>Create a business plan, how much will it cost? What benefits with the project bring to the team?</p> </li> <li>Create the roadmap, what are the milestones of this project.</li> </ul> </li> <li> <p>Prototype</p> <ul> <li>Prototype to validate concepts</li> <li>Define the MVP, what is the first iteration released to the users.</li> </ul> </li> </ul>"},{"location":"methodology/pm/#business-plan","title":"Business plan","text":""},{"location":"methodology/pm/#roadmap","title":"Roadmap","text":"<ul> <li>Align customer's pains and needs to product feature to define priority</li> <li>Aligning the product roadmap with the overall business strategy and goals.</li> <li>Rely on data and analytics to inform product roadmap decisions. Use various metrics, user behavior data, and market trends to assess the performance and impact of existing product features.</li> <li>Define PR/FAQ</li> <li>Define the roadmap using agile approach, breaking down the product roadmap into smaller, manageable user stories. </li> <li>Regularly evaluate and reassess the roadmap to ensure it remains aligned with the evolving needs of customers and the business.</li> </ul>"},{"location":"methodology/pm/#mastering-design-thinking","title":"Mastering Design thinking","text":"<p>Design thinking is based on logic, imagination, intuition, and systemic reasoning to explore the possibilities of what could be and create desired results that benefit the end-user (customer). It can, and should be used in the context of product management to better understand the end-users.</p> <p>Design thinking is a human-centered, iterative, problem-solving, solution-based framework.</p> <p>Iterative means that you will use the results to review, question, and upgrade any primary assumptions, understandings, and outcomes at the various stages of the design thinking process.</p> <p>It encourages inclusive, multidisciplinary collaboration to exploit other people's skills, backgrounds, and thought styles.</p>"},{"location":"methodology/pm/#five-stages","title":"Five stages","text":"<ul> <li>Stage 1 \u2013 Empathize: This stage's goal is to gain an empathic understanding of your users, their needs, and what they care for. You should observe, engage (e.g. through interviews) and empathize with people to understand their experiences, what they value and what motivates them.</li> <li>Stage 2 \u2013 Define (the Problem): At this stage, the information you have collected in the first stage should be analyzed, sorted out, and sequenced in such a way that you can better define the problem you are dealing with.</li> <li> <p>Stage 3 \u2013 Ideate: You will start generating some rational concepts that seek to solve the problem by using the Stage 2 problem statement. Think out of the box. Generate ideas for future prototyping.</p> </li> <li> <p>Stage 4 \u2013 Prototype: During this point, you will work with your team to create some cheap models to examine and test the potential solutions that have been suggested so far. Prototype acts as interactions with users to get feedback. You don't try to find the right solution. Each solution is prototyped, tested, approved, improved, r\u0435-examined or rejected.</p> </li> <li> <p>Stage 5 \u2013 Test: The successful prototyping approaches were tested using developers, evaluators, and actual customers in the sense of the real product. Testing is conducted to assess each model and to determine the degree to which it solves the problem being discussed. The results of these tests can be used to refine the problem, the prototypes proposed, and the solutions.</p> </li> </ul> <p>Design thinking can be used in large and small enterprises. The goal is to move away from an established way of thinking and approach a problem or project with the mentality of a designer.</p> <p>Design thinking combines creative and critical thinking that allows organizing information and ideas, making decisions, improving situations, and gaining knowledge. It is a solution-focused mindset, not the problem.</p>"},{"location":"methodology/pm/#skills","title":"Skills","text":"<ul> <li>Empathy, use design thinking empathy map as a tool to understand end user's pains, what they say, think, do.</li> <li>Be data driven: customer survey, pricing study.</li> <li>Working cross-functionally and leading without authority.</li> <li>Dealing with ambiguity</li> <li>Customer obsession and communication (story teller)</li> </ul>"},{"location":"methodology/pm/#sources","title":"Sources","text":"<ul> <li>YouTube enablement</li> <li>Lean startup</li> </ul>"},{"location":"methodology/abrd/","title":"Decision Service Development with Agile Methodology","text":""},{"location":"methodology/abrd/#introduction","title":"Introduction","text":"<p>The Decision Service Development with Agile Methodology which was developed in 2004 at ILOG, has been widely utilized on numerous projects worldwide to incorporate decision management into business applications. This methodology is based on the book written by Hafedh Mili and Jerome Boyer's book. The content discussed in the book is not specific to any particular product and supports an incremental and iterative approach to developing decision services. The methodology begins with the identification of a business operation improvement idea, which is then evaluated against a set of criteria to determine the suitability of utilizing a Decision Management System. Subsequently, it addresses all the necessary project implementation tasks.</p>"},{"location":"methodology/abrd/#goals","title":"Goals","text":"<p>The goals of the Decision Service Development (DSD) methodology can be summarized as follows:</p> <ol> <li>Separate rules as a manageable artifact: The method aims to break down business rules into manageable components through activities such as discovery, analysis, and authoring. This allows for better organization and understanding of the rules.</li> <li>Traceability throughout the rule lifecycle: DSD emphasizes the need to trace rules from their initial requirements all the way to deployment and maintenance. This ensures transparency and accountability in rule management.</li> <li>Linking rules to business context and motivation: DSD focuses on establishing a clear connection between rules and the broader business context and motivation behind them. This helps stakeholders understand the purpose and rationale behind each rule.</li> <li>Rule description using business terms and high-level rule language: DSD promotes the use of business terms and a high-level rule language to describe rules. This facilitates communication between business stakeholders and technical teams, making it easier to capture and implement rules accurately.</li> <li>Preparation of the fact model: DSD emphasizes the importance of preparing a fact model, which represents the relevant business entities and their relationships. This ensures that the rule implementation aligns with the underlying business structure.</li> <li>Implementation and deployment of rule sets: DSD supports the implementation and deployment of rule sets as decision services within a Service-Oriented Architecture (SOA) or as Micro Services executed by a rule engine. This enables efficient execution and management of the rules.</li> <li>Articulation of rule governance processes: DSD recognizes the need for rule governance processes to ensure consistent management, maintenance, and monitoring of rules. This includes defining roles, responsibilities, and procedures for rule governance.</li> </ol> <p>By aligning with these goals, the methodology aims to streamline the development and implementation of business policies as executable rules, enabling faster and more efficient rule execution and maintenance.</p>"},{"location":"methodology/abrd/#concepts","title":"Concepts","text":"<p>To get a good understanding of business rules, rule engines and other terms used in this content, see this note</p>"},{"location":"methodology/abrd/#how-to-adopt-abrd","title":"How to Adopt ABRD","text":"<p>BRMS enable managing business rule as a standalone artifact, owned by the business user, and maintainable over time into production system. The implementation of a business rule application follows some activities and tasks that are slightly different than traditional software development life cycle. The integration of one or more business analysts as part of the development team is also less traditional. Finally the core value of such technology is to be able to have business user maintaining the business rules in production with a minimum involvement of IT. Technology is one side of the coin, methodology and best practices is the other.</p> <p>A Project Management Office can leverage ABRD as-is or integrate it as content for their own methodology.</p> <p>If the team is new to business rule application, it is best to start with a small rule set and incrementally add rules and best practices over time. The team has to integrate the rule discovery and analysis activities in their own project plan.</p> <p>Prototyping or Minimum Viable Product is a major approach, as it shows to the team concrete execution, and helps to drive issues, and requirements around business rules and even business process.</p>"},{"location":"methodology/abrd/#common-pitfalls","title":"Common Pitfalls","text":"<p>The most common pitfalls in implementing business rules application include: * Starting by harvesting all the rules in a document without any implementation * Ignoring the importance of the data model * Not understanding where the rules are (should be) enforced * Mixing the data model from the implementation point of view, with the domain model that is used by the business * Involving IT only: never forget the business, involve them as early as possible and make them take ownership of the rules. * Outsourcing business rule implementation: business rules are enterprise assets. * Forgetting to test the rules outside of the application * Not involving the business in the rule validation process * Badly designing a rule set, by not applying standard design pattern as separation of concerns.</p>"},{"location":"methodology/abrd/#incremental-development","title":"Incremental Development","text":"<p>The following diagram illustrates how a decision service implementation is built from a business idea to an executable SOA service integrated with a BPM system or any consumers.</p> <p></p> <p>Within those phases we can find project activities that are executed multiple times during each phase:</p> <ul> <li>Rule discovery</li> <li>Rule analysis</li> <li>Fact model implementation</li> <li>Rule implementation and unit test</li> <li>Rule validation</li> <li>Rule deployment</li> </ul> <p>Like all agile methodologies, ABRD prescribes an incremental development using time boxed iterations. However, in ABRD, the entire process lifecycle may not be followed for each iteration. Instead, a number of very short cycles between phases may occur, with the entire ruleset gradually evolving.</p> <p>Using a business rule management platform enables managing business rule as a standalone artifact, owned by the business user, and maintainable by them directly  time into production system. The implementation of a business rule application follows some activities and tasks that are slightly different than traditional software development life cycle. The strong involvement of one or more business rule analysts as part of the development team is recommended to harvest the rule while they are implemented. Technology is one side of the coin, methodology and best practices is the other. Executing a project to implement a solution using rule engine and a BRMS follows the following progression as illustrated</p> <p></p> <p>A Project Management Office can leverage ABRD as-is and integrate it within their own methodology or tailor it by injecting their own practices. As open source you can also contribute to it.</p> <p>If the team is new to business rule application, it is best to start with a small rule set and incrementally add rules and best practices over time. The team has to integrate the rule discovery and analysis activities in their own project plan.</p> <p>Prototyping is a major value, as it shows to the team concrete execution, and helps to drive issues, and requirements around business rules and even business process.</p>"},{"location":"methodology/abrd/#common-pitfalls_1","title":"Common Pitfalls","text":"<p>The most common pitfalls in implementing business rules application include:</p> <ul> <li>Starting by harvesting all the rules in a document</li> <li>Ignoring the importance of the data model</li> <li>Not understanding where the rules are (should be) enforced</li> <li>Mixing the data model from the implementation point of view, with the domain model that is used by the business</li> <li>Involving IT only: never forget the business, involve them as early as possible and make them take ownership of the rules.</li> <li>Outsourcing business rule implementation: business rules are enterprise assets.</li> <li>Forgetting to test the rules outside of the application</li> <li>Not involving the business in the rule validation process</li> <li>Badly designing a rule set, by not applying standard design pattern as separation of concerns.</li> </ul>"},{"location":"methodology/abrd/#cycle-approach","title":"Cycle Approach","text":"<p>Like all agile methodologies, ABRD prescribes an incremental development using time boxed iterations. However, in ABRD, the entire process lifecycle may not be followed for each iteration. Instead, a number of very short cycles between phases may occur, with the entire ruleset gradually evolving. The entire process may be represented by the diagram below:</p> <p></p> <p>In the process flow diagram above, each of the task loops back to previous task in the flow. These 'loops', or cycles, are repeated as required, with the loops gradually moving from left to right through the process until the rule sets are deployed to production environment. This method of rule development reflects the nature of BRMS: a mature and stable domain object model is a prerequisite for rule development, therefore we may multiply cycles through the early part of the process flow in order to develop such models early in the process. As each of these cycles is completed, the ruleset will gradually grow until it reaches a state where it reflects the requirements set forth by the business.</p> <p>The phases and activities apply for both operational, inference business rules as event processing rules. The event processing rule will, most likely require different technologies: a BRMS and a CEP platform, but the development methodology is similar.</p>"},{"location":"methodology/abrd/#a-typical-abrd-project","title":"A typical ABRD Project","text":"<p>From a project management perspective, there are four phases, each of which maps onto parts of the technical process flow described above. The project phases are:</p> <ol> <li>Harvesting, the activity of gathering business rules</li> <li>Foundation, the activity of defining the decision service interface, to develop the domain specific data model: the Rule Business Object, to implement the different business rule pattern into the BRMS, develop the unit tests</li> <li>Building, the activity of building a working decision service, representing the organizations business rules, it includes integration with the decision service consumers or the event producers and any reference data management sources.</li> <li>Enhancement groups completing the business logic by adding more rules, and start the rule governance process, the decision logic monitoring, and maintaining and enhancing business rules. This specific phase should be done with less involvement from IT, and with modern BRMS, business analysts can support the full cycle of authoring rules, validating them, and hot deploy to production.</li> </ol>"},{"location":"methodology/abrd/#cycle-1-harvesting","title":"Cycle 1- Harvesting","text":"<p>Rule harvesting, consisting of the Discovery &amp; Analysis tasks, involves gathering together all the sources of knowledge available to the team. These sources may include a business process description, subject matter experts, policy manuals, existing code or other sources of rules. The goal of this activity is to understand the Domain Object Model within the scope of the application and to identify enough rule patterns to begin implementation of rules in the next phase.</p> <p>During this activity, the development teams divide the day into two parts, executing discovery workshops in the morning (in 2 or 3-hour session), then performing analysis and documentation for the remainder of the day. The team iterates on these two steps during 2 to 5 days maximum, depending on the number of rule pattern and their complexity.  </p> <p></p> <p>The starting point of the Rule Discovery is a Decision Point Table artifact: During the inception phase (See OpenUP for more information) the project team is doing business modeling activities (not covered here) which aim at describing the business process and decisions applied to any business events supported by the application. One important work product built during this modeling phase is the Decision Point Table which describes the point in the process (task, activities, transition) where decisions are made. Decision point represents potential candidate for rule sets.</p>"},{"location":"methodology/abrd/#cycle-2-building-foundations","title":"Cycle 2- Building Foundations","text":"<p>The building foundation extends the tasks performed during harvesting to include rule business object implementation and rule authoring. When a workable number of rules (this number will vary by project) have been harvested the development team should start to define the structure of the rule project: the decision service interface, the rule set variables (input-output business objects), the basic sequencing of the rules (the Rule Flow), and the major elements of the Business Object Model. The team should then be able to implement and unit test some rules.</p> <p>A best practice is to execute the step \"Rule Authoring\" as soon as possible to uncover possible analysis and design issues. Indeed, most of the rules look good on paper but real issues will surface during implementation and test. The next morning workshop session the rule developer should communicate the issues back to the business team to quickly address the change to the business logic. This leverages the feedback loop approach and provides an efficient mechanism to build a pragmatic, adequate and business-relevant executable rule set.</p> <p></p>"},{"location":"methodology/abrd/#cycle-3-building","title":"Cycle 3- Building","text":"<p>The goal of the build phase is to create rules that can be 'seen', 'used' and, most importantly, tested, by business users. Rules that can be executed by the rule engine are more valuable than ones defined on paper or in abstract forms. This fully endorses the agile statement:\"Working software over comprehensive documentation.\"</p> <p>The second goal of this phase is to create a set of realistic test scenarios, using real data when possible, to test the rules created during the Authoring activity. This is a form of Test Driven Development (TDD). This activity extends the Build activity by adding a Rule Validation task.</p> <p></p> <p>The day-to-day authoring activities consist of a series of small steps that include: * Test case implementation * Writing rules and executing rules * Validating rules with the business team members * Deploy automatically to the target test platform so decision services are consumable</p> <p>The build activity consists of all the previous tasks, with short iterations: * Enhance rules (Authoring &amp; Validation tasks) * Improve rules (Analysis, Authoring &amp; Validation) as needs are identified * Complement rules (Discovery, Analysis, Authoring &amp; Validation) with additional rules to provide complete coverage of the business domain (once every two days).</p> <p>The Building activity should take between two and three weeks iteration, and the output artifacts should be a ruleset that can be loaded into the rule engine and executed.</p> <p>Checkpoint:</p> <pre><code>    During the first iteration of this activity, the total number of rules will be only around 40% to 60% completed. The idea is to get something to business users that is sufficient to get comments and feedbacks on, and that will form the basis of the next phase. By this point, the Rule Business Object Model (RBO) should be at least 90% complete.\n</code></pre> <p>Depending on the size of the ruleset, this activity may deliver less than 40% of the rules. In that case, this activity may have to be repeated. In this situation, it is recommended that the activity be time-boxed from two to three weeks maximum so that a concrete build may be delivered to the validation team for functional verification, integration test and non regression testing. Once a build is delivered, another iteration of this cycle may be started.</p> <p>As soon as the decision service interface is stable, or the event structure is well defines the development team should be able to deploy the rule set under construction to the execution server in order to test it with an end-to-end scenario. The integration of the decision service and the domain object model is an important task. Integration involves the marshaling of data to/from external data sources, transform it to the rule business object model view so it can be understood by the execution engine. This is 'mapping' of the data to the Rule Business Object Model created in previous activities is very important as it is a very bad practice to take an existing enterprise model, or a canonical model to be the foundation of the rule processing. Too much genericity and complexity will impact rule adoption by the business users, and knowledge transfer between developers too.</p> <p>There are a number of tools and techniques that can be used to marshal data, and the best choice will vary depending of the technologies available: ESB, micro services, ETL, batch processing, event based processing.... When this activity is completed, data (preferably 'real' data) will be sent to the rule engine to fire rules, make decision and the results delivered over the same channels as the production system. The test scenarios developed in previous phases, are run with all of the 'plumbing' in place. In the future they will serve as non-regression test suite.</p>"},{"location":"methodology/abrd/#cycle-4-enhancing","title":"Cycle 4- Enhancing","text":"<p>Cycle 4 is used during the development phase as well as during the rule maintenance or governance activity post production deployment. One of the goal here is to empower business users or at least business analysts to author and test the rules.</p> <p>The Enhancing cycle is a transition phase in which last minute changes can be made involving some short, face to face discovery meetings with SMEs (Subject Matter Experts) to wrap up any outstanding issues or questions before entering the Governance activity. The Governance activity is an open-ended, long running process to enhance and maintain the rules created in previous activities. A governance process will vary by organization and is a fundamentally different SDLC process, usually performed by a different team than the rule development team. This team is more business oriented and, as owners of the rules and business policies, they can develop at their own pace, using the infrastructure put in place by the development team.</p> <p></p> <p>It is important to note that there will almost certainly be some need to enhance the rule business object model to add some new facts, attributes, or entities. These modifications should follow the standard release management process of the core business application using continuous build and delivery.</p>"},{"location":"methodology/abrd/concept/","title":"Agile Business Rules Development Methodology","text":""},{"location":"methodology/abrd/concept/#introduction","title":"Introduction","text":"<p>The Agile Business Rule Methodology was developed in 2004 at ILOG and then used on hundred of projects worldwide when projects need to integrate decision management into the business application. It is based from Hafedh Mili and Jerome Boyer's book. The content is product agnostic and support incremental and iterative development for decision services. It starts from a business operation improvement idea that is assessed versus of set of criteria to validate the fit to use a Business Rule Management System, and then addresses all the project implementation tasks.</p>"},{"location":"methodology/abrd/concept/#goals","title":"Goals","text":"<p>ABRD applies the agile manifesto to efficiently implement business policies as executable rules. The goal is not having a team spending month to document thousand of rules, where those rules can be implemented, executed, tested in less time, even as early as the first day of your analysis. The goals can be summarized as:</p> <ul> <li>Separate rules as a manageable artifact using discovery, analysis and authoring activities and work products</li> <li>Trace rules during their full life cycle from requirement to deployment and maintenance</li> <li>Link rules to business context and motivation</li> <li>Develop the rule description using business terms and high level rule language</li> <li>Prepare the fact model for the rule implementation</li> <li>Prepare the rule set implementation and deployment as decision services in a Service Oriented Architecture or as Micro Services executed by a rule engine</li> <li>Articulate the rule governance processes</li> </ul>"},{"location":"methodology/abrd/concept/#how-to-adopt-abrd","title":"How to Adopt ABRD","text":"<p>BRMS enable managing business rule as a standalone artifact, owned by the business user, and maintainable over time into production system. The implementation of a business rule application follows some activities and tasks that are slightly different than traditional software development life cycle. The integration of one or more business analysts as part of the development team is also less traditional. Finally the core value of such technology is to be able to have business user maintaining the business rules in production with a minimum involvement of IT. Technology is one side of the coin, methodology and best practices is the other.</p> <p>A Project Management Office can leverage ABRD as-is or integrate it as content for their own methodology.</p> <p>If the team is new to business rule application, it is best to start with a small rule set and incrementally add rules and best practices over time. The team has to integrate the rule discovery and analysis activities in their own project plan.</p> <p>Prototyping or Minimum Viable Product is a major approach, as it shows to the team concrete execution, and helps to drive issues, and requirements around business rules and even business process.</p>"},{"location":"methodology/abrd/concept/#common-pitfalls","title":"Common Pitfalls","text":"<p>The most common pitfalls in implementing business rules application include: * Starting by harvesting all the rules in a document without any implementation * Ignoring the importance of the data model * Not understanding where the rules are (should be) enforced * Mixing the data model from the implementation point of view, with the domain model that is used by the business * Involving IT only: never forget the business, involve them as early as possible and make them take ownership of the rules. * Outsourcing business rule implementation: business rules are enterprise assets. * Forgetting to test the rules outside of the application * Not involving the business in the rule validation process * Badly designing a rule set, by not applying standard design pattern as separation of concerns.</p>"},{"location":"methodology/abrd/concept/#incremental-development","title":"Incremental Development","text":"<p>The following diagram illustrates how a decision service implementation is built from a business idea to an executable SOA service integrated with a BPM system or any consumers.</p> <p></p> <p>Within those phases we can find project activities that are executed multiple times during each phase:</p> <ul> <li>Rule discovery</li> <li>Rule analysis</li> <li>Fact model implementation</li> <li>Rule implementation and unit test</li> <li>Rule validation</li> <li>Rule deployment</li> </ul> <p>Like all agile methodologies, ABRD prescribes an incremental development using time boxed iterations. However, in ABRD, the entire process lifecycle may not be followed for each iteration. Instead, a number of very short cycles between phases may occur, with the entire ruleset gradually evolving.</p> <p>Using a business rule management platform enables managing business rule as a standalone artifact, owned by the business user, and maintainable by them directly  time into production system. The implementation of a business rule application follows some activities and tasks that are slightly different than traditional software development life cycle. The strong involvement of one or more business rule analysts as part of the development team is recommended to harvest the rule while they are implemented. Technology is one side of the coin, methodology and best practices is the other. Executing a project to implement a solution using rule engine and a BRMS follows the following progression as illustrated</p> <p></p> <p>A Project Management Office can leverage ABRD as-is and integrate it within their own methodology or tailor it by injecting their own practices. As open source you can also contribute to it.</p> <p>If the team is new to business rule application, it is best to start with a small rule set and incrementally add rules and best practices over time. The team has to integrate the rule discovery and analysis activities in their own project plan.</p> <p>Prototyping is a major value, as it shows to the team concrete execution, and helps to drive issues, and requirements around business rules and even business process.</p>"},{"location":"methodology/abrd/concept/#common-pitfalls_1","title":"Common Pitfalls","text":"<p>The most common pitfalls in implementing business rules application include:</p> <ul> <li>Starting by harvesting all the rules in a document</li> <li>Ignoring the importance of the data model</li> <li>Not understanding where the rules are (should be) enforced</li> <li>Mixing the data model from the implementation point of view, with the domain model that is used by the business</li> <li>Involving IT only: never forget the business, involve them as early as possible and make them take ownership of the rules.</li> <li>Outsourcing business rule implementation: business rules are enterprise assets.</li> <li>Forgetting to test the rules outside of the application</li> <li>Not involving the business in the rule validation process</li> <li>Badly designing a rule set, by not applying standard design pattern as separation of concerns.</li> </ul>"},{"location":"methodology/abrd/concept/#cycle-approach","title":"Cycle Approach","text":"<p>Like all agile methodologies, ABRD prescribes an incremental development using time boxed iterations. However, in ABRD, the entire process lifecycle may not be followed for each iteration. Instead, a number of very short cycles between phases may occur, with the entire ruleset gradually evolving. The entire process may be represented by the diagram below:</p> <p></p> <p>In the process flow diagram above, each of the task loops back to previous task in the flow. These 'loops', or cycles, are repeated as required, with the loops gradually moving from left to right through the process until the rule sets are deployed to production environment. This method of rule development reflects the nature of BRMS: a mature and stable domain object model is a prerequisite for rule development, therefore we may multiply cycles through the early part of the process flow in order to develop such models early in the process. As each of these cycles is completed, the ruleset will gradually grow until it reaches a state where it reflects the requirements set forth by the business.</p> <p>The phases and activities apply for both operational, inference business rules as event processing rules. The event processing rule will, most likely require different technologies: a BRMS and a CEP platform, but the development methodology is similar.</p>"},{"location":"methodology/abrd/concept/#a-typical-abrd-project","title":"A typical ABRD Project","text":"<p>From a project management perspective, there are four phases, each of which maps onto parts of the technical process flow described above. The project phases are:</p> <ol> <li>Harvesting, the activity of gathering business rules</li> <li>Foundation, the activity of defining the decision service interface, to develop the domain specific data model: the Rule Business Object, to implement the different business rule pattern into the BRMS, develop the unit tests</li> <li>Building, the activity of building a working decision service, representing the organizations business rules, it includes integration with the decision service consumers or the event producers and any reference data management sources.</li> <li>Enhancement groups completing the business logic by adding more rules, and start the rule governance process, the decision logic monitoring, and maintaining and enhancing business rules. This specific phase should be done with less involvement from IT, and with modern BRMS, business analysts can support the full cycle of authoring rules, validating them, and hot deploy to production.</li> </ol>"},{"location":"methodology/abrd/concept/#cycle-1-harvesting","title":"Cycle 1- Harvesting","text":"<p>Rule harvesting, consisting of the Discovery &amp; Analysis tasks, involves gathering together all the sources of knowledge available to the team. These sources may include a business process description, subject matter experts, policy manuals, existing code or other sources of rules. The goal of this activity is to understand the Domain Object Model within the scope of the application and to identify enough rule patterns to begin implementation of rules in the next phase.</p> <p>During this activity, the development teams divide the day into two parts, executing discovery workshops in the morning (in 2 or 3-hour session), then performing analysis and documentation for the remainder of the day. The team iterates on these two steps during 2 to 5 days maximum, depending on the number of rule pattern and their complexity.  </p> <p></p> <p>The starting point of the Rule Discovery is a Decision Point Table artifact: During the inception phase (See OpenUP for more information) the project team is doing business modeling activities (not covered here) which aim at describing the business process and decisions applied to any business events supported by the application. One important work product built during this modeling phase is the Decision Point Table which describes the point in the process (task, activities, transition) where decisions are made. Decision point represents potential candidate for rule sets.</p>"},{"location":"methodology/abrd/concept/#cycle-2-building-foundations","title":"Cycle 2- Building Foundations","text":"<p>The building foundation extends the tasks performed during harvesting to include rule business object implementation and rule authoring. When a workable number of rules (this number will vary by project) have been harvested the development team should start to define the structure of the rule project: the decision service interface, the rule set variables (input-output business objects), the basic sequencing of the rules (the Rule Flow), and the major elements of the Business Object Model. The team should then be able to implement and unit test some rules.</p> <p>A best practice is to execute the step \"Rule Authoring\" as soon as possible to uncover possible analysis and design issues. Indeed, most of the rules look good on paper but real issues will surface during implementation and test. The next morning workshop session the rule developer should communicate the issues back to the business team to quickly address the change to the business logic. This leverages the feedback loop approach and provides an efficient mechanism to build a pragmatic, adequate and business-relevant executable rule set.</p> <p></p>"},{"location":"methodology/abrd/concept/#cycle-3-building","title":"Cycle 3- Building","text":"<p>The goal of the build phase is to create rules that can be 'seen', 'used' and, most importantly, tested, by business users. Rules that can be executed by the rule engine are more valuable than ones defined on paper or in abstract forms. This fully endorses the agile statement:\"Working software over comprehensive documentation.\"</p> <p>The second goal of this phase is to create a set of realistic test scenarios, using real data when possible, to test the rules created during the Authoring activity. This is a form of Test Driven Development (TDD). This activity extends the Build activity by adding a Rule Validation task.</p> <p></p> <p>The day-to-day authoring activities consist of a series of small steps that include: * Test case implementation * Writing rules and executing rules * Validating rules with the business team members * Deploy automatically to the target test platform so decision services are consumable</p> <p>The build activity consists of all the previous tasks, with short iterations: * Enhance rules (Authoring &amp; Validation tasks) * Improve rules (Analysis, Authoring &amp; Validation) as needs are identified * Complement rules (Discovery, Analysis, Authoring &amp; Validation) with additional rules to provide complete coverage of the business domain (once every two days).</p> <p>The Building activity should take between two and three weeks iteration, and the output artifacts should be a ruleset that can be loaded into the rule engine and executed.</p> <p>Checkpoint:</p> <pre><code>    During the first iteration of this activity, the total number of rules will be only around 40% to 60% completed. The idea is to get something to business users that is sufficient to get comments and feedbacks on, and that will form the basis of the next phase. By this point, the Rule Business Object Model (RBO) should be at least 90% complete.\n</code></pre> <p>Depending on the size of the ruleset, this activity may deliver less than 40% of the rules. In that case, this activity may have to be repeated. In this situation, it is recommended that the activity be time-boxed from two to three weeks maximum so that a concrete build may be delivered to the validation team for functional verification, integration test and non regression testing. Once a build is delivered, another iteration of this cycle may be started.</p> <p>As soon as the decision service interface is stable, or the event structure is well defines the development team should be able to deploy the rule set under construction to the execution server in order to test it with an end-to-end scenario. The integration of the decision service and the domain object model is an important task. Integration involves the marshaling of data to/from external data sources, transform it to the rule business object model view so it can be understood by the execution engine. This is 'mapping' of the data to the Rule Business Object Model created in previous activities is very important as it is a very bad practice to take an existing enterprise model, or a canonical model to be the foundation of the rule processing. Too much genericity and complexity will impact rule adoption by the business users, and knowledge transfer between developers too.</p> <p>There are a number of tools and techniques that can be used to marshal data, and the best choice will vary depending of the technologies available: ESB, micro services, ETL, batch processing, event based processing.... When this activity is completed, data (preferably 'real' data) will be sent to the rule engine to fire rules, make decision and the results delivered over the same channels as the production system. The test scenarios developed in previous phases, are run with all of the 'plumbing' in place. In the future they will serve as non-regression test suite.</p>"},{"location":"methodology/abrd/concept/#cycle-4-enhancing","title":"Cycle 4- Enhancing","text":"<p>Cycle 4 is used during the development phase as well as during the rule maintenance or governance activity post production deployment. One of the goal here is to empower business users or at least business analysts to author and test the rules.</p> <p>The Enhancing cycle is a transition phase in which last minute changes can be made involving some short, face to face discovery meetings with SMEs (Subject Matter Experts) to wrap up any outstanding issues or questions before entering the Governance activity. The Governance activity is an open-ended, long running process to enhance and maintain the rules created in previous activities. A governance process will vary by organization and is a fundamentally different SDLC process, usually performed by a different team than the rule development team. This team is more business oriented and, as owners of the rules and business policies, they can develop at their own pace, using the infrastructure put in place by the development team.</p> <p></p> <p>It is important to note that there will almost certainly be some need to enhance the rule business object model to add some new facts, attributes, or entities. These modifications should follow the standard release management process of the core business application using continuous build and delivery.</p>"},{"location":"methodology/abrd/harvesting/","title":"Rule Harvesting","text":"<p>Update</p> <p>This chapter is extracted from the Agile Business Rule Development Book with integration in a 2024 context.</p> <p>Rule harvesting includes the two main activities of rule discovery and analysis, with the goal to understand the business entities (conceptual data model) within the scope of the application and to identify and extract the rules. A key activity in the rule harvesting phase is to formalize the decisions made during the execution of the business process by defining the different decision point candidate for business rule implementation. </p> <p>Agile Business Rule Development puts the emphasis on developing the system through short iterations. Each iteration produces a working set of rules. Feedback from the harvesting and prototyping phases force the subject matter experts to better understand their own business processes, and help them to adapt those processes for more efficiency. Rule harvesting is a short project activity executed for each decision point in scope for implementation.</p> <p>The process flow may look like in the following Figure:</p> <p></p> <p>The purposes of Rule Discovery and Analysis activities are: </p> <ul> <li>To define the rule classification from the business process </li> <li>To attach decision point to business process activities and / or use case steps </li> <li>To define the rule sources for the discovery and the type of roadmap to support the knowledge acquisition. </li> <li>To formalize the rules using templates and business term and language understood by the business user </li> <li>To prepare the test data \u00a0for the rules </li> <li>To define the object model for the rules</li> </ul>"},{"location":"methodology/abrd/harvesting/#rule-discovery","title":"Rule Discovery","text":"<p>Rule discovery, also called Business Rules Modeling in the industry, aims to develop simple modeling artifacts like rule descriptions, business entity diagrams, and business process maps.</p> <p>Rule discovery involves an iterative process wherein a subset of rules is identified and documented, rather than investing months in comprehending all the rules initially and generating an extensive document.</p> <p>Business rule discovery techniques share similarities with traditional requirements elicitation methods, but with one key distinction: they prioritize identifying specific needs that directly impact decision-making within the company. </p> <p>The sources for starting a rule discovery activity is a business process description, or use case description. From the business process description in BPMN map study the task or activity description and search for verb\u00a0which involves mental processing or thinking. Work with the Subject Matter Expert or with the business analyst responsible to document the Business Process on how the decision on those activities are done. For any decision log them in a table with the task reference, the intent and enforcement description.</p> Decision Point\u00a0 Name Description Source for Rule Discovery Current State of Automation Enforcement Ownership Data validation Validate the information entered around the borrower, the amount requested, the property 1003 forms &amp; legal document per state and federal Some rules are implemented in the current GUI used to enter the data in the system Data entry in system John Calculate Risk grade and credit score Calculate the value of different parameters such as the credit score and the risk grade Score cards, Excel files None Once loan and property data validated Jack <p>Example of Decision Point Table</p> <p>The decision points table (DPT), helps at defining where to find the rules (rule sources), and which method to use for the rule harvesting. The rule discovery process changes according to the sources used. For example, working from a legal document implies a different discovery process than the discovery based on interviewing a subject matter expert, or extracting rule from executable software code.</p> Decision point table <p>There are different ways to extract the decision points table. For user stories approach, search for verb\u00a0which involves mental processing or thinking: <code>check, qualify, compute, calculate, estimate, evaluate, determine, assess, compare, verify, validate, confirm, decide, diagnose, and process</code>. Behinds those verbs the team extracts a lot of business knowledge and how the business decisions are made in each activity. When using process modeling approach, identify the branch points in the process. The decisions to route to one of the sub branches can be considered as business rules, but the most important focus should be on the activities before the diamond: is there any mental thinking verbs in the description of this task?Assess how decisions are made within those tasks.</p>"},{"location":"methodology/abrd/harvesting/#classification-of-business-rules","title":"Classification of business rules","text":"<p>Before deciding how to write rules and where to implement them, you first need to understand which types of rules your team will be harvesting. In early 2008, the Object Management Group (OMG) finalized a specification for documenting the semantics of business vocabularies and business rules, entitled called Semantics of Business Vocabulary and Business Rules (SBVR)</p> <p>SBVR is part of the OMG\u2019s Model Driven Architecture (MDA) with the goal to\u00a0capture specifications in natural language and to represent them in formal logic so they can be automated. SBVR includes two specialized vocabularies: </p> <ul> <li>Vocabulary for describing Business Vocabularies, which deals with all kinds of terms and meanings</li> <li>Vocabulary for describing Business Rules, which deals with the specification of the meaning of business rules, and builds on the business vocabulary.</li> </ul> <p>The meaning is what someone understands or intends to express. The meanings are derived into concepts, questions\u00a0and propositions. A phrase such as \"We deny the invoice if the medical treatment was done after one year of the accident\" has a\u00a0clear meaning for a claim processor within a car insurance company. Analysts need to logically transform this meaning into concepts that have a unique interpretation so that we can represent the business knowledge within a comprehensive vocabulary. Concepts include a unique combination of characteristics or properties. </p> <p>Within the Business Motivation Model, the OMG has also defined the relation between business policies, directives, business processes, and business rules. This work is very important to clearly classify each of those concepts. </p> <p>The OMG definition of business policy is: \u201cA non-actionable directive whose purpose is to govern or guide the enterprise. Business policies govern business processes\u201d. A Business rule is \u201c\u2013 A directive, intended to govern, guide, or influence business behavior, in support of business policy that has been formulated in response to an opportunity, threat, strength, or weakness. It is a single directive that does not require additional interpretation to undertake strategies or tactics. Often, a business rule is derived from business policy. Business rules guide a business process\u201d. </p> <p>For the purpose of rule harvesting, keep in mind that business rules are actionable, unambiguous, and derived from the business policy. Considering rules as semantically meaningful, rather than business policies, is key to making them executable.</p> <p>The OMG BMM reuses some classifications from the SBVR:  business rules are separated into two possible classes: </p> <ul> <li>Structural (definitional) business rules which are\u00a0about how the business organizes the things it deals with, they are considered as necessity. In this context the statements describing the rule can describe the necessity, the impossibility or the restricted possibility. </li> <li>Operational (behavioral)\u00a0business rules govern the business conduct. They\u00a0are considered as an obligation and are directly enforceable. When considering operational business rules it is important to look at the level of enforcement and where the rule enforcement occurs. Statements to describe the rules include obligation, prohibition, and restricted permission. The action part of the rule is as important as the condition. The action drives where the enforcement may happen. </li> </ul> <p>In SBVR, rules are always constructed by applying necessity or obligation to fact types. A fact type is an association between two or more concepts.  Another approach to define facts is to use the Ontology Web Language (OWL) and Resource Description Framework (RDF). Developed to specify semantic web3, OWL and RDF can be used to model the enterprise ontology. The ontology is the source for data models used by the rules as an alternate to traditional OOA and SBVR. OWL and RDF implement an object-relational model allowing creation of a directed graph, a network of objects and relationships describing data. </p> <p>Using a mix of the SBVR classification for business rules, OWL \u2013 RDF to describe the domain and an older rule classification model which we have used for years in consulting engagements, the different types of business rules can be presented as follows: \u00a0 </p> <p>Figure 2: Business Rule Schema</p> <p>This schema represents a conceptual breakdown of the different types of rules that are relevant to the business, including structural and operational rules. </p> <p>Structural rules define the terms used by the business in expressing their business rules and the relationships (facts) among those terms. These comprise the vocabulary used in rule authoring. As an example a statement like: An Insurance Policy includes a set of coverage. The policy is effective at a given date and needs to be renewed every six months. Transforming this statement implies defining a structure in the insurance domain, where an insurance policy entity has an effective date, expiration date and a list of coverage.</p> <p>Operational rules are the rules that implement business decision logic. When a business decision is made (e.g. whether to sell a given insurance policy, whether to accept or reject a claim), the business rules are the individual statements of business logic that are evaluated by the rule engine to determine the decision result. </p> <p>The following table is adapted from the work by Barbara Von Halle, and is a simplified view of the business rules group classification. It details those categories:</p> Rule Classification Explanation Expression Mandatory constraints Rules that reject the attempted business transaction. Grammar to use during rule documentation: {term} MUST HAVE {at least, at most, exactly n of} {term}; {term} MUST BE IN LIST [a,b,c]. For the SBVR expression: it is [not] necessary that {fact} Guidelines Rules that does not reject the transaction; they merely warn about an undesirable circumstance. Usually translates to warning messages. {term} SHOULD HAVE {at least, at most, exactly n of} {term}; {term} SHOULD BE IN LIST [a,b,c]. For the SBVR expression: it is [not] possible that {fact}** Action-enablers Rules that tests conditions and upon finding them true, initiate another business event, message, business process or other activity IF {condition} THEN {action} Computations Rules that create new information from existing information based on mathematical computation. {term} IS COMPUTED AS {formula} Inferences Rules that create new information from existing information. The result is a piece of knowledge used as a new fact for the rule engine to consider. IF {term} {operator} {term} THEN {term} {operator} {term} Event Condition Action (ECA) Rules where the condition is evaluated once the occurrence of an event is found. Most ECA rules use temporal operators to search events related to their timestamp of creation or occurrence. On {event} when {condition} then {action} <p>To implement guidelines and constraints, you need to consider what happens when they are violated. Most of the time, the action raises an exception or a high priority issue to be managed later in the business process, which may reject the business event. The rule needs to report clearly on the selected decision so that a human can understand and act on the business transaction. </p> <p>A guideline written as: \"The date of loss should be before the expiration date of the policy\" may translate to the following rule: if the date of loss is after the expiration date of the policy, then create a warning ticket for the claim processor to handle. This implementation allows the insurer to make allowances for an insured person who has a history of regularly renewing the policy but for some reason forgot to renew on time.</p> <p>A constraint written as: \"The borrower must put a minimum cash down of 5%\", translates to this rule: if the minimum cash is below 5% then reject the loan application with the reason \u2018The borrower must put minimum cash down of 5%\u2019. </p> <p>Action enabler rules modify, create or delete terms or association between terms, or execute methods which can be web service. For example, a rule like: \"if a driver has made one or more at-fault claims in the current year, decrease the discount rate by 3%\", updates an attribute (\u201cdiscount rate\u201d) of an object.</p> <p>Computation rules implement mathematical equations and assign values to variables according to a set of given criteria. For example, a risk factor variable can be computed according to the age of the driver. It is important to note that management of computation rules may require managing the entire ruleset together, if there are rules that are required to be managed prior to those calculations and at the terms of the calculation. </p> <p>Process flow routing rules direct the movement through a process flow or workflow. Process flow rules are distinct from business logic rules. It may be helpful to distinguish process flow rules from the business logic rules that determine the values of the parameters on which the process flow is directed as such rules are more often complex and numerous than routing rules. Routing rules may be written as: \"if there is at least one exception in previous activity of the process goes to this task if not continue on the main path\". The business logic to define if there is an exception is made within a rule engine with a lot of rules to evaluate and execute. </p> <p>Inference rules use syntax similar to action enabler rules, but they create new objects or facts which may bring the rule engine to re-evaluate some other rule\u2019s eligibility. </p> <p>During discovery, it is important to understand the execution context as seen by the business user, and be able to answer questions like: \u201cIf executing this rule modifies the state of the claim, will the eligibility rules that have already executed need to be reevaluated?\u201d  For example, an insurance policy underwriting rule that says if the age of the driving license is below 3, add a risk factor of 50 and reevaluate the total risk score, modifies the risk scoring variables, which requires that other rules be reevaluated.</p> <p>It is possible to continue the decomposition of those rules. For example transformation rules in\u00a0ETL (Extract Transform Load) are often considered separate from other business rules; although in pattern they are essentially inference rules and computation rules. Data transformation rules, while important to the business, are a side effect of system implementation rather than a reflection of core business logic. For implementation, the decision to use a rule engine for data transformation rules depends on whether the rules are static, dynamic, or business driven. Some implementations use a rule engine to easily implement transformation rules between two data models instead of using a complex scripting language when the transformations have to be maintained by business users.</p> <p>Complex Event Processing (CEP) statements (or rules) support a category of business rules related to real-time event filtering, event aggregation and correlation, and applying pattern matching conditions on attributes of events. CEP applies business rules to a stream of data. A business rule to detect fraud on banking cards may be written as: \"Raise a warning alarm if more than one transaction for an amount over $100 is received from the same merchant on the same card number within the last 15-minutes\".</p> <p>According to the preceding rule classifications, this rule would be considered a mix of ECA and inference rules. However, one important dimension of this type of rule is the time window constraint on which the rules apply and the type of language used to write the rule. Today most of those languages are based on SQL and includes operators to look at time window. The action part of the rule creates new events (called the complex events) which are managed by a downstream application. </p> <p>In addition to industry standards, as described above, the following table include commonly used rules:</p> Rule Classification Type of application Example Compliance Rules Rules that reject the attempted business transaction. Yes/no result but completed with reason code and explanation. Found in Underwriting, Fraud detection, Data and form validation. Example: Whoever receives a commission paid by an insurance company for the sale of an insurance policy needs an insurance license. Rating Strongly interrelated rules that compute metrics for a complex object model. Scoring and rating, Contracts and allocation, Pure calculations on an object providing a final value (or rating). Example: if the driver is between 16 and 22 years old the risk is set to 400. If the driver is between 23 and 27 the risk is set to 300 Correlation Strongly interrelated rules that correlate information from a set of objects to Compute some complex metrics. Billing and cost estimation. Complement by inserting information. Example: if the medical bill references a patient, and the patient is not declared in the related claim then there is an issue on the claim description or the invoice is not related to a patient covered Stateful Strongly interrelated rules that correlate events in a stateful way. Stateful in this context means the internal states of the engine are maintained between rule execution invocations. Alarm filtering and correlation, Web page navigation, GUI customization Example: if there is an alarm L1 on network element A and an alarm L2 on network element B and a direct link exists between A and B then the alarm L1 is linked to L2, remove alarm L2. <p>Rule classification plays a crucial role in streamlining the decision service design, as it allows for an optimal implementation of each rule. Inference and action enabler rules are particularly well-suited for utilization within a rule engine. On the other hand, purely computational rules are typically implemented through code, unless they are subject to frequent changes in applicability criteria or have strong interdependencies with other business rules. Furthermore, rule classification aids in assessing the complexity of rules and estimating the associated implementation workload.</p>"},{"location":"methodology/abrd/harvesting/#discovery-activities","title":"Discovery activities","text":"<p>The discovery activities are conducted during the elaboration phase of the project, but the same process is conducted even after the system has gone into production when there is a new business event, or when there is a need to modify some decision or some business policy. </p> <p>Companies have been operating with business rules for many years, but the form of these rules is not externalized and managed as standalone artefact. Capturing business rules relies on a combination of eliciting business requirements, reverse engineering existing application code and formalizing expert\u2019s knowledge. Business rules are not just requirements: they specify how a business process is executed, how decisions are made, and how domain knowledge is structured. When using a business rule approach, we are working at the business process and policies level to understand the constraints and the behaviors of the process. </p> <p>The most unique aspect of the rules discovery phase is the perception of a business event as a set of decision-rich activities. We unfold the processing of a business event as a set of decisions and policies. We then dissect the decisions and policies into executable and precise business rules that guide the business event in a predictable and desirable manner. </p> <p>There are two dimensions to consider when preparing the rule discovery activities or roadmap:</p> <ol> <li> <p>The source of rule, which can be:</p> <ul> <li>the documentation which includes all sorts of documents (business plans, deliverables of earlier projects, legislation, regulations, standards, business requirements for the current project), </li> <li>the tacit know-how: the unwritten \"way we do things\", embodied as a collective way of doing things (organizational intelligence), or as individuals' expertise, </li> <li>the legacy system which includes an operational system that implements some of the business logic (code mining)</li> <li>the bBusiness records as the way particular customer requirements have been satisfied, pricing formulas used in previous contracts</li> </ul> </li> <li> <p>The type of analysis techniques used by the project team: </p> <ul> <li>Business event analysis performed during event storming workshop</li> <li>Use case and user story elaboration, </li> <li>Business process modeling using BPMN or event storming </li> <li>Business mission and strategy analysis, using design thinking practices</li> <li>Data analysis: it is true that doing Machine Learning with decision tree construction may be implemented using operational rules.</li> </ul> </li> </ol> <p>Obviously the know-how discovery is the most difficult elicitation task to execute, and the one that usually takes the longest time. We will provide details later in this section on how to conduct such an elicitation workshop. </p> <p>The following table is giving the different possible starting points for the discovery activities based on the analysis method used:</p> Starting Point Analysis Description Business Events Start with the business events and review how it is processed, and enforced policies Use case Analyze use case description to find decision points and then rules. This is the preferred approach, for teams familiar with use cases, or user stories. Business Process - Workflow Evaluate individual process steps and tasks to define the decision behind activity and then the rules. Used when the organization uses process decomposition for the requirements gathering and analysis phase. Data analysis \"Take into account the life cycle of key business objects and extract the underlying processes and decisions involved in altering the data's state. This analysis can commence with the examination of the logical data model and the methods by which business entities are created, updated, and deleted, along with their respective states. One example of such an approach is to look at the States of an Insurance Policy, and how, who, when changes are made. <p>It is important to set the expectation among the stakeholders that not all the rules will be discovered during this phase. The goal is to complete the rule discovery up to 40-60% so we can have some tangible decisions on standard business events to process. The rule developers  will increase the amount of rules in scope during future project iterations. </p>"},{"location":"methodology/abrd/harvesting/#discovery-roadmap","title":"Discovery roadmap","text":"<p>The selection of the type of roadmap is linked to the rule source. </p> <ul> <li>Static analysis process uses reading and highlighting the rules within documentation which can be legal, internal policies, procedure. The team has to gather all the related documents with the reference on version, date of creation and validity... The elicitation is based on reading sessions completed with Question / Answer workshop sessions.</li> <li>Interactive involves working sessions with subject matter experts who have the knowledge of the business process and the decisions within a process task. Also a person doing the day to day activity is a very good source to understand how decisions are made and how exceptions to the main business process are handled. The process to elicit rules from people will be accomplished by using elicitation workshop.</li> <li>Automated involve using a computer and special applications to search for rule statement within procedure code, SQL procedures, code listing\u2026 When using rule mining technology we have to be careful to not lose the context of execution in which the if-then-else statement was implemented. Therefore code review should always be complemented by workshop sessions with Q&amp;A.</li> </ul> Code mining <p>Code mining may be interesting, but developers need to be able to assess:</p> <ul> <li>Who has the knowledge of the current code? Is this person still in the company?</li> <li>Should the current business context use the same business rules as 15 or 20 years ago?</li> <li>Not all \u2018If-then-else statement\u2019 in legacy code represents business rules, sometimes procedures, functions and algorithms may be an implementation of business rules. The context of execution is a very important dimension to understand before reusing a coded business rule as-is.</li> <li>Variable names were often limited to 8 characters in a flat data model. There is no need to keep it that way. You may want to think about designing an efficient object oriented model.</li> <li>Most of the time, automatic translation of badly coded business rules will generate bad business rules in the new environment.</li> <li>Business rules implemented for a business rule engine have a different structure than procedural code. They should be more atomic and isolated, and the rule writer may leverage the inference capacity of the engine. Therefore the automatic translation will produce poor results.</li> </ul> <p>The following table summarizes the different techniques classified per type of source:</p> Source Static analysis Interactive Automated Documentation Very good fit As a complement of static analysis Potential with Generative AI Know-how Not applicable Unique solution AI Agent may help defining discovery question to SMEs Code Efficient As a complement of the other processes Good result, improved with Gen AI <p>When the source of the business rules is people, individual interviews are required to get the core of the knowledge and then followed up with workshops to resolve outstanding issues and process exception paths with the team.</p> <p>Rule elicitation is an ongoing activity you perform throughout the project. Collaboration with your stakeholders is critical. They will change their minds as the project proceeds and that\u2019s perfectly fine. Remember that there are different types of languages for expressing business rules. </p> <ul> <li>Natural language</li> <li>Restricted Language</li> <li>Formal expression using a specific grammar The natural language is initially used during business conversations to describe the rules, informally, without trying to impose any structure, for example with people sitting around a table. At this stage, we don't have any templates or guidelines for structure that we need to abide to. Using this language we may have redundancy and inconstancy in the rule expressions and in the business terms used. </li> </ul> <p>A second evolution is using a restricted language, still consumable by both analysts and developers, but where we have imposed some structure and grammar to the language so we can express rule statements with proper form. SBVR is proposing the restricted English to support this. The statement may not be correct semantically (redundancy, consistency, etc.) but we can formalize the business term and glossary of terms.</p> <p>The third type of language is precise and there are no ambiguities: the rule refers exactly to information system objects. This language is parse-able and non-ambiguous, and can be executed by a computer.</p> <p>An example of rule template:</p> Business Activity: Decision Name: Policies reference: Ownership: Rule Name Rule Statement Notes Accident Prone Customer Use the raw natural language of the business conversation. Later we may need to use a more strict language like the restricted English of SBVR.  A customer who had an accident report in the past is marked as accident prone Use comment for example to describe the type of rule, inference Rule Language <p>A formal language features sentences which have a clear and unambiguous interpretation. There are different kinds of formal languages:</p> <ul> <li>Predicate logic using syntax like: (\" X,Y) [Claim(X) L MedicalInvoice(Y) L Relation(X,Y) =&gt; (claimRefNumber(Y) = claimNumber(X))]</li> <li>Object Constraint Language (OCL) : is an addition to UML to express constraints between objects that must be satisfied</li> <li>Truth tables or decision table which present rule as row and columns representing conditions and actions</li> <li>Semantics of Business Vocabulary and Business Rules or SBVR which defines structural and operational rules as well a vocabulary to define business concepts.</li> <li>JRules Technical Rule Language executable by a rule engine</li> <li>JRules Business Action Language, high level language close to English which is formal as it is using a unique interpretation and unique translation. Rule writers pick among a set of predefined sentences</li> </ul>"},{"location":"methodology/abrd/harvesting/#discovering-rules-from-smes","title":"Discovering rules from SMEs","text":"<p>Interviews and analysis workshops are the two types of interaction used with subject matter expert. For interviews, the typical number of people in the same room is around two or three and for workshops six to ten people are involved. Workshops can last several days. Interviews are used at the beginning of the discovery phase and will most likely address one area of the business process. The analysis workshop is perhaps the most powerful technique for eliciting a lot of requirements. It gathers all key stakeholders together for a short but intensely focused period. The use of a facilitator experienced in requirements management can ensure the success of the workshop. Brainstorming is the most efficient technique used during the sessions.  \u00a0Brainstorming involves both idea generation and idea reduction. The most creative, innovative ideas often result from combining, seemingly unrelated ideas. Various voting techniques may be used to prioritize the ideas created. Allow for human behavior but control the following points:</p> <ul> <li>Do not \u201cattack\u201d other members</li> <li>Do not come back late from a break, even if Key shareholders may be late returning because they have other things to do. The sessions are short so they should be able to do other activities during the day.</li> <li>Avoid domineering position</li> </ul> <p>Some suggestions to improve the process:</p> <ul> <li>Facilitator keeps a timer for all breaks and fines anyone that is late, everyone gets one free pass</li> <li>Facilitator encourages everyone to use 5-minute position statement</li> <li>In case of long discussion without reaching firm conclusion or agreement it is good to use the business concerns to drive the elicitation</li> <li>If the rule is not clear this is good practice to prototype it.</li> <li>Use concrete scenarios to illustrate some rules. These scenarios can later be leveraged for tests.</li> </ul> <p>The following table lists the standard questions the analyst team may ask during the workshop depending of the source: </p> Type of input document Questions Type of artifacts impacted Use case or Business Process map In this activity, what kind of control  the worker responsible to perform the task ? What kind of decisions? On this use case step, the person assess the application, what kind of assessment is he doing? Is there a standard check list? Use case or BPM, Rule description document Rule description, Conceptual  data model What do you mean by \u2026. (a business term to clearly define). \u00a0How does it relate to \u2026. (other business term) Conceptual  data model Rule statement What about the other ranges of possible values for this condition? How often does this condition change? Do you have some other cases? Business Entities Diagram, Rule description document <p>Between sessions, verify that business terms are well defined and the rules make sense and do not have logical conflicts. Log all the questions related to this analysis in an issue tracking document (Open Points).</p>"},{"location":"methodology/abrd/harvesting/#discovering-rules-from-documents","title":"Discovering rules from documents","text":"<p>This approach is used when Governmental Administration or policy group issues legal documents. We did observe this work requires courage and rigor. When using electronic documents, we used the following practices:</p> <ul> <li>Annotate the document on anything that needs some future discussion</li> <li>Copy and paste the business policy declared in the document to the rule template to clearly isolate it for future analysis. </li> <li>Work in a consistent/systematic way to ensure a good coverage. </li> <li>Check for agreement with the current business model as you go along. </li> <li>Investigate discrepancies and log them. </li> <li>Focus on stakeholder understanding (communication is key), and insist to clarify how a legal rule is interpreted by the line of business.</li> </ul> <p>One risk with this approach is that the reader is making his own interpretation of the context, and the document may not include all the cases and criteria leading to interpretations. It is sometimes difficult to get the business motivation behind a written policy. We recommend applying a rigorous method to be able to achieve the following goals: </p> <ul> <li>Get an exhaustive list of the business events under scope: log them in a table </li> <li>Get the activities, tasks, processes that support the processing of those business events </li> <li>Identify where the business rules could be enforced in the process</li> <li>Get the business motivation behind the rules </li> <li>Get explanation on rules if they are unclear, ambiguous. </li> <li>Try to extract the object model under scope, domain values by looking at the terms used by the rules\u2026. </li> </ul> <p>We should still apply agile modeling by involving the SMEs to get feedbacks on the findings, assumptions and issues. Use simple diagrams to communicate with the project stakeholders.</p>"},{"location":"methodology/abrd/harvesting/#discovering-rules-from-code","title":"Discovering rules from code","text":"<p>Discovering rules from application code is time consuming and does not lead to great results. The analyst needs to study a lot of lines of code, and procedures to find the conditional operators which are linked to business decisions. Depending on the design and code structure of the application this work can be very time consuming. It is important to remember the context of execution when the \u201cif statement\u201d is executed, some variables may change the context of this \u201cbusiness rules\u201d. With some languages using limited length to define variable names it is more difficult to relate such variables to business entities. A variable in one procedure can have a different name but the same meaning. Code mining tools exist on the market and help to extract business rules and the variables meanings. It is important to keep in mind that rules written in the past may not be relevant any more. Lastly, as stated previously, most of the rules implemented as procedural function, need a deep refactoring before deployment to a rule engine. </p> <p>Code mining is commonly requested by people as it reassures the business team that the rule harvesting starts by the existing behavior of the legacy code. Code mining is usually better used to confirm behavior of some litigious points identified from using other techniques than to try to extract all of the rules as a whole. Rule discovery with SME, using workshop sessions, may conduct to ambiguities or misconceptions. Trying to understand how the rules are implemented in the current system helps to resolve such situations.  </p>"},{"location":"methodology/abrd/harvesting/#documenting-the-business-rules","title":"Documenting the business rules","text":"<p>To document the rule, try to use the language of the business (\"problem domain\") rather than the language of the technology (\"solution domain\"). The following rule is as stated by a business user in a car rental industry: <code>A driver authorized to drive a car of group K must be over 29</code></p> <p>A rule developer may think to document the rule as:</p> <p><code>If the age of the driver is less than 29 and the requested group of the reservation is K, modify the authorized attribute of the driver accordingly.</code></p> <p>As stated above it is important to identify the different languages used to document the rule. The rule statements may evolve with time. We use different templates for documenting rules, depending of the type of discovery roadmap. </p>"},{"location":"methodology/data-intensive/","title":"Develop Data Intensive Application Methodology","text":"<p>In this section we are introducing the different elements of the software life cycles, particular to the development of intelligent applications that leverage data, machine learning models, analytics and cloud native microservices.</p> <p>The method supports lightweight development practices, to start the implementation of a MVP (Minimum Viable Product), to support scaling-up the architecture and the complexity of an end-to-end integrated solution. This method always guarantees to have deployable components at any point in time, a concept known in CI/CD (Continuous Integration, Continuous Delivery) applied to microservices, integration, data and machine learning models.</p>"},{"location":"methodology/data-intensive/#integrating-data-devops-and-ai-analytics-development-practices","title":"Integrating data - devops and AI-Analytics development practices","text":"<p>Most organizations need to manage software life cycles. The key tenets listed above imply the need for a separate lifecycle for data, because the outcome or deliverable for any of the key tenets should not be considered static and immutable. Like data, you can think of analytics as having its own lifecycle independent from the software lifecycle and the data lifecycle, although they are all complementary.</p> <p>To help achieve digital transformation, your organization should integrate three development life cycles:</p> <ul> <li>Software/Application</li> <li>AI-Analytics</li> <li>Data</li> </ul> <p></p> <p>Each development lifecycle is representative of an iterative workflow that can be used by agile development teams to craft higher value business outcomes. Lifecycles are often timeboxed iterations and can follow a fail-fast paradigm to realize tangible assets. Speed is important in business; most businesses work to meet new needs quickly. The objective of each lifecycle iteration is to address business speed efficiently and proficiently while maximizing business value. </p>"},{"location":"methodology/data-intensive/#personas","title":"Personas","text":"<p>We recommend reading this article to assemble the team to support a data-driven project where many roles support the project execution. The following table presents the icons we are using in subsquent figures, with a short description for the role.</p> <p></p>"},{"location":"methodology/data-intensive/#differences-between-analysts-and-data-scientists","title":"Differences between analysts and data scientists","text":"<p>The MITSloan did a review of the major differences between data scientists and business analysts by considering a set of common dimensions. The table below summarizes the results:</p> Analysts Data Scientists Types of data Structured mostly numeric data All data types, including unstructured data Preferred tools Statistical and modeling tools using data repository Programming language with strong statistical library, machine learning, deep learning. Use ML cluster servers Nature of work Report, predict, prescribe and optimize Explore, discover, investigate and visualize Typical educationl background Operations research, statistics, applied mathematics, predictive analytics Computer science, data science, cognitive science Mindset Entreprenaurial 69%, explore new ideas 58%, gain insights outside of formal projects  54% Entreprenaurial 96%, explore new ideas 85%, gain insights outside of formal projects  89%"},{"location":"methodology/data-intensive/#devops-lifecycle","title":"DevOps lifecycle","text":"<p>The software/application development lifecycle (SDLC) is a well-known and supports both traditional and agile development. The SDLC iterates on incorporating business requirements, adopt test driven development, continuous deployment and continuous integration. The diagram below demonstrates the iteration over recurring developer tasks to build the business intelligent application (internal loop), and the release loop (external) to continuously deliver application features to production.</p> <p></p> <p>Before enterring the development iteration cycles, there are tasks to scope the high level business challenges and opportunities, define the business case for the project, define and build the development and operation strategy, define the target infrastructure, security... </p> <p>The smaller loop represents development iteration, while the outer loop represents software release to production with continous feedback to monitor and assess features acceptance. This task list is not exhaustive, but represents a common ground for our discussion.</p> <p>\"Understanding business objectives\" is a common task in each lifecycle, but in the context of microservice solution, adoption event storming practice and domain driven design will help understanding the business process, the data aggregates, and the bounded contexts.</p> <p>The solution will group a lot of out of the shelves components and a set of microservices supporting the implementation of the business logic and the intelligent application. A lot of things need to be considered while implementing each microservice from a data point of view. </p> <p>Among the tasks described in these release and development iteration loops, we do not need to cover each of them, but may be highlight the Integration service task  which has a blue border to demonstrate integration activities between the different life cycles, like ML model integration which is developed in the MLops iteration.</p>"},{"location":"methodology/data-intensive/#dataops","title":"DataOps","text":"<p>The data development lifecycle (DataOps) places the data management philosophy into an organic and evolving cycle that is better suited to the constantly changing needs of a business. The DataOps is impacted by the DevOps and the MLOps. It iterates on both the incorporation of business requirements and on the manifestation of data. Data has value and the evaluation of data article introduces to the concepts to recognize the value of data.</p> <p></p> <p>The discover business objectives activity task, groups a set of different subjects depending of the context: data, integration, machine learning model development. The goal is to highlight the measurable outcomes expected by business stakeholders. The build a business objective article) presents the concepts and some questions that can be used to assess the general business requirements and current knowledge of the data. And the translating a business problem into an AI and data science solution practice helps the analysis team to assess what data may be needed and what kind of model to develop.</p> <p>As you can see activities are addressing data preparation and understanding, so data architecture need to be in place before doing any data sciences work.</p> <p>As part of the gather data requirements, it is important to review the dimensions of value as introduced in the \"valuation of data\" article, then formalize the value chain of the data  in the scope of the project, and finally address if the data contains the correct information to answer the business challenges and support the business process.</p> <p><code>Transform data for AI</code> and <code>Deploy data integration flow</code> tasks have different border colors to demonstrate integration activities between the different life cycles.</p>"},{"location":"methodology/data-intensive/#mlops-lifecycle","title":"MLOps lifecycle","text":"<p>The Machine learning development lifecycle supports the full spectrum of analytical work in the artificial intelligence ladder. This lifecycle incorporates model development and remediation to avoid drift. Because one of the purposes of analytics is to enable an action or a decision, MLOps relies on feedback mechanisms to help enhancing machine models and the overall analytical environment. </p> <p></p> <p>An example of a feedback mechanism is capturing data points on the positive or negative effects or outcomes from an action or a decision. This process iterates on data. </p> <p>The developed AI or Analytics model is deployed as one to many services that are integrated in the microservice architecture. So synchronization with devops team is important and part of the method.  </p> <p>Understanding the data and analytics goals task is explained in this note with the business analytic patterns.</p> <p>Defining the analytic approach task groups sub activities that help to understand the past activity and assess what kind of predictions, actions are expected by the business users. The patterns and goals analysis will help to assess for supervised or unsupervised leaning needs.</p>"},{"location":"methodology/data-intensive/#integrating-the-cycles","title":"Integrating the cycles","text":"<p>Although the three life cycles are independents, you can use them together and establish dependencies to help drive business outcomes. Each lifecycle should be agile and should be incorporated into a DevOps process for development and deployment.</p> <p></p> <p>The intersection of the three life cycles highlights the need for unified governance. The intersection between software/app and data, highlights integration and access paths to information. The intersection between data and analytics, highlights integration with the underlying data stores. The intersection between analytics and software/app highlights integration and the use of APIs or other data exchange techniques to assist in resolving complex algorithms or access requirements.</p> <p>An other interesting view, is to consider the high level artifacts built in those overlapping areas, as they are very important elements to project managed efficiently to avoid teams waiting for others. </p> <p></p> <p>Interface definitions and data schema are important elements to address as early as possible in the SDLC. Data access integration includes dedicated microservices managing the full life cycles and business operations for each major business entities of the solution. The integration can be event-based and adopt an event-driven architecture.</p> <p>The <code>data store integration</code> addresses storage of high volume data, but also access control, any transformation logic, and event data schema to be consumable by AI workbench.</p> <p>The AI model as a service can be mocked-up behind <code>Facade</code> interface so the developed microservice in need to get prescriptive scoring can be developed with less dependencies. </p> <p>Finally the integration of those three lifecycle over time can be presented in a Gantt chart to illustrate the iterations and the different focuses over time.</p> <p></p> <p>Each development life cycle includes architecture and development tasks. Architecture activities focus on defining infrastructure for runtimes and machine learning environment as well as data topology etc...</p> <p>The different iterations of the data, IA-Analytics and devops life cycle are synchronized via the integration artifacts to build. When components are ready for production, the go-live occurs and the different <code>operate</code> tasks are executed, combined with the different monitoring. From the production execution, the different teams get continuous feedbacks to improve the application.</p> <p>The AI-Analytics tasks are colored in blue and green, on purpose to show the strong dependencies between data and AI. This means the data activities should start as early as possible before doing too much of modeling. </p>"},{"location":"methodology/data-intensive/#challenges","title":"Challenges","text":"<p>There are a set of standard challenges while developing an IT solution which integrates results  from analytics model. We are listing some that we want to address, document and support as  requirements.</p> <ul> <li>How will the model be made available to developers?</li> <li>Is it a batch process updating/appending static records or real time processing on a data stream or transactional data</li> <li>How to control resource allocation for Machine Learning job.</li> <li>How to manage consistency between model, data and code: version management / data lineage</li> <li>How to assess the features needed for the training sets.</li> </ul>"},{"location":"methodology/design-thinking/","title":"Design Thinking","text":"<p>Design thinking is a methodology to standardize innovation process to develop creative solutions to business problems. This is a mindset and approach to problem-solving and innovation anchored around human-centered design: Who will be using this product? and How will this solution impact the user?</p> <p>It uses design techniques, such as personas, empathy maps, as-is scenarios, design ideation, to-be scenarios, wireframe sketches, hypothesis-driven design, and minimum viable product (MVP) definition.</p>"},{"location":"methodology/design-thinking/#personas","title":"Personas","text":"<p>Get to know the people that we intend to help with the future product. Collect information and answer a wide array of questions about them. </p> <ul> <li>Who are they?</li> <li>What are their personal demographics?</li> <li>What are their normal tasks?</li> <li>What motivates them?</li> <li>What problems do they face?</li> <li>What frustrates them?</li> </ul>"},{"location":"methodology/design-thinking/#sources","title":"Sources","text":"<ul> <li>Wikipedia </li> <li>Harvard Business Review - what is design thinking?</li> </ul>"},{"location":"techno/apicurio/","title":"Apicurio","text":"<p>Apicur.io includes a schema registry to store schema definitions.  It supports Avro, json, protobuf schemas, and an API registry to manage OpenApi and AsynchAPI definitions.</p> <p>It is a Cloud-native Quarkus Java runtime for low memory footprint and fast deployment times. It supports different persistences like Kafka, Postgresql, Infinispan and supports different deployment models.</p>"},{"location":"techno/apicurio/#registry-characteristics","title":"Registry Characteristics","text":"<ul> <li>Apicurio Registry is a datastore for sharing standard event schemas and API designs across API and event-driven architectures. In the messaging and event streaming world, data that are published to topics and queues often must be serialized or validated using a Schema.</li> <li>The registry supports adding, removing, and updating the following types of artifacts: OpenAPI, AsyncAPI, GraphQL, Apache Avro, Google protocol buffers, JSON Schema, Kafka Connect schema, WSDL, XML Schema (XSD).</li> <li>Schema can be created via Web Console, core REST API or Maven plugin</li> <li>It includes configurable rules to control the validity and compatibility.</li> <li>Client applications can dynamically push or pull the latest schema updates to or from Apicurio Registry at runtime. Apicurio is compatible with existing Confluent schema registry client applications.</li> <li>It includes client serializers/deserializers (Serdes) to validate Kafka and other message types at runtime.</li> <li>Operator-based installation of Apicurio Registry on OpenShift</li> <li>Use the concept of artifact group to collect schema and APIs logically related.</li> <li>Support search for artifacts by label, name, group, and description</li> </ul> <p>When using Kafka as persistence, special Kafka topic <code>&lt;kafkastore.topic&gt;</code> (default <code>_schemas</code>), with a single partition, is used as a highly available write ahead log.  All schemas, subject/version and ID metadata, and compatibility settings are appended as messages to this log.  A Schema Registry instance therefore both produces and consumes messages under the <code>_schemas</code> topic.  It produces messages to the log when, for example, new schemas are registered under a subject, or when updates to  compatibility settings are registered. Schema Registry consumes from the <code>_schemas</code> log in a background thread, and updates its local  caches on consumption of each new <code>_schemas</code> message to reflect the newly added schema or compatibility setting.  Updating local state from the Kafka log in this manner ensures durability, ordering, and easy recoverability.</p>"},{"location":"techno/apicurio/#apicurio-studio","title":"Apicurio Studio","text":"<p>The apicurio studio project is a standalone API design studio that can be used to create new or edit existing API designs.</p> <p>There are three runtime components in Apicurio and one Keycloak authentication server:</p> <ul> <li>apicurio-studio-api - the RESTful API backend for the user interface.</li> <li>apicurio-studio-ws - a WebSocket based API used only by the Apicurio Editor to provide real-time collaboration with other users.</li> <li>apicurio-studio-ui - the Angular 5+ based user interface.</li> </ul> <p>To run a light version of Studio use Apicurito (edit locally OpenAPI doc):</p> <pre><code>docker run -it -p 8080:8080 -p 8443:8443 apicurio/apicurio-studio-ui:latest-release\n</code></pre>"},{"location":"techno/apicurio/#v13x","title":"V1.3.x","text":"<p>As of Event Streams version 10.5, the schema registry was based on Apicur.io 1.3.x.</p> <p>Product documentation.</p> <p>The docker compose in EDA quickstarts environment: kafka-2.8-apicurio-1.3 uses 1.3.2 in-memory image for development purpose.</p> <p>Access the user interface with: http://localhost:8091/ui</p> <p>REST, maven or Java API can  be used to upload schemas.</p> <ul> <li>Add maven dependencies for rest client</li> <li>URL is with <code>/api/artifacts</code></li> </ul>"},{"location":"techno/apicurio/#manage-artifacts-with-rest-api","title":"Manage artifacts with REST api","text":"<p>Use the Registry REST API with curl to upload schema definition,  artifactType and ArtifactId in HTTP header to send the metadata about the schema:</p> <pre><code>curl -X POST -H \"Content-type: application/json; artifactType=AVRO\" -H \"X-Registry-ArtifactId: share-price\" --data '{\"type\":\"record\",\"name\":\"price\",\"namespace\":\"com.example\",\"fields\":[{\"name\":\"symbol\",\"type\":\"string\"},{\"name\":\"price\",\"type\":\"string\"}]}' http://localhost:9081/api/artifacts\n</code></pre> <p>When a schema is uploaded, you may configure content rules for schema validity and compatibility checks.</p>"},{"location":"techno/apicurio/#kafka-deserializer","title":"Kafka de/serializer","text":"<p>To configure the producer we need:</p> <ul> <li>URL of Apicurio Registry</li> <li>Apicurio Registry serializer to use with the messages (AvroKafkaSerializer)</li> <li>Strategies to map kafka message to artifact ID and to look up or register the schema used for serialization.</li> </ul> <p>Then we have to add the following properties to the Kafka producer:</p> <pre><code>// Define serializers\nprops.putIfAbsent(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\nprops.putIfAbsent(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, io.apicurio.registry.utils.serde.AvroKafkaSerializer.class.getName());\n // Configure Service Registry location and ID strategies\nprops.putIfAbsent(AbstractKafkaSerDe.REGISTRY_URL_CONFIG_PARAM, \"http://localhost:8091/api\");\n</code></pre> <ul> <li>Define the <code>strategy</code> to map the Kafka message to an artifact ID in Apicurio Registry</li> <li>Define the <code>Strategy</code> to look up or register the schema used for serialization in Apicurio Registry</li> </ul> <pre><code>props.putIfAbsent(AbstractKafkaSerializer.REGISTRY_ARTIFACT_ID_STRATEGY_CONFIG_PARAM,\n            SimpleTopicIdStrategy.class.getName());\nprops.putIfAbsent(AbstractKafkaSerializer.REGISTRY_GLOBAL_ID_STRATEGY_CONFIG_PARAM,\n            FindBySchemaIdStrategy.class.getName());\n</code></pre> <ul> <li>Create the schema within the registry using REST API client:</li> </ul> <pre><code>private static void createSchemaInServiceRegistry(String artifactId, String schema) throws Exception {\n    try {\n          ArtifactMetaData metaData = registryClient.getArtifactMetaData(getConfiguration().getArtifactId());\n          logger.info(\"Schema already in registry: \" + metaData.toString());\n      } catch (WebApplicationException e) {\n          InputStream content = new ByteArrayInputStream(avroSchema.toString().getBytes(StandardCharsets.UTF_8));\n          ArtifactMetaData metaData = \n            registryClient.createArtifact(getConfiguration().getArtifactId(), ArtifactType.AVRO, IfExistsType.RETURN, content);\n          logger.info(\"Schema created in registry:\" + metaData.toString());\n      }\n}\n</code></pre> <p>Example of output:</p> <pre><code>Successfully created Avro Schema artifact in Service Registry: \nArtifactMetaData{name='BasicMessage', description='null', labels='[]', createdBy='null', createdOn=1615340019637, modifiedBy='null', modifiedOn=1615340019637, id='test', version=1, type=AVRO, globalId=18, state=ENABLED, properties={}}\n</code></pre> <p>See simplest code in the <code>apicurio</code> folder of Kafka Studies</p> <p>Or Kafka producer code template in Kafka with API and apicur.io and consumer.</p>"},{"location":"techno/apicurio/#apicurio-22x","title":"Apicur.io 2.2.x","text":"<p>Event Streams version 11.0.x uses Apicur.io 2.2.x. This version is using different APIs and rest client code.</p> <p>It supports OpenAPI, AsyncAPI, GraphQL, Apache Avro, Google protocol buffers, JSON Schema, Kafka Connect schema, WSDL, XML Schema (XSD)</p> <p>The registry can be configured to store data in various back-end storage systems depending on use-case, including Kafka, PostgreSQL, and Infinispan.</p> <p>Quarkus dev mode with the extension: <code>quarkus-apicurio-registry-avro</code> start apicurio 2.x container, and reactive messaging with the <code>io.apicurio.registry.serde.avro.AvroKafkaSerializer</code> serializer create schema definition directly to the registry.</p> <p>See Quarkus apicur.io dev mode guide and the kafka - apicur.io guide.</p>"},{"location":"techno/apicurio/#installation-in-the-context-of-event-streams","title":"Installation in the context of event streams","text":"<ul> <li>For Event Streams - schema registry deployment the installation is done by adding the following element into the kafka cluster definition (see this deployment):</li> </ul> <pre><code>  apicurioRegistry:\n    livenessProbe:\n      initialDelaySeconds: 120\n</code></pre> <p>The Event Stream UI includes a way to manage schema by human. For code based management we need to have a user with ACL to create schema.</p> <ul> <li>We can install a standalone Apicur.io registry outside of IBM Event Streams, which may make sense when we need to manage multiple Kafka clusters.</li> </ul> <p>Via OpenShift Operator hub or via subscription.yaml see eda-gitops-catalog/apicurio/operator</p> <p>For application code that uses version 2.x quarkus-reactive-kafka-producer template or the order demo app (producer) and the simplest string consumer.</p>"},{"location":"techno/apicurio/#installing-standalone-in-ocp","title":"Installing standalone in OCP","text":"<ul> <li>Install Apicurio operator (search from registry in Operator Hub for <code>Apicurio Registry Operator</code>)</li> <li>Get Kafka bootstrap internal listener address</li> <li>Define one registry instance</li> </ul> <pre><code>\n</code></pre> <ul> <li>Create the <code>storage-topic</code> and <code>global-id-topic</code> topics</li> </ul>"},{"location":"techno/apicurio/#reactive-messaging","title":"Reactive messaging","text":"<p>For the Microprofile Reactive messaging configuration consider the following:</p> <ul> <li>Set at least the value serializer to use AvroKafkaSerializer so it can transparently manage auto registry of  the schema. (example for the <code>orders</code> channel). NOT needed any more with quarkus that will use as the <code>apicurio.avro</code> extension is in the libraries. </li> </ul> <pre><code>mp.messaging.outgoing.orders.key.serializer=org.apache.kafka.common.serialization.StringSerializer\nmp.messaging.outgoing.orders.value.serializer=io.apicurio.registry.serde.avro.AvroKafkaSerializer\nmp.messaging.outgoing.orders.apicurio.registry.auto-register=true\nmp.messaging.outgoing.orders.apicurio.registry.artifact-id=io.apicurio.registry.utils.serde.strategy.SimpleTopicIdStrategy\nmp.messaging.outgoing.orders.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.GetOrCreateIdStrategy\n</code></pre> <p>if you want to configure cross channels then move the declaration at kafka level:</p> <pre><code>kafka.apicurio.registry.auto-register=true\nkafka.apicurio.registry.url=http://localhost:8081/apis/registry/v2\n</code></pre> <ul> <li>For securized connection, add TLS and authentication</li> </ul> <pre><code># Use same schema registry server cross channel\n%prod.kafka.apicurio.registry.url=${ES_APICURIO_URL}\n# Or by using a per channel setting\n%prod.mp.messaging.outgoing.orders.apicurio.registry.url=${ES_APICURIO_URL}\n</code></pre> <p>Be sure the URL finishes with <code>/apis/registry/v2</code>. URL may be set in a config map, as in the  eda-demo-order-gitops app-eda-demo-order-ms project.</p> <p>In the Quarkus guide in quarkus-quickstarts, running in <code>quarkus dev</code>, we need to look at the trace to get the apicurio URL, then we can use the api to search for the movies schema already created. The id of the schema is the topic name + <code>-value</code>.</p> <p><pre><code>http://localhost:61272/apis/registry/v2/search/artifacts\n</code></pre> For mTLS authentication to schema registry see the Store simulator project.</p> <p>For order consumer with schema registry see the eda-demo-order-mp-consumer project.</p>"},{"location":"techno/apicurio/#developer-experience","title":"Developer experience","text":"<ul> <li>Define your microservice with JAXRS and Swagger annotation. Here is a guide for quarkus openapi and swagger-ui for details. Below is an example of properties to add to a quarkus <code>application.properties</code> file:</li> </ul> <pre><code>mp.openapi.extensions.smallrye.info.title=Example API\nmp.openapi.extensions.smallrye.info.version=1.0.0\nmp.openapi.extensions.smallrye.info.description=Just an example service\nmp.openapi.extensions.smallrye.info.termsOfService=Your terms here\nmp.openapi.extensions.smallrye.info.contact.email=techsupport@example.com\nmp.openapi.extensions.smallrye.info.contact.name=API Support\nmp.openapi.extensions.smallrye.info.contact.url=http://exampleurl.com/contact\nmp.openapi.extensions.smallrye.info.license.name=Apache 2.0\nmp.openapi.extensions.smallrye.info.license.url=https://www.apache.org/licenses/LICENSE-2.0.html\n</code></pre> <ul> <li>When starting from the openAPI document, we can push the api as file to  the <code>META-INF/openapi.yaml</code> or as a <code>.json</code></li> <li>Get the OpenAPI definition using curl http://localhost:8080/q/openapi</li> <li>Define an artifact group to group elements inside Apicurio Registry - can be by environment or can be line of business  or any thing to group elements.</li> <li>Decide if you want to use Avro or JSON Schema - Apicurio has both serdes to integrate into you code:</li> </ul>"},{"location":"techno/apicurio/#producer","title":"Producer","text":"<p>Define AsyncAPI.</p>"},{"location":"techno/apicurio/#consumer","title":"Consumer","text":"<p>The consumer uses the GenericRecord class</p> <pre><code>    @Incoming(\"shipments\")\n    @Outgoing(\"internal-plan-stream\")                             \n    @Broadcast                                              \n    @Acknowledgment(Acknowledgment.Strategy.POST_PROCESSING)\n    public Uni&lt;ShipmentPlans&gt; process(GenericRecord spse){  \n        //GenericRecord spse = evt.getPayload();\n        int idx = 0;\n        logger.info(spse);\n        CloudEvent ce = jsonb.fromJson(spse.toString(), CloudEvent.class);\n        for (ShipmentPlanEvent spe : ce.data.Shipments) {\n            ShipmentPlan plan = ShipmentPlan.from(spe);\n            logger.info(plan.toString());\n            plans.put(\"SHIP_\" + idx,plan);\n            idx++;s\n        }\n        ShipmentPlans shipmentPlans = new ShipmentPlans();\n        shipmentPlans.plans = new ArrayList&lt;ShipmentPlan&gt;(this.plans.values());\n        return Uni.createFrom().item(shipmentPlans);\n    }\n</code></pre>"},{"location":"techno/apicurio/#read-more","title":"Read more","text":"<ul> <li>Official demo</li> <li>Using Debezium With the Apicurio API and Schema Registry</li> <li>Quarkus apicur.io dev mode guide</li> <li>kafka - apicur.io Quarkus guide.</li> <li>Clement Escoffier's blog</li> <li>Kafka Quarkus reference guide</li> </ul>"},{"location":"techno/apicurio/#avro","title":"Avro","text":"<p>Quick getting started on Avro with Java. We need to:</p> <ul> <li>Define .avsc file (JSON doc) for each type. Schemas are composed of primitive types (null, boolean, int, long, float, double, bytes, and string)  and complex types (record, enum, array, map, union, and fixed).</li> <li>Use Avro Maven plugin to generate Java beans from the schema definition</li> </ul> <p>To access avro classes directly add this dependencies:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.apache.avro&lt;/groupId&gt;\n  &lt;artifactId&gt;avro&lt;/artifactId&gt;\n  &lt;version&gt;1.10.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"techno/apicurio/#from-schema-to-beans","title":"From schema to beans","text":"<p>News</p> <p>With Quarkus 2.x the extension <code>apicurio-registry-avro</code> auto generates the java classes from .avcs declaration in <code>src/main/avro</code>.  The <code>.avsc</code> file needs to be an union of schema definitions. See this quarkus guide,  quarkus dev and apicurio  and this reactive messaging project See Clement Escoffier's blog</p> <p>Define one to many <code>.avcs</code> file in the <code>src/main/avro</code> folder. </p> <p>See note above, the following is for older quarkus version or pure java: to generate java beans from those schema  definitions, use the avro maven plugin. The order of schema processing is important to build the dependencies  before the records using them (see imports statement below):</p> <pre><code>&lt;plugin&gt;\n  &lt;groupId&gt;org.apache.avro&lt;/groupId&gt;\n  &lt;artifactId&gt;avro-maven-plugin&lt;/artifactId&gt;\n  &lt;version&gt;${avro.version}&lt;/version&gt;\n  &lt;executions&gt;\n    &lt;execution&gt;\n      &lt;phase&gt;generate-sources&lt;/phase&gt;\n      &lt;goals&gt;\n        &lt;goal&gt;schema&lt;/goal&gt;\n      &lt;/goals&gt;\n      &lt;configuration&gt;\n        &lt;sourceDirectory&gt;src/main/avro/&lt;/sourceDirectory&gt;\n        &lt;outputDirectory&gt;src/main/java/&lt;/outputDirectory&gt;\n        &lt;!-- need to specify order for dependencies --&gt;\n        &lt;imports&gt;\n          &lt;import&gt;src/main/avro/Address.avsc&lt;/import&gt;\n          &lt;import&gt;src/main/avro/OrderEvent.avsc&lt;/import&gt;\n        &lt;/imports&gt;\n        &lt;stringType&gt;String&lt;/stringType&gt;\n      &lt;/configuration&gt;\n    &lt;/execution&gt;\n  &lt;/executions&gt;\n&lt;/plugin&gt;\n</code></pre> <p>The beans created will be in the Java package as defined by the <code>namespace attribute</code>:</p> <pre><code>  \"namespace\": \"ibm.eda.demo.app.infrastructure.events\",\n  \"type\": \"record\",\n  \"name\": \"OrderEvent\",\n  \"fields\": []\n</code></pre>"},{"location":"techno/apicurio/#cloudevent-and-avro","title":"CloudEvent and Avro","text":"<p>Need to get the following maven dependencies</p> <pre><code>    &lt;dependency&gt;\n      &lt;groupId&gt;io.cloudevents&lt;/groupId&gt;\n      &lt;artifactId&gt;cloudevents-kafka&lt;/artifactId&gt;\n      &lt;version&gt;2.3.0&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n      &lt;artifactId&gt;quarkus-apicurio-registry-avro&lt;/artifactId&gt;\n    &lt;/dependency&gt;\n</code></pre> <p>With <code>quarkus-apicurio-registry-avro</code> a set of beans are created under target/generated-source/avsc</p> <pre><code>     CloudEventBuilder ceb = CloudEventBuilder.v1().withSource(URI.create(\"https://github.com/jbcodeforce/eda-demo-order-count-kstream/\"))\n        .withType(\"OrderEvent\");\n        Address address = Address.newBuilder()\n            .setStreet(\"mission street\")\n            .setCity(\"San Francisco\")\n            .setState(\"CA\")\n            .setCountry(\"USA\")\n            .setZipcode(\"94000\")\n            .build();\n        OrderEvent oe = OrderEvent.newBuilder()\n        .setOrderID(\"O01\")\n        .setCustomerID(\"C01\")\n        .setProductID(\"P01\")\n        .setQuantity(10)\n        .setStatus(\"Pending\")\n        .setCreationDate(\"2022-03-10\")\n        .setUpdateDate(\"2022-03-15\")\n        .setEventType(\"OrderCreated\")\n        .setShippingAddress(address)\n        .build();\n        System.out.println(oe.toString());\n        String id = UUID.randomUUID().toString();\n        ByteBuffer bb = OrderEvent.getEncoder().encode(oe);\n        System.out.println(bb.toString());\n        byte[] data = new byte[bb.remaining()];\n        bb.get(data, 0, data.length);\n        CloudEvent ce = ceb.newBuilder().withId(id).withData(\"application/avro\",  data).build();\n        System.out.println(ce.toString());\n</code></pre>"},{"location":"techno/apicurio/#from-beans-to-schema","title":"From beans to schema","text":"<p>The Jackson parser offers such  capability, so you can add a small program to create a schema from your beans, using  the AvroFactory, SchemaGenerator...</p> <p>Add this to your pom:</p> <pre><code> &lt;dependency&gt;\n    &lt;groupId&gt;io.apicurio&lt;/groupId&gt;\n    &lt;artifactId&gt;apicurio-registry-utils-converter&lt;/artifactId&gt;\n    &lt;version&gt;${apicurio.registry.version}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt;\n    &lt;artifactId&gt;jackson-dataformat-avro&lt;/artifactId&gt;\n    &lt;version&gt;${jackson2-version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Here how to get the <code>RootType.class</code> schema definition using Avro API:</p> <pre><code>ObjectMapper mapper = new ObjectMapper(new AvroFactory());\nAvroSchemaGenerator gen = new AvroSchemaGenerator();\nmapper.acceptJsonFormatVisitor(RootType.class, gen);\nAvroSchema schemaWrapper = gen.getGeneratedSchema();\n\norg.apache.avro.Schema avroSchema = schemaWrapper.getAvroSchema();\nString asJson = avroSchema.toString(true);\n</code></pre>"},{"location":"techno/apicurio/#install-on-openshift-with-kafka-as-persistence-storage","title":"Install on OpenShift with Kafka as persistence storage","text":"<p>See the notes here which can be summarized as:</p> <ul> <li>Install Strimzi operator - create a Strimzi cluster (See vaccine-gitops strimzi env)</li> <li>Install Apicurio operator (search from registry in Operator Hub) - define one registry instance.</li> <li>Get Kafka bootstrap internal listener address</li> <li>Create the <code>storage-topic</code> and <code>global-id-topic</code> topics</li> <li>Create the Apicurio instance using the bootstrap URL. Below is an example of registry definition using Kafka as persistence.  (See the vaccine-gitops Apicurio env)</li> </ul> <pre><code>apiVersion: registry.apicur.io/v1\nkind: ApicurioRegistry\nmetadata:\n  name: eda-apicurioregistry\nspec:\n  configuration:\n    persistence: kafkasql\n    kafkasql:\n      bootstrapServers: \"vaccine-kafka-kafka-bootstrap.vaccine-order.svc:9092\"\n      security:\n        tls:\n          keystoreSecretName: tls-user\n          truststoreSecretName: kafka-cluster-ca-cert\n</code></pre>"},{"location":"techno/apicurio/#manage-artifacts-with-maven","title":"Manage artifacts with Maven","text":"<p>You can use the Apicurio Registry Maven plug-in to upload or download registry artifacts as part of your development build process. The plugin is:</p> <pre><code>&lt;plugin&gt;\n  &lt;groupId&gt;io.apicurio&lt;/groupId&gt;\n  &lt;artifactId&gt;apicurio-registry-maven-plugin&lt;/artifactId&gt;\n  &lt;version&gt;${registry.version}&lt;/version&gt;\n  &lt;executions&gt;\n    &lt;execution&gt;\n      &lt;phase&gt;generate-sources&lt;/phase&gt;\n      &lt;goals&gt;\n        &lt;goal&gt;register&lt;/goal&gt;\n      &lt;/goals&gt;\n      &lt;configuration&gt;\n        &lt;registryUrl&gt;http://localhost:8090/api&lt;/registryUrl&gt;\n        &lt;artifactType&gt;AVRO&lt;/artifactType&gt;\n        &lt;artifacts&gt;\n          &lt;!-- the name of the tag will be the id of the schema\n\n          --&gt;\n          &lt;order-schema1&gt;src/main/avro/Address.avsc&lt;/order-schema1&gt;\n          &lt;order-schema2&gt;src/main/avro/OrderCreatedEvent.avsc&lt;/order-schema2&gt;\n          &lt;order-schema3&gt;src/main/avro/OrderEvent.avsc&lt;/order-schema3&gt;\n        &lt;/artifacts&gt;\n      &lt;/configuration&gt;\n    &lt;/execution&gt;\n  &lt;/executions&gt;\n&lt;/plugin&gt;\n</code></pre> <p>but it is recommended to put this plugin in a special maven profile if you do not have connection to an apicurio server while building apps.</p>"},{"location":"techno/apicurio/#code-using-apicurio","title":"Code using Apicurio","text":"<ul> <li>Quarkus kafka producer template</li> <li>Postgresql, Debezium Outbox Quarkus plugin and Debezium change data capture with Kafka Connect</li> <li>Freezer service</li> </ul>"},{"location":"techno/apicurio/#useful-links","title":"Useful links","text":"<ul> <li>Apicurio Registry Operator community in GitHub</li> <li>Integrating Spring boot app with Apicurio</li> <li>Apicurio user interface demo</li> </ul>"},{"location":"techno/atlas/","title":"Apache Atlas","text":"<p>Apache Atlas is a data governance tool which facilitates gathering, processing, and maintaining metadata.</p> <p>The architecture illustrates that Atlas exposes APIs to add and query elements of the repository, but also is integrated with Kafka for asynchronous communication.</p> <p></p> <p>The Core framework includes a graph database based on JanusGraph.</p> <p>The Kafka integration may be used to consume metadata change events from Atlas.</p>"},{"location":"techno/atlas/#key-features","title":"Key features","text":"<ul> <li>Centralized metadata management platform.</li> <li>Data classification based on rules and regex. Identify the incoming data and filter them out based on those classifications.</li> <li>Data lineage: shows the origin, movement, transformation and destination of data.</li> </ul>"},{"location":"techno/atlas/#concepts","title":"Concepts","text":"<p>Some important concepts to know:</p> <ul> <li>a Type in Atlas is a definition of how particular types of metadata objects are stored and accessed.  A type represents one or a collection of attributes that define the properties for the metadata object.</li> <li>An <code>Entity</code> is an instance of a Type.</li> <li> <p>A type has a metatype. Atlas has the following metatypes:</p> <ul> <li>Primitive metatypes: boolean, byte, short, int, long, float, double, biginteger, bigdecimal, string, date</li> <li>Enum metatypes</li> <li>Collection metatypes: array, map</li> <li>Composite metatypes: Entity, Struct, Classification, Relationship</li> <li> <p>Atlas comes with a few pre-defined system types: Referenceable, Asset, Infrastructure, DataSet, Process. The ones very interesting are:</p> </li> <li> <p>Infrastructure extends Asset and may be used for cluster, host,...</p> </li> <li>DataSet extends Referenceable, represents a type that stores data. Expected to have a Schema to define attributes.</li> <li>Process extends Asset represents any data transformation operation.</li> <li>Relationships to describe connections between entities.</li> <li>We can define <code>Classification</code> which can be associated to entities but are not attributes: </li> </ul> </li> </ul> <pre><code> {\n    \"category\": \"CLASSIFICATION\",\n    \"name\": \"customer_PII\",\n    \"description\": \"Used for classifying a data which contains customer personal information, hence indicating confidential private\",\n    \"typeVersion\": \"1.0\",\n    \"attributeDefs\": [],\n    \"superTypes\": []\n}\n</code></pre> <ul> <li>A type can extend another type: A <code>kafka_topic_schema</code> is an array of <code>kafka_message_schema</code>:</li> </ul> <pre><code>{\n        \"superTypes\" : [ \"kafka_topic\" ],\n        \"category\" : \"ENTITY\",\n        \"name\" : \"kafka_topic_and_schema\",\n        \"attributeDefs\" : [\n          {\n            \"name\" : \"value_schema\",\n            \"typeName\" : \"array&lt;kafka_message_schema&gt;\",\n            \"isOptional\" : true,\n            \"cardinality\" : \"SINGLE\",\n            \"valuesMinCount\" : 1,\n            \"valuesMaxCount\" : 1,\n            \"isUnique\" : false,\n            \"isIndexable\" : false\n          },\n        ]\n}\n</code></pre>"},{"location":"techno/atlas/#remarks","title":"Remarks","text":"<p>Each of the predefined sources have matching type definitions. For Kafka, it may be needed to adapt  the definition or develop new definition to support and end-to-end governance of Kafka components.</p>"},{"location":"techno/atlas/#getting-started","title":"Getting started","text":"<ul> <li>Start with docker:</li> </ul> <pre><code>docker run -d -p 21000:21000 -p 21443:21443 --name atlas sburn/apache-atlas /opt/apache-atlas-2.1.0/bin/atlas_start.py\n</code></pre> <p>Login in as <code>admin/admin</code>.</p> <ul> <li>Start with a docker compose:</li> </ul> <pre><code>services:\n  atlas:\n    container_name: atlas\n    hostname: atlas\n    image: sburn/apache-atlas\n    ports:\n      - 21000:21000 \n      - 21443:21443\n    environment:\n      MANAGE_LOCAL_HBASE: \"true\"\n      MANAGE_LOCAL_SOLR: \"false\"\n    command:\n      /opt/apache-atlas-2.1.0/bin/atlas_start.py\n    volumes:\n      - $PWD/environment/atlas/data:/tmp/data/\n</code></pre> <ul> <li>Define new types See project eda-governance: </li> </ul> <p>Her is an example defining a Kafka_Cluster type to be an Infrastructure</p> <pre><code>\"entityDefs\": [\n    {\n      \"superTypes\": [\n        \"Infrastructure\"\n      ],\n      \"category\": \"ENTITY\",\n      \"name\": \"eda_kafka_cluster\",\n      \"description\": \"a Kafka Cluster groups multiple Kafka Brokers and references topics\",\n      \"typeVersion\": \"1.0\",\n      \"attributeDefs\": [\n        {\n          \"name\": \"cluster.name\",\n          \"typeName\": \"string\",\n          \"isOptional\": false,\n          \"cardinality\": \"SINGLE\",\n          \"valuesMinCount\": 1,\n          \"valuesMaxCount\": 1,\n          \"isUnique\": true,\n          \"isIndexable\": true\n        }, \n        ....\n</code></pre> <ul> <li>Define dataset entities</li> </ul> <pre><code>\"entities\": [\n    {\n      \"typeName\": \"eda_kafka_cluster\",\n      \"createdBy\": \"maas_service\",\n      \"attributes\": {\n        \"qualifiedName\": \"assets-arch-eda.eda-dev\",\n        \"cluster.name\": \"eda-dev\",\n        \"description\": \"EDA team's event streams 2020 cluster\",\n        ...\n</code></pre> <p>The qualifiedName is important to define some naming convention like <code>&lt;kafka_component&gt;@&lt;clustername&gt;</code> for example.  Also important that entities represent instances. So if the same topic is defined into two clusters we need to have two entities in Atlas using the different qualifiedNames.</p> <ul> <li>Define process entities</li> </ul> <p>For example an application is a process:</p> <pre><code>\n</code></pre> <ul> <li>Define relationship between any defined types. Example a topic is linked to a cluster:</li> </ul> <pre><code>{\n \"category\": \"ENTITY\",\n  \"name\": \"eda_kafka_topic\",\n  \"superTypes\": [\n                \"DataSet\"\n            ],\n  \"attributeDefs\": [\n    {\n      \"name\": \"cluster\",\n      \"typeName\": \"eda_kafka_cluster\",\n      \"isOptional\": false,\n      \"cardinality\": \"SINGLE\",\n      \"isUnique\": false,\n      \"isIndexable\": true\n    },\n    {\n      \"name\": \"clusterQualifiedName\",\n      \"typeName\": \"string\",\n      \"isOptional\": false,\n      \"cardinality\": \"SINGLE\",\n      \"isUnique\": false,\n      \"isIndexable\": true\n    }]\n}\n</code></pre> <p>Then the entity will use the following setting in the cluster name:</p> <pre><code>{\n        \"typeName\": \"eda_kafka_topic\",\n        \"attributes\": {\n          \"qualifiedName\": \"items@eda-dev\",\n          \"name\": \"items\",\n          \"cluster\": {\"uniqueAttributes\": {\"qualifiedName\": \"eda-dev\"}, \"typeName\": \"eda_kafka_cluster\"},\n          \"clusterQualifiedName\": \"eda-dev\"\n        }\n}\n</code></pre> <p>We can also define the composition, like a broker is included in the cluster and will not live outside of a cluster:</p> <pre><code> {\n      \"superTypes\": [\n        \"Infrastructure\"\n      ],\n      \"category\": \"ENTITY\",\n      \"name\": \"eda_kafka_cluster\",\n    ...\n    \"attributeDefs\": [\n    {\n    \"name\": \"brokers\",\n    \"typeName\": \"array&lt;eda_kafka_broker&gt;\",\n    \"isOptional\": true,\n    \"cardinality\": \"SET\",\n    \"valuesMinCount\": 1,\n    \"valuesMaxCount\": 3000,\n    \"isUnique\": false,\n    \"isComposite\": true,\n    \"isIndexable\": false,\n    \"includeInNotification\": false,\n    \"searchWeight\": -1,\n    \"relationshipTypeName\": \"eda_kafka_topic\"\n}\n</code></pre>"},{"location":"techno/atlas/#data-lineage","title":"Data lineage","text":"<p>Atlas will address part of the data lineage as we can declare the physical dependencies between components:</p> <p></p>"},{"location":"techno/atlas/#continuous-visibility-of-flows","title":"Continuous visibility of flows","text":"<p>Atlas can listen to Kafka topic to get updates to propagate to the entities. But there is no real time view of the data inside of the topic, for example, to see where  a data land. </p>"},{"location":"techno/atlas/#deploy-to-kubernetes","title":"Deploy to Kubernetes","text":"<p>Apache Atlas requires 3 applications to be installed</p> <ul> <li>Solr: This is used to index the Atlas Data so that we can search the data in Atlas UI.</li> <li>Cassandra: acts as a backend and stores the data ingested by Apache Atlas, which is nothing but the Metadata.</li> <li>Zookeeper: for cluster management.</li> </ul>"},{"location":"techno/atlas/#source-of-readings","title":"Source of readings","text":"<ul> <li>REST API v2</li> <li>Model governance with Atlas - part 1</li> <li>Model governance with Atlas - part 2</li> <li>Model governance with Atlas - part 3</li> <li>Atlas Helm Chart with Solr and cassandra</li> </ul>"},{"location":"techno/code-engine/","title":"Code engine","text":"<p>Code engine is a fully managed, serverless platform that runs any containerized workloads.</p>"},{"location":"techno/cos/","title":"Cloud Object Storage","text":"<p>IBM Cloud Object Storage is a highly scalable cloud storage service, designed for high durability, resiliency and security. Store, manage and access your data via our self-service portal and RESTful APIs.</p> <p>Comparable to AWS S3 service</p>"},{"location":"techno/cos/#benefits","title":"Benefits","text":"<ul> <li>Reduce storage costs</li> <li>Reduce downtime</li> <li>Simple access via REST APIs with object based access for developer</li> <li>Secure data using automatic server-side encryption and get encryption options with keys managed by IBM Key Protect key management system, or encryption with keys that you manage.</li> </ul>"},{"location":"techno/cos/#use-cases","title":"Use cases","text":"<ul> <li>Backup and recovery</li> <li>Data archiving</li> <li>cloud native app: store large amounts of unstructured IoT data or mobile data</li> <li>AI and big data analytics: build data lake for analytics</li> </ul>"},{"location":"techno/cos/#getting-started","title":"Getting started","text":"<p>Product documentation to learn how to create bucket and access it via API</p>"},{"location":"techno/cos/#analytics","title":"Analytics","text":"<ul> <li>Query data in place: IBM Cloud SQL Query is a fully managed service that lets developers analyze and transform data stored across multiple files in various formats using ANSI SQL statements. </li> <li>Perform Apache Spark analytics</li> <li>Perform intelligent data discover using Watson Knowledge Catalog</li> </ul>"},{"location":"techno/cos/#code","title":"Code","text":"<p>A demo of loud object storage as Sink of Kafka topic. cos tutorial and this demo gitops to deploy Kafka connector</p>"},{"location":"techno/docker/","title":"Quick docker summary","text":""},{"location":"techno/docker/#why","title":"Why","text":"<p>Lower memory consumption than VM. Define configuration, dependencies, command in a unique file (Dockerfile). It uses registry to store images.  Image is like a VM image. Container is a running instance of an image. Docker engine helps to manage the  container life cycle and  exposes API for the CLI.</p> <p></p>"},{"location":"techno/docker/#value-propositions-for-container","title":"Value propositions for container","text":"<p>Just to recall the value of using container for the cloud native application are the following:</p> <ul> <li>Docker ensures consistent environments from development to production. Docker containers are configured to maintain all configurations and dependencies internally.</li> <li>Docker containers allows you to commit changes to your Docker image and version control them. It is very easy to rollback to a previous version of your Docker image. This whole process can be tested in a few minutes.</li> <li>Docker is fast, allowing you to quickly make replications and achieve redundancy.</li> <li>Isolation: Docker makes sure each container has its own resources that are isolated from other containers</li> <li>Removing an app/ container is easy and won\u2019t leave any temporary or configuration files on your host OS.</li> <li>Docker ensures that applications that are running on containers are completely segregated and isolated from each other, granting you complete control over traffic flow and management</li> </ul> <p>The container filesystem is represented as a list of read-only layers stacked on top of each other using a storage driver. The layers are generated when commands are executed during the Docker image build process. The top layer has read-write permissions. Docker daemon configuration is managed by the Docker configuration file (/etc/docker/daemon.json) and Docker daemon startup options are usually controlled by the systemd unit: <code>docker</code>. With environment variables you can control one container, while using <code>linked containers</code>, docker automatically copies all environment variables from one container to another.</p>"},{"location":"techno/docker/#dockerfile","title":"Dockerfile","text":"<p>ENTRYPOINT specifies the default command to execute when the image runs in a container CMD provides the default arguments for the ENTRYPOINT instruction</p> <p>Example to build a custom Apache Web Server container image.</p> <pre><code>FROM ubi7/ubi:7.7\nENV PORT 8080\nRUN yum install -y httpd &amp;&amp; yum clean all\nRUN sed -ri -e \"/^Listen 80/c\\Listen ${PORT}\" /etc/httpd/conf/httpd.conf &amp;&amp; \\\n    chown -R apache:apache /etc/httpd/logs/ &amp;&amp; \\\n    chown -R apache:apache /run/httpd/\nUSER apache\nEXPOSE ${PORT}\nCOPY ./src/ /var/www/html/\nCMD [\"httpd\", \"-D\", \"FOREGROUND\"]\n</code></pre> <p>Best practices for writing Dockerfiles.</p> <p>Run java using openJDK image.</p>"},{"location":"techno/docker/#how-to-run-without-docker-desktop","title":"How to run without Docker Desktop","text":"<ul> <li>Install a VM hyperviser: <code>brew install hyperkit</code></li> <li>Install minikube: <code>brew install minikube</code></li> <li>Install CLIs</li> </ul> <pre><code>brew install docker\nbrew install docker-compose\n</code></pre> <ul> <li>Use minikube: <code>minikub start --vm-driver=hyperkit</code></li> <li>Get docker env of the minikube: <code>minikube docker-env</code> or better <code>eval $(minikube docker-env)</code> so env variables are set to be used by the docker CLI. Evaluating the docker-env is only valid for the current terminal. </li> <li>Use docker CLI, it should contact the docker daemon running inside minikube.</li> <li>If the minikube pods are not able to contact internet add a names</li> </ul> <pre><code>\"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io\n</code></pre>"},{"location":"techno/docker/#some-docker-and-docker-compose-tricks","title":"Some docker and docker compose tricks","text":""},{"location":"techno/docker/#tricks","title":"Tricks","text":"<ul> <li>Modify the PATH:  <code>ENV PATH=\"/opt/ibm/db2/V11.5/bin:${PATH}\"</code></li> </ul>"},{"location":"techno/docker/#run-a-ubuntu-image","title":"Run a ubuntu image","text":"<p>This could be a good approach to demonstrate a linux base app.</p> <pre><code>docker run --name my-linux  --detach ubuntu:20.04 tail -f /dev/null\n# connect\ndocker exec -ti my-linux -v $(pwd):/home bash\n# Update apt inventory and install jdk, maven, git, curl, ... \napt update\napt install -y openjdk-11-jre-headless maven git curl vim\n# You have a linux based development environment\n</code></pre> <p>This project includes a Dockerfile-ubuntu to build a local image with the above tools.</p> <pre><code>docker build -t jbcodeforce/myubuntu:20.04 -f Dockerfile-ubuntu .\n</code></pre>"},{"location":"techno/docker/#docker-volume","title":"Docker volume","text":"<p>For mounting host directory, the host directory needs to be configured with ownership and permissions allowing access to the container.</p> <pre><code>docker run -v /var/dbfiles:/var/lib/mysql rhmap47/mysql\n</code></pre>"},{"location":"techno/docker/#reclaim-disk-space","title":"Reclaim disk space","text":"<pre><code>docker system df\n</code></pre> <p>what-to-do-when-docker-runs-out-of-space post</p>"},{"location":"techno/docker/#docker-network","title":"Docker network","text":"<pre><code>docker network list\n# create a network\ndocker network create kafkanet\n# Assess which network a container is connected to\ndocker inspect 71582654b2f4 -f \"{{json .NetworkSettings.Networks }}\"\n\n&gt; \"bridge\":{\"IPAMConfig\":null,\"Links\":null,\"Aliases\":null,\"NetworkID\":\"7db...\"\n\n# disconnect a container to its network\ndocker network disconnect bridge 71582654b2f4\n# Connect an existing container to a network\ndocker network connect docker_default containernameorid\n</code></pre> <p>Inside the container the host name is in DNS: <code>host.docker.internal</code>.  The other solution is to use --network=\"host\" in docker run command, then 127.0.0.1 in the docker container will point to the docker host.</p>"},{"location":"techno/docker/#start-a-docker-bypassing-entry-point-or-cmd","title":"Start a docker bypassing entry point or cmd","text":"<pre><code>docker run -ti --entrypoint \"/bin/bash\" imagename\n</code></pre> <p>or use the command after the image name:</p> <pre><code>docker run -ti imagename /bin/bash \n</code></pre>"},{"location":"techno/docker/#docker-build-image-with-tests-and-env-variables","title":"Docker build image with tests and env variables","text":"<p>Inject the environment variables with --build-arg</p> <pre><code>docker build --network host \\\n            --build-arg KAFKA_BROKERS=${KAFKA_BROKERS} \\\n            --build-arg KAFKA_APIKEY=${KAFKA_APIKEY} \\\n            --build-arg POSTGRESQL_URL=${POSTGRESQL_URL}  \\\n            --build-arg POSTGRESQL_USER=${POSTGRESQL_USER} \\\n            --build-arg POSTGRESQL_PWD=${POSTGRESQL_PWD} \\\n            --build-arg JKS_LOCATION=${JKS_LOCATION} \\\n            --build-arg TRUSTSTORE_PWD=${TRUSTSTORE_PWD} \\\n            --build-arg POSTGRESQL_CA_PEM=\"${POSTGRESQL_CA_PEM}\"  -t ibmcase/$kname .\n</code></pre> <ul> <li>Python docker env</li> </ul>"},{"location":"techno/docker/#docker-compose","title":"Docker compose","text":"<p>Docker compose helps to orchestrate different docker containers and isolates them with network.  Examples of interesting docker-compose file:</p> <ul> <li>Kafka Strimzi</li> <li>Kafka Confluent</li> <li> <p>Flink</p> </li> <li> <p>An interesting option to start the container is to build the image if it does not exist locally:</p> </li> </ul> <pre><code>docker-compose -f docker-compose-db2.yaml up --build\n# where the declaration includes build statements\ndb2server:\n  image: debezium/db2-cdc:${DEBEZIUM_VERSION}\n  build:\n    context: ./debezium-db2-init/db2server\n  ...\n</code></pre> <p>See this note for the build declaration.</p>"},{"location":"techno/egeria/","title":"Egeria","text":"<p>Egeria is an open source project that provides open standards and implementation libraries to connect tools, catalogs and platforms together so they can share metadata and the technology that supports it.</p> <p>It uses the open metadata and governance standards and ensures compliance to it.</p> <p>The OMAG Server Platform provides a runtime process and platform for Open Metadata and Governance (OMAG) Services. The OMAG services are configured and activated in OMAG Servers using the Administration Services.  The configuration operations of the admin services create configuration documents, one for each OMAG Server.  Inside a configuration document is the definition of which OMAG services to activate in the server.  These include the repository services (any type of server), the access services (for metadata access points and metadata servers),  governance services (for governance servers) and view services (for view servers). </p> <p></p> <p>Once a configuration document is defined, the OMAG Server can be started and stopped multiple times by the admin services server instance operations.</p> <p>The OMAG Server Platform can host multiple OMAG servers at any one time. Each OMAG server is isolated within the server platform and so the OMAG server platform can be used to support multi-tenant operation for a cloud service, or host a variety of different OMAG Servers needed at a particular location.</p>"},{"location":"techno/egeria/#getting-started","title":"Getting started","text":""},{"location":"techno/egeria/#omag-only-with-docker","title":"OMAG only with docker","text":"<p>Start a single OMAG server platform.  <pre><code>docker run --publish 19443:9443 odpi/egeria\n</code></pre></p> <p>Access to https://localhost:19443/swagger-ui/index.htm</p>"},{"location":"techno/egeria/#example-with-docker-compose","title":"Example with docker compose","text":"<p>Once the main repo cloned, under <code>open-metadata-resources/open-metadata-deployment/compose/tutorials</code> starts</p> <pre><code>docker-compose -f ./egeria-tutorial.yaml up -d\n</code></pre> <p>Go to http://localhost:18888 to open the Jupyter interface and load a tutorial notebook.</p> <p>or the simplest topology from this project under <code>studies/egeria</code></p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"techno/egeria/#features","title":"Features","text":"<p>Egeria includes the following features:</p> <ul> <li>libraries that can be embedded into technologies that need to share metadata</li> <li>an integration platform called the Open Metadata and Governance (OMAG) Platform for hosting connectors, metadata servers and governance servers</li> </ul> <p>Servers are part of a cohort which exchanges metadata using peer to peer replication protocol and federated queries.</p> <p>There are three types of cohort member to consider:</p> <ul> <li>A Metadata Server that uses a native Egeria repository to store open metadata. There should be at least one of these servers in a cohort. It used to support either a community of users that are using the Egeria functions directly or to fill in any gaps in the metadata support provided by the third party tools that are connected to the cohort.</li> <li>A Metadata Access Point that has no metadata repository of its own and uses federated queries to retrieve and store metadata in the other repositories connected to the cohort.</li> <li>A Repository Proxy that connects in a thrid party metadata server.</li> </ul> <p></p> <p>Once a server has joined a cohort it can exchange metadata with the other members of that cohort.At the heart of each cohort is an event Kafka topic.</p> <p>Governance server is a service to perform:</p> <ul> <li>discover and analyze content from metadata resources. The result of a discovery service's analysis is stored in a metadata server as a discovery analysis report that is chained off of the asset's definition.</li> <li>governance action services monitor the asset metadata and verify that it is set up correctly, they also determine how to fix anomalies, errors and ommisions, make the necessary changes and provision real-world artifacts and resources beased on the resulting metadata.</li> </ul> <p>Users can add assets for governance and can contribute and comments. They can connect to any sever within the cohort to see the metadata. Egeria has multiple levels of security so that access to individual metadata instances can be controlled</p>"},{"location":"techno/egeria/#install-on-kubernetes","title":"Install on kubernetes","text":"<p>The minimum configuration is:</p> <ul> <li>Kafka </li> <li>A metadata server with a persistent graph store</li> <li>a server for the UI</li> </ul> <p>There is an operator under construction. </p> <p>Otherwise use the Helm install and use the charts from open-metadata-resources/open-metadata-deployment/charts/</p>"},{"location":"techno/egeria/#define-assets","title":"Define Assets","text":"<p>An Asset represents a real resource of value that needs to be governed to ensure it is properly managed and used.</p> <p>Assets are stored in Catalog. Governance zones are groups of related assets, and assets within a zone can be access controlled.</p> <p>Types of assets:</p> <ul> <li>Infrastructure</li> <li>Process</li> <li>DataStore</li> <li>DataFeed</li> <li>Dataset</li> <li>Deployed API</li> </ul> <p>Schema describes the data fileds and operations of the asset. Connection object define information to connect to an asset. It is used to create connector. </p> <p>Asset has ownership who is responsible for the asset.  Governance zones allow assets to be grouped according to their usage. It is possible to assign supported zones to Egeria's open metadata access services (OMAS).</p> <p>Classifiers add labels and properties to the asset, so they can be easily retrieved: it canbe glossary of terms, reference value, informal tag.</p> <p>Assets and classifiers are referenceable, they have a unique qualified name.</p>"},{"location":"techno/egeria/#cloud-information-model","title":"Cloud information model","text":"<p>Cloud Information Model (otherwise known as CIM)  is an open specification to help integrate data across different cloud and on-premise applications. The CIM provides a common language to describe the different types of data. This is extremely valuable to the open metadata and governance ecosystem with its focus on building an open and transparent view of data across an organization.</p> <p>Standard models are released in many different formats, some following open standards and others using a proprietary standard,  often defined by a particular modeling tool. Egeria typically provides a parser to read the specific model format and then a builder to convert the content into an open metadata archive and then a writer to write out the contents to a file for distribution</p>"},{"location":"techno/egeria/#asset-lineage","title":"Asset lineage","text":"<p>Track the origin of the data that is held by the asset. There are different perspectives on what is meant the \"origin\". </p> <p>Lineage helps to decide what asset to use for a particular task. It is also used to build reports for regulators. </p>"},{"location":"techno/egeria/#open-discovery-framework-odf","title":"Open Discovery Framework (ODF)","text":"<p>Open Discovery Framework (ODF) defines open interfaces for components that implement specific types of metadata discovery.  These components can then be called from tools offered by different vendors through the open APIs. Discovery service can access the existing metadata about an asset, and store additional information about the asset that it discovers.</p> <p>Discovery services are specialist governance services. They are grouped together into a useful collection of capability called a governance engine. The same discovery service may be used in multiple governance engines.</p> <p>Egeria provides a governance server called the engine host server that can host one or more governance engines. The engine host server has APIs to call the discovery services in order to drive the analysis a specific asset, and then to view the results. The discovery services can also scan through all assets, running specific analysis on any it finds.</p> <p>Governance engines tend to be paired and deployed close to the data platforms they are analyzing because the discovery services tend to make many calls to access the content of the asset.</p>"},{"location":"techno/egeria/#more-reading","title":"More reading","text":"<ul> <li>Building a Data Catalog</li> </ul>"},{"location":"techno/elasticsrch/","title":"Elastic Search","text":"<p>Open source project to offer RESTful search and analytics engine for all types of data including textual, numerical, geospatial, structured or unstructured.</p>"},{"location":"techno/elasticsrch/#value-proposition","title":"Value proposition","text":"<ul> <li>Add search capability to a web or mobile app</li> <li>Support NoSQL query and build indexes</li> <li>Keep index in shards distributed over a cluster. Expose federated indexing and search capabilities across all servers within the cluster. Cluster can be extended by adding new node.</li> <li>Near real time between when indexing a document and until it becomes searchable.</li> <li>Combined with Kibana to present data, build dashboards and manage ES. Even supports some machine learning execution.</li> <li>Store a vast volume of data</li> </ul>"},{"location":"techno/elasticsrch/#use-cases","title":"Use cases","text":"<ul> <li>Time series data like logs. ELK stack. IT operations</li> <li>Search on unstructured data: website search, URL search, ride data</li> <li>Analytics: maketing insights, customer sentiment, fraud detection, anomaly detection.</li> <li>Data sources for Data scientists</li> </ul>"},{"location":"techno/elasticsrch/#architecture","title":"Architecture","text":"<ul> <li>Start as a scalable Lucene to be an horizontal scalable search engine</li> <li>This is a server to return json payload.</li> <li>Kibana is the Web UI for searching and visualizing with complex analytics, graphs...</li> <li>Can be use to aggregate logs.</li> <li>Logstash and Beats feed data into elasticsearch, but not just Log files</li> <li>X-Pack: to add security, alerting, monitoring, reporting, machine learning and Graph exploration</li> </ul>"},{"location":"techno/elasticsrch/#concepts","title":"Concepts:","text":"<ul> <li>Documents: things we are searching for, text or Json. Every document has a unique ID and a type. They are immutable and has a <code>_version</code> field which is increased at each update. Old documents are marked for deletion.</li> <li>Indices: supports search into all documents within a collection of types. Indices contain inverted indices that let us search across  everything within them at once. Mappings define schema for the data within.</li> <li>TF-IDF: Term Frequency (how often a term appears in a given document) * Inverse Document Frequency (term appears in all document). TF/ DF measures the relevance of a term in a document.</li> <li>Use REST API to search and post document.</li> <li>An index is split into Shards. Documents are hashed to particular shard. </li> <li>An index has two primary shards and two replicas. </li> <li>Write requests are routed to the primary shard then replicated</li> <li>Read requests are routed to the primary or any replica</li> <li>Number of primary shards can not be changed later</li> <li>As a fully distributed solution, two clients can try to update at the same time. So Elasticsearch uses the \"Optimistic Consurrency Control\", by adding <code>_seq_no</code> and <code>_primary_term</code> fields to each document. With those fields a second update will generate an error. </li> </ul>"},{"location":"techno/elasticsrch/#getting-started","title":"Getting started","text":"<p>See docker compose under <code>studies/elasticsearch</code> folder and this note or the rt-inventory gitops repo for a local run.</p> <p>Assess the node: <code>curl -X GET \"localhost:9200/_cat/nodes?v&amp;pretty\"</code>.</p> <p>Access to Kibana: http://localhost:5601/app/home</p> <p>Use Kibana dev tools console to put documents and do query.</p> <p>From first experience with Kibana, use the <code>Sample eCommerce orders</code> from Analytics &gt; Overview &gt; Add data menu. See this kibana tutorial.</p>"},{"location":"techno/elasticsrch/#some-examples-playground","title":"Some examples / playground","text":"<p>The files are in this project: <code>studies/elasticsearch/ml-latest-small</code> folder.</p> <ul> <li>Upload a movie:</li> </ul> <pre><code>curl -X PUT localhost:9200/movies/_doc/109487 -H 'content-type: application/json' -d '\n{\n    \"genre\": [\"IMAX\", \"Sci-Fi\"], \"title\": \"Interstellar\", \"year\": 2024\n}\n'\n</code></pre> <p>The ouput is </p> <pre><code>{\"_index\":\"movies\",\"_id\":\"109487\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":0,\"_primary_term\":1}\n</code></pre> <ul> <li>Upload movies using bulk API</li> </ul> <pre><code>curl -X POST localhost:9200/movies/_bulk -H 'content-type: application/json' -d'@movies.json'\n</code></pre> <ul> <li>Upload a new collection</li> </ul> <pre><code>curl -X PUT localhost:9200/_bulk  -H 'content-type: application/json' --data-binary @series.json\n</code></pre> <p>Bulk API documentation</p> <ul> <li>To specify a date mapping do</li> </ul> <pre><code>curl -X PUT localhost:9200/movies -H 'content-type: application/json' -d '\n{\n    \"mappings\": {\n        \"properties\": {\n            \"year\" : { \"type\": \"date\" }\n        }\n    }\n}\n'\n# Verify\n\ncurl -X GET localhost:9200/movies/_mapping\n</code></pre> <ul> <li> <p>Some other common mappings: </p> <ul> <li>Not analyzing a field for search</li> </ul> <pre><code>\"properties\": {\n        \"genre\" : { \"index\": \"not_analyzed\" }\n    }\n</code></pre> <ul> <li>Field analyzer: to do character filters, tokenizer, and token filter</li> </ul> <pre><code>\"properties\": {\n        \"description\" : { \"analyzer\": \"english\" }\n    }\n</code></pre> </li> <li> <p>Search using Lucene query DSL</p> </li> </ul> <pre><code># with ID\ncurl -X GET \"localhost:9200/movies/_doc/58559?pretty\"\n\n# with query\ncurl -X GET \"localhost:9200/movies/_search?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"query\": {\n    \"match\": {\n      \"title\": \"Star Trek\"  \n    }\n  }\n}\n'\n</code></pre> <p>The results will include records wil Star or Trek or both in the title. It will add a confident <code>_score</code>.</p> <pre><code>  \"max_score\" : 2.4749136,\n  \"hits\" : [\n      {\n        \"_index\" : \"movies\",\n        \"_id\" : \"135569\",\n        \"_score\" : 2.4749136,\n        \"_source\" : {\n          \"id\" : \"135569\",\n          \"title\" : \"Star Trek Beyond\",\n          \"year\" : 2016,\n          \"genre\" : [\n            \"Action\",\n            \"Adventure\",\n            \"Sci-Fi\"\n          ]\n        }\n      },\n      {\n        \"_index\" : \"movies\",\n        \"_id\" : \"122886\",\n        \"_score\" : 0.6511494,\n        \"_source\" : {\n          \"id\" : \"122886\",\n          \"title\" : \"Star Wars: Episode VII - The Force Awakens\",\n          \"year\" : 2015,\n          \"genre\" : [\n            \"Action\",\n            \"Adventure\",\n            \"Fantasy\",\n            \"Sci-Fi\",\n            \"IMAX\"\n          ]\n        }\n      }\n</code></pre> <ul> <li>Update: can be done with a PUT and the full document as payload or use a partial update with a POST.</li> </ul> <pre><code>curl -X PUT  -H 'content-type: application/json'  localhost:9200/movies/_doc/109487  -d '\\n{\\n    \"genre\": [\"IMAX\", \"Sci-Fi\"], \"title\": \"Interstellar\", \"year\": 2024\\n}\\n'\n# OR Partial update\ncurl -X POST  -H 'content-type: application/json'  localhost:9200/movies/_doc/109487/_update -d '{\"title\": \"Interstellar 2\"}'\n# Partial update with retry conflict\ncurl -X POST  -H 'content-type: application/json'  localhost:9200/movies/_doc/109487/_update?retry_on_conflict=5 -d '{\"doc\": {\"title\": \"Interstellar 2\"}}'\n</code></pre>"},{"location":"techno/elasticsrch/#searching","title":"Searching","text":"<p>To search on exact word, for example on some enumerated value, use a mapping for the enumerated field to be of type <code>keyword</code>.</p>"},{"location":"techno/elasticsrch/#data-modeling","title":"Data Modeling","text":"<p>Recall that, normalized data is used to reduce storage space, and it makes easy to change one field in one of the aggregated records. But it requires two queries to get the full expected data: the movie title and the rating.</p> <p>For parent-child relationship we can create series by joining two collections. This is done by defining a mappings of type <code>join</code>.</p> <p><pre><code>curl -X PUT localhost:9200/series -H 'content-type: application/json' -d '\n{\n    \"mappings\": {\n        \"properties\": {\n            \"film_to_franchise\" : {\n                \"type\": \"join\",\n                \"relations\": {\"franchise\": \"film\" }\n            }\n        }\n    }\n}'\n# results\n{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"series\"}\n</code></pre> * Get default mappings for an index</p> <pre><code>curl -XGET \"http://127.0.0.1:9200/demo-default/_mapping?pretty=true\"\n</code></pre> <ul> <li>Upload data: (see folde/studies/elasticsearch/ml-latest-small)</li> </ul> <pre><code>curl -X PUT localhost:9200/_bulk  -H 'content-type: application/json' --data-binary @series.json\n</code></pre> <ul> <li>Search using the parent - child relationship <pre><code>{ \n    \"query\": { \n        \"has_parent\": { \n            \"parent_type\": \"franchise\", \n            \"query\": {\n                \"match\": { \n                    \"title\": \"Star Wars\"}\n            }\n        }\n    }\n}\n</code></pre></li> </ul> <p>Should return the films from Start Wars franchise.</p> <p>or</p> <pre><code>{ \n    \"query\": { \n        \"has_child\": { \n            \"type\": \"film\", \n            \"query\": {\n                \"match\": { \n                    \"title\": \"The Force Awakens\"}\n            }\n        }\n    }\n}\n</code></pre> <p>Should return \"Star Wars\" franchise.</p>"},{"location":"techno/elasticsrch/#flattened-datatype","title":"Flattened Datatype","text":"<p>To control the explosion of mappings, when the data model is hierarchy, ElasticSearch uses the concepts of flattened datatype. The host content is flattened:</p> <pre><code>curl -XPUT \"http://127.0.0.1:9200/demo-flattened/_mapping\" -d'{\n  \"properties\": {\n    \"host\": {\n      \"type\": \"flattened\"\n    }\n  }\n}'\n</code></pre> <p>No tokenizer or analyzer will be used in flattened. </p>"},{"location":"techno/elasticsrch/#deploying-on-openshift","title":"Deploying on OpenShift","text":"<p>Use OpenShift Operator Hub and elasticsearch community operator.</p> <p>See the ECK quickstart.</p>"},{"location":"techno/elasticsrch/#connect-kafka-sink-connector-to-elasticsearch","title":"Connect Kafka Sink connector to Elasticsearch","text":"<ul> <li>Here a json to send to distributed kafka connector:</li> </ul> <pre><code>{\n    \"name\": \"elastic-sink\",\n    \"config\":\n    {\n        \"connector.class\": \"com.ibm.eventstreams.connect.elasticsink.ElasticSinkConnector\",\n        \"tasks.max\": \"1\",\n        \"topics\": \"store.inventory,item.inventory\",\n        \"es.connection\": \"elastic:9200\",\n        \"es.document.builder\": \"com.ibm.eventstreams.connect.elasticsink.builders.JsonDocumentBuilder\",\n        \"es.index.builder\": \"com.ibm.eventstreams.connect.elasticsink.builders.DefaultIndexBuilder\",\n        \"es.identifier.builder\": \"com.ibm.eventstreams.connect.elasticsink.builders.DefaultIdentifierBuilder\",\n        \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n        \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n        \"value.converter.schemas.enable\": \"false\"\n    }\n}\n</code></pre> <ul> <li>Command is in the sendESSinkConfig script here</li> </ul> <pre><code>curl -X POST  -w \"%{http_code}\" -H 'content-type: application/json' -d@\"../kconnect/elastic-sink.json\" http://$1/connectors\n</code></pre>"},{"location":"techno/elasticsrch/#tutorial","title":"Tutorial","text":"<ul> <li>Sundog's material from the Udimy training</li> <li>Quarkus elasticsearch guide for simple CRUD of java bean to json doc into ES.</li> </ul>"},{"location":"techno/elasticsrch/#kibana","title":"Kibana","text":"<p>Product documentation</p> <p>Major constructs we can manage in Kibana:</p> <ul> <li>Analytics</li> <li>Ingestion</li> <li>Indices</li> <li>Data Views</li> </ul>"},{"location":"techno/kar/","title":"KAR","text":"<p>KAR is an IBM research initiative to implement the actor model for cloud native stateless or stateful microservices.</p> <p>KAR is deployed as a side car to the microservice to expose a REST api so the service can make synch or asynch calls to other microservices, produce ot consume events or manage its persistence state.</p> <p></p> <p>Together the KAR processes form a mesh, which uses kafka for decoupling and reliable req/rep interactions. mesh has no leader, no single point of failure, and no external dependency other than a Kafka and Redis instances</p>"},{"location":"techno/kar/#some-concepts","title":"Some concepts","text":"<ul> <li>KAR makes it easy to structure the state of microservices as a collection of actor instances</li> <li>Each actor instance is responsible for its own state. </li> <li>The state of an actor instance can be saved or restored safely (into Redis)</li> <li>Actors are relocatable</li> <li>A KAR application is composed of one or more application components. Individual components may be deployed as simple OS processes running directly on a development machine or as containers running inside one or more Kubernetes or OpenShift clusters. </li> <li>A KAR application consists of a dynamic set of components, event sources and sinks, and a persistent store.</li> <li>An application component is a unit of compute and state.  It is joined to a specific application at launch time </li> <li>To address scalability, KAR handles dynamic scaling of replicated stateless and stateful microservices. It leverages K8S scheduler for stateless applications and itw own mechanism for stateful</li> <li>Stateful components intended to be scalable or fault-tolerant should either manage their state on their own or leverage KAR's actors and persistent store</li> <li>Server component handle HTTP requests issued by the KAR runtime and emit HTTP request to the runtime too.</li> <li>Leverage Apache Camel for integrating with external source and sink.</li> <li>An application component may offer a single service identified by its name,</li> </ul>"},{"location":"techno/kar/#ddd-and-actors","title":"DDD and Actors","text":"<p>The actor model fits well on supporting reactive system implementation as they are message driven, resilient, and elastic.  The architecture is simplified to two layers: the communication (web controller) and the domain model. Actors can be seen as Object (of OOD) done right.</p> <p>Bounded contexts are also reactives, reacting to asynch web request or messages and producing events and messages. Actors are within the bounded context.  Persistence is separated from command to query.</p> <p>Stateful logic in the domain layer maps nicely into Actors. Actors run in different processing and can be remote to the existing controller.  Router actors can handle the logistics of sending messages to other actors that may be distributed across  the cluster. A router actor receives messages, but it does not handle the message itself.  It forwards the message to a worker actor.</p> <p>Schema registry is used to define structure for inter bounded context communication. </p>"},{"location":"techno/kar/#akka","title":"Akka","text":"<p>Akka is the well established, scala based, actor model framework. It helps to develop distributed application.  KAR is a polyglot actor model, and application components communicate over Kafka and persist state in Redis.  KAR runtime process runs alongside each application component. It offers APIs to facilitate the implementation of actors</p>"},{"location":"techno/kar/#vertx","title":"Vert.x","text":"<p>Polyglot event driven, non-blocking programming library. See study here. The event bus is an internal protocol, but could use Kafka too.</p>"},{"location":"techno/kar/#getting-started","title":"Getting started","text":"<p>See Getting started, source code is under ~/Tools/kar-1.xx.</p> <p>Kar uses Redis and Kafka so docker-compose will start both.</p> <pre><code>RESTART_POLICY=always ./scripts/docker-compose-start.sh\n# Set environment variable for Redis and Kafka\nsource ./scripts/kar-env-local.sh\n</code></pre> <p>Then code, once packaged, is executed via the <code>kar</code> cli. </p>"},{"location":"techno/kar/#kcontainer-revisited","title":"KContainer revisited","text":"<p>See this repository.</p>"},{"location":"techno/microcks/","title":"Microcks.io","text":"<p>The Open source Kubernetes Native tool for API Mocking and Testing. </p>"},{"location":"techno/microcks/#value-propositions","title":"Value propositions","text":"<ul> <li>One tool for all your APIs, events and WS</li> <li>Experiment with new APIs using the Backend as a Service feature, play and iterate before creating your API contract</li> <li>Mock and test continuously: Integrate seemlessly in your continuous build or pipelines</li> <li>Microcks can be deployed on any cloud provider on in-house infrastructure using Kube.</li> </ul>"},{"location":"techno/microcks/#installation","title":"Installation","text":""},{"location":"techno/microcks/#local","title":"Local","text":"<p>See getting started doc here:</p> <pre><code># ~/Code/Studies/microcks/install/docker-compose\ndocker compose up -d\n</code></pre>"},{"location":"techno/microcks/#kube","title":"Kube","text":"<ul> <li>install operator</li> </ul> <pre><code>kubectl apply -f https://microcks.io/operator/operator-latest.yaml\n</code></pre> <ul> <li>Define a deployment</li> </ul> <pre><code>apiVersion: microcks.github.io/v1alpha1\nkind: MicrocksInstall\nmetadata:\n  name: eda-microcks\n  namespace: edademo-dev\nspec:\n  name: eda-microcks\n  version: 1.4.1\n  microcks:\n    replicas: 1\n  postman:\n    replicas: 1\n  keycloak:\n    install: true\n    persistent: true\n    volumeSize: 1Gi\n  mongodb:\n    install: true\n    persistent: true\n    volumeSize: 2Gi\n</code></pre> <ul> <li>install operand</li> </ul> <pre><code># from project \noc apply -k environments/edademo-dev/apps/services/microcks\n</code></pre> <p>Get pods:</p> <pre><code>oc get pods | grep microcks                                \neda-microcks-76f78d67dd-zxh6x                       1/1     Running   0          6m17s\neda-microcks-keycloak-7978fc6675-m2v56              1/1     Running   0          7m38s\neda-microcks-keycloak-postgresql-644d9554c4-hvgwj   1/1     Running   0          9m23s\neda-microcks-mongodb-b787c7ddd-p97vn                1/1     Running   0          10m\neda-microcks-postman-runtime-788f4f8f4c-kdkbz       1/1     Running   0          5m35s\nmicrocks-ansible-operator-56cc794cd5-9vqr5          1/1     Running   0          31m\n</code></pre> <ul> <li>Add user to microcks</li> </ul> <p>Access KeyCloack secrets <code>eda-microcks-keycloak-admin</code> for username and password to  access KeyCloack Administration UI</p> <p>rNOvqKAqcVLVyQAGCKhihZkISxpTfdXe  adminuOLFM Then from the exposed route, go to the UI and login to the admin console, then Manage &gt; Users &gt; Add user Add user johndoe and as an admin role: Role Mapping</p>"},{"location":"techno/minikube/","title":"Minikube how to","text":"Update <p>Created 2023 - Update 9/2024 consolidate notes</p> <p>Under construction</p> <p>Minikube is officially backed by the Kubernetes project. It supports different backend drivers like KVM, Docker, Podman.</p>"},{"location":"techno/minikube/#getting-started","title":"Getting started","text":"<p>Official getting started</p>"},{"location":"techno/minikube/#minikube-on-local-home-network","title":"Minikube on local home network","text":"<p>We have ultiple choices: </p> <ol> <li>A remote dedicated Ubuntu workstation, install minikube, podman and then remote ssh to the Ubuntu machine.</li> <li>Use WSL2 on Windows or directly install on MacOS</li> </ol> <p>Consult minikube FAQ</p>"},{"location":"techno/minikube/#install-on-ubuntu","title":"Install on Ubuntu","text":"<ul> <li>Install docker podman</li> </ul> <pre><code># Ubuntu\nsudo apt install podman\n# Fedora\nsudo dnf install podman\n</code></pre> <ul> <li>Verify system resources</li> </ul> <pre><code>lscpu\n</code></pre> <ul> <li>modify /etc/sudoers by adding <code>jerome ALL=(ALL) NOPASSWD: /usr/bin/podman</code></li> </ul> <pre><code>sudo vi /etc/sudoers\n</code></pre> <ul> <li>verify user can see the podman version</li> </ul> <pre><code>sudo -n -k podman version\n</code></pre> <ul> <li>Installation minikube</li> </ul> <pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube &amp;&amp; rm minikube-linux-amd64\n</code></pre>"},{"location":"techno/minikube/#remote-access-to-ubuntu-computer-on-local-lan","title":"Remote access to Ubuntu computer on local LAN","text":"<ul> <li>Start ssh server within the ubuntu host:</li> </ul> <pre><code>sudo apt install openssh-server\nip a\n</code></pre>"},{"location":"techno/minikube/#remote-access-to-fedora-computer-on-local-lan","title":"Remote access to Fedora computer on local LAN","text":"<ul> <li>Start ssh server:</li> </ul> <pre><code>sudo dnf install openssh-server\nsudo systemctl start sshd\nsudo systemctl enable sshd\nip a\n</code></pre> <ul> <li>Verify potential firewall setting</li> </ul> <pre><code>sudo firewall-cmd --list-all\n# To allow SSH through the firewall:\nsudo firewall-cmd --permanent --add-service=ssh\nsudo firewall-cmd --reload\n</code></pre> <ul> <li>To avoid Fedora laptop sleep while on power supply</li> </ul> <pre><code>sudo -u gdm dbus-run-session gsettings set org.gnome.settings-daemon.plugins.power sleep-inactive-ac-timeout 0\n</code></pre> <ul> <li>On remote host to Fedora do</li> </ul> <pre><code>ssh jeromeboyer@10.0.0.192\n</code></pre>"},{"location":"techno/minikube/#install-on-mac","title":"Install on Mac","text":"<pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64\nsudo install minikube-darwin-amd64 /usr/local/bin/minikube\nrm minikube-darwin-amd64\n</code></pre>"},{"location":"techno/minikube/#update-existing-minikube","title":"Update existing Minikube","text":"<pre><code>minikube update-check\n</code></pre>"},{"location":"techno/minikube/#run-a-cluster","title":"Run a cluster","text":""},{"location":"techno/minikube/#with-docker-driver","title":"With docker driver","text":"<p>In WSL2 on Windows and Docker Desktop installed on Windows it is possible to share the docker driver with WSL2.</p> <pre><code>minikube start\n# ip address\nminikube ip\n# 192.168.49.2\n# Start with enough resources:\nminikube start --cpus 3 --memory 3072\n# Verify the state\nminikube status\n</code></pre> <p>To point the docker CLI to minikube docker environment: </p> <pre><code>eval $(minikube -p &lt;profile&gt; docker-env)\n</code></pre>"},{"location":"techno/minikube/#with-podman-driver","title":"With podman driver","text":"<p>To be able to run minikube with a <code>podman</code> driver, the user needs to be a sudoers see this note:</p> <pre><code>minikube start --driver=podman\n</code></pre> <p>Personal script is <code>~/bin/ministart</code>, may take some time as it may download new VM image.</p> <ul> <li>In case of problem delete the vm with <code>minikube delete</code></li> </ul>"},{"location":"techno/minikube/#add-any-needed-addons","title":"Add any needed addons","text":"<pre><code>minikube addons list\nminikube addons enable metrics-server\nminikube addons enable ingress\n</code></pre>"},{"location":"techno/minikube/#kubectl","title":"Kubectl","text":"<ul> <li>If kubectl is not install on the host, we can alias it to the minikube:</li> </ul> <pre><code>alias k=\"minikube kubectl\"\nalias kubectl=\"minikube kubectl\"\n</code></pre> <ul> <li>Retrieve all Kubernetes context (they are saved in <code>~/bin/.kube/config</code>)</li> </ul> <pre><code>kubectl config get-contexts\n</code></pre> <ul> <li>Change kubectl context between openshift and minikube:</li> </ul> <pre><code>kubectl config use-context minikube\nkubectl config use-context default/c1....com:31580/IAM#&lt;email&gt;\n# same with\nkubectx minikube\n</code></pre> <ul> <li>Retrieve the  nodes: </li> </ul> <pre><code>kubectl get nodes\n</code></pre> <ul> <li>List existing addons</li> </ul> <p>Addons are built-in list of applications and services</p> <pre><code>minikube addons list\n\n# enabling\nminikube addons enable &lt;name&gt;\n</code></pre>"},{"location":"techno/minikube/#user-interface-and-networking","title":"User interface and networking","text":"<ul> <li>Dashboard UI</li> </ul> <pre><code>minikube dashboard\n</code></pre> <p>Then click to the URL constructed using the proxy, to access from the minikube host machine. To access remotly from another computer using a static port, we need a proxy to access it from a static port.</p> <ul> <li>Start a kubernetes proxy so the Kubernetes APIs are served through port 8001</li> </ul> <pre><code>kubectl proxy \n</code></pre> <ul> <li>So the dashboard is accessible remotely, using  SSH to the server, using -L option. (ubuntu1 was added to local <code>/etc/hosts</code>)</li> </ul> <pre><code>ssh -L 12345:localhost:8001 jerome@ubuntu1\n</code></pre> <p>Now the Kubernetes Dashboard is accessible remotly at http://localhost:12345/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy</p>"},{"location":"techno/minikube/#use-docker-cli-to-build-image","title":"Use docker CLI to build image","text":"<ul> <li>Install docker CLI</li> </ul> <pre><code>brew install docker\n</code></pre> <ul> <li>Expose the Docker daemon from minikube to the local terminal environment.</li> </ul> <pre><code>eval $(minikube docker-env)\n</code></pre> <ul> <li>Enable docker local daemon to push images to minikube registry</li> </ul> <pre><code>minikube addons enable registry\n</code></pre> <ul> <li>Build a quarkus app and deploy it to minikube</li> </ul> <pre><code>mvn verify -Dquarkus.kubernetes.deploy=true\n</code></pre> <ul> <li>Get the service and app url:</li> </ul> <pre><code>kubectl get svc\nminikube service quarkus-reactive-kafka-producer --url\n</code></pre> <ul> <li>Deploy an existing app</li> </ul> <pre><code>kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4\nkubectl expose deployment hello-minikube --type=NodePort --port=8080\n</code></pre> <ul> <li>Deploy nginx from studies/minikube</li> </ul> <pre><code>k create -f nginx-svc.yaml\n\nk create -f nginx-deploy.yaml\n# tunnel between Ubuntu and minikube \nminikube service nginx-service\n# Alternatively, use kubectl to forward the port:\n\nkubectl port-forward service/nginx-service 8083:80\n</code></pre> <ul> <li>Build docker image</li> </ul> <pre><code>\n</code></pre>"},{"location":"techno/minikube/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Clean all at the docker engine level</li> </ul> <pre><code>docker system prune -a --volume -f\n</code></pre> <ul> <li>Error starting minikube: Error validating CNI config file /etc/cni/net.d/minikube.conflist</li> </ul> <pre><code>Removing the failed install of minikube cant hurt: `minikube delete --all`\nCheck your package version of containernetworking-plugins:`apt show containernetworking-plugins`\n\nGo to http://archive.ubuntu.com/ubuntu/pool/universe/g/golang-github-containernetworking-plugins/ and download an up to date version\n\nInstall: `sudo dpkg -i containernetworking-plugins_1.1.1+ds1-3_amd64.deb`\n\n`minikube start`\n</code></pre> <ul> <li></li> </ul>"},{"location":"techno/podman/","title":"Podman as docker cli and daemon","text":"<p>Getting started from podman.io</p> <p>The official podman installation instructions from the containers organisation are  to be found https://podman.io/getting-started/installation</p> <pre><code>brew install podman\n</code></pre> <p>To understand how podman works on mac: podman-mac-machine-architecture</p> <ul> <li>It runs a virtualized Linux (Fedora CoreOS) distribution using native macOS virtualization</li> <li>The Podman client securely communicates with the Linux VM using secure shell (SSH) keys</li> </ul> <p>How to replace Docker with Podman on a Mac</p>"},{"location":"techno/podman/#summary-of-cli","title":"Summary of CLI","text":"<ul> <li>to create a Linux VM for your containers</li> </ul> <pre><code>podman machine init\n</code></pre> <p>The machine description is a text file that describes the attributes of the VM that it will create. </p> <ul> <li>Start the VM</li> </ul> <pre><code>podman machine start\n</code></pre> <p>The ignition file is injected into the VM during this first boot and then run in the boot process.</p> <p>The gvproxy application manages port mapping between the host and VM.  The VM routes its traffic through the host system.  Once the image is pulled successfully, the container runs on the VM.</p>"},{"location":"techno/podman/#major-issues","title":"Major issues","text":"<p>podman push is not responding</p>"},{"location":"techno/podman/#tricks","title":"Tricks","text":"<p>Official troubleshooting</p>"},{"location":"techno/prometheus/","title":"Prometheus","text":"<p>Prometheus is an open source project, originally built at SoundCloud, used to monitor apps. It is part of CNSF since 2016.</p> <p>Prometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.</p> <p>Time series collection happens via a pull model over HTTP.</p> <p>The figure from prometheus site shows the components of a prometheus deployment:</p> <p></p> <p>Prometheus scrapes metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. It stores all scraped samples locally and runs rules over this data to either aggregate and record new time series from existing data or generate alerts.</p> <p>Prometheus configuration is YAML. See example of monitoring the real-time inventory here.</p>"},{"location":"techno/prometheus/#openshift-context","title":"OpenShift context","text":"<ul> <li>With Red Hat OpenShift Container Platform, you can monitor the OpenShift platform.</li> <li>The monitoring stack for the platform is based on Prometheus and Grafana.</li> <li>During Day 1 operation you may want to keep time series data in a persistence volume, and configure the alert manager to forward to the good destination</li> </ul> Component Prometheus Used to provide a time-series data store for metrics, rule evaluation engine and alert generation. AlertManager Responsible for alerts handling and notification to external systems. Thanos Responsible for metric aggregation across Prometheus instances as well as alert generation engine. Grafana Used to provide dashboard and metric visualization capabilities.  This is a read only instance of Grafana to show platform metrics <ul> <li> <p>Day 02 operations include verifying platform metrics, performance data, and alerts are reaching the correct destination</p> </li> <li> <p>Knowledge base: IBM Cloud architecture red-hat-openshift-container-platform-day-2-ops. Some guiding principles for the definition and design of Site Reliability Engineer activities:</p> <ul> <li>Immutable / Disposable Infrastructure</li> <li>Infrastructure as Code</li> <li>Automate as much as possible</li> </ul> </li> <li> <p>During the application design, spend time to define the saturation monitor goals. On Day 2, verify application metrics, monitor app to verify it acts as expected, liveness, health check.</p> </li> </ul>"},{"location":"techno/rancherdesktop/","title":"Ranger desktop","text":"<ul> <li>Rancher Desktop is running Linux in order to run the containers.</li> <li>It includes dockerd socket as an alternative to nerdctl and containerd.</li> <li>Docker engine is built on the Moby project</li> <li>Rancher Desktop provides Kubernetes, using the k3s distribution, at the version of your choice</li> </ul>"},{"location":"techno/rancherdesktop/#installation","title":"Installation","text":"<p>Product documentation</p>"},{"location":"techno/redis/","title":"Redis","text":"<p>Redis is in-memory data structure store <code>&lt;key-value&gt;</code>, used as a database, cache, and message broker. </p> <p>Redis provides:</p> <ul> <li>Support to different data structures such as Strings, hashes, lists, sets, sorted sets, range queries, bitmaps, hyperloglogs, geospatial indexes, and streams. </li> <li>built-in replication.</li> <li>transactions support.</li> <li>different levels of on-disk persistence.</li> <li>high availability via Redis Sentinel and automatic partitioning with Redis Cluster.</li> </ul>"},{"location":"techno/redis/#getting-started-concepts","title":"Getting started / concepts","text":"<p>Can be used using docker:</p> <pre><code>docker run --ulimit memlock=-1:-1 -it --rm=true --memory-swappiness=0 --name redis_quarkus_test -p 6379:6379 redis:6.2\n# Then in the shell use redis-cli\nredis-cli \n....:6379&gt; &lt;enter a command&gt;\n....:6379&gt; SET counter 100\n....:6379&gt; GET counter\n....:6379&gt; INCR counter\n</code></pre> <p>See the list of commands</p> <p>Redis can be told that a key should only exist for a certain length of time.</p> <pre><code>SET alert valueofthealert\n# time in ms\nexpire alert 50 \n</code></pre>"},{"location":"techno/redis/#run-it-in-cluster","title":"Run it in cluster","text":"<p>See the docker compose under <code>studies/redis</code> folder. See some details on the Bitnami Redis image.</p> <p>Then connect to an existing node:</p> <pre><code>docker exec -ti redis_redis-node-1 bash\n</code></pre> <p>Or start a new docker on the same network</p> <pre><code>docker run -ti --network redis_default docker.io/bitnami/redis-cluster:6.2 bash\n# remote connect to one of the node in the cluster\nredis-cli -h redis-node-0 -a bitnami \n</code></pre>"},{"location":"techno/redis/#redis-for-distributed-database","title":"Redis for distributed database","text":"<p>At any given time, a Redis Enterprise cluster node can include between zero and a few hundred Redis databases</p> <p></p> <p>See some details on Redislab documentation</p>"},{"location":"techno/redis/#redis-client","title":"Redis client","text":"<ul> <li>Jedis redis client for Java.</li> <li>Redisson is the most advanced and easiest Redis Java client.</li> <li>Spring data, looks very similar to JMS Spring template.</li> <li>Quarkus</li> <li>Flink Redis sink connector</li> <li>Redis python client</li> </ul>"},{"location":"techno/redis/#using-redis-in-quarkus","title":"Using redis in quarkus","text":"<ul> <li>Add redis client dependency, which comes with a reactive driver and classical one:</li> </ul> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-redis-client&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <ul> <li>Add connection properties: <code>quarkus.redis.hosts=redis://localhost:6379</code></li> <li>Add code with redis client</li> </ul> <pre><code>@Inject\nRedisClient redisClient;\n//...\nvoid set(String key, Integer value) {\n    redisClient.set(Arrays.asList(key, value.toString()));\n}\n\nvoid increment(String key, Integer incrementBy) {\n    redisClient.incrby(key, incrementBy.toString());\n}\n\nString get(String key) {\n    return redisClient.get(key).toString();\n}\n</code></pre> <p>The INCR or incrby operation are safe atomic operations </p> <p>With Reactive driver:</p> <pre><code>@Inject\nReactiveRedisClient reactiveRedisClient;\n\n// retrieve all the values for a given key\nUni&lt;List&lt;String&gt;&gt; keys() {\n    return reactiveRedisClient\n            .keys(\"*\")\n            .map(response -&gt; {\n                List&lt;String&gt; result = new ArrayList&lt;&gt;();\n                for (Response r : response) {\n                    result.add(r.toString());\n                }\n                return result;\n            });\n}\n</code></pre> <p>If you are using the <code>quarkus-smallrye-health</code> extension, <code>quarkus-vertx-redis</code> will automatically add a readiness  health check to validate the connection to the Redis server.</p>"},{"location":"techno/redis/#redis-as-streaming","title":"Redis as streaming","text":"<p>Redis Streams are primarily an append only data structure. It includes features a set of blocking operations  allowing consumers to wait for new data added to a stream by producers. It used the <code>Consumer Groups</code> concept of Kafka, to allow a group of clients to cooperate consuming a different portion of the same stream of messages.</p> <p>Can do streaming processing on top of this append log. </p> <p>Example to add an <code>event</code> to a log named <code>telemetries</code> with auto generated ID.</p> <pre><code>XADD telemetries * sensor-id 1234 temperature 19.8 co2 14.1\n\"1621553018591-0\"\n</code></pre> <p>The ID is <code>&lt;millisecondsTime&gt;-&lt;sequenceNumber&gt;</code> coded on 64 bits. Sequence Number is for the timestamp at the ms level. Redis streams support range queries by ID. Because the ID is related to the time the entry is generated, this gives the ability to query for time ranges basically for free.</p> <p>The streams is able to fan out messages to multiple clients. The operation to read from a stream is XREAD:</p> <pre><code># Read from a specific Id or the start of the log\nXREAD STREAMS telemetries 0\n# Read from current last records to get only new records, with 0 timeout so infinite\nXREAD BLOCK 0 STREAMS telemetries $\n</code></pre> <p>It can be seen as a time series store, and then clients can get records by ranged of time, </p> <pre><code># query from start to end\nXRANGE telemetries - +\nXRANGE telemetries 1621553351764 +\n# only the 2 first records\nXRANGE telemetries - + COUNT 2\n# return in reverse order\nXREVRANGE telemetries + - \n</code></pre> <p>or alternatively to iterate the messages using a cursor to incrementally check all the history</p>"},{"location":"techno/redis/#consumer-groups","title":"Consumer groups","text":"<p>With XREAD we can consume the same stream from different clients and even to replica to provide more read scalability. But we can also use consumer group to provide a different subset of messages from the same stream to many clients.</p> <p>Some characteristics:</p> <ul> <li>The same message will not be delviered to multiple consumers</li> <li>Consumers are uniquely identified, within a consumer group. </li> <li>Each consumer group has the concept of the first ID never consumed.</li> <li>Consumer need to acknowledge the read so the message can be evicted from the consumer group</li> <li>Consumer group keep track of pending messages (not yet acknowledged)</li> <li>A single stream can have multiple consumer groups</li> <li>Consumer groups are a server-side load balancing system of messages from a given stream to N different consumers</li> </ul> <pre><code># create a consumer group\nXGROUP CREATE telemetries mygroup $\n# instead of using last message ID $,  If we specify 0 instead the consumer group \n# will consume all the messages in the stream history to start with\n\n# Add consumer reading one message\nXREADGROUP GROUP mygroup clientID_1 COUNT 1 STREAMS telemetries\n#  Or listening indefinitively\nXREADGROUP GROUP mygroup clientID_1 BLOCK 0 STREAMS telemetries\n# acknowledge the message as processed, it will no longer be part of the pending messages history\nXACK telemetries mygroup 1526569495631-0\n</code></pre> <p>Remarks: all those commands are accessible via the Redis client.</p> <ul> <li>Consumers are auto-created the first time they are mentioned</li> <li>XREADGROUP can read from multiple keys at the same time</li> <li>XREADGROUP is a write command as the consumer group is modified as a side effect of reading.  It can only be called on active node instances.</li> <li>When restarted a consumer may see duplicate messages as the acknowledge was not perform while crashing</li> <li>In case of consumer that will never restart, it is possible to claim the pending messages of this given consumer so that such messages will change ownership. Consumer has to inspect the list of pending  messages, and will have to claim specific messages using a special command, otherwise the server will  leave the messages pending forever. (<code>XPENDING</code> combined with <code>XCLAIM</code>)</li> </ul> <p>The processing looks like the Kafka Topic partitions, but here the partitions are logical and the messages are put in the same Redis Key. The way the different clients are served is based on who  is ready to process new messages and not from which partition clients are reading.</p> <p>If we really want to partition messages in the same stream into multiple Redis instances,  we have to use multiple keys and some sharding system such as Redis Cluster. </p>"},{"location":"techno/redis/#kafka-or-redis-streams","title":"Kafka or Redis Streams","text":"<p>Both Kafka and Redis streams provide all the features we would expect from a streaming  solution and can scale as much as is needed.</p> <p>Kafka will be relevant when we have the following requirements</p> <ul> <li>Huge amount of data</li> <li>Exactly once deliver</li> <li>no data loss, Fault tolerant to number of nodes -1</li> <li>persist messages for a long time</li> <li>doing consume-process-produce loop with Kafka Streams constructs</li> <li>connect to a lot of soures and sink via the Kafka connector framework</li> <li>strongly parallelism execution for consumer</li> <li>Kafka cluster to maintain is complex</li> <li>Cost more money and infrastructure power</li> <li>Schema registry, binary serialization, schema for message</li> <li>Secure connection, ACL on topic, multi tenancy</li> </ul> <p>Redis will fit well for</p> <ul> <li>Speed is very important, with in-memory database</li> <li>Accept data loss</li> <li>Redis provides high availability through a component known as Sentinel, which is using active/passive mechanism</li> <li>The cluster configuration of Redis provides fault tolerance by sharding the data and having a similar internal active-passive node  configuration. All nodes of the clusters are constantly pinging each other to find issues in the mix and once one is detected,   it will reconfigure itself to promote an appropriate passive node.</li> <li>Any type of data structure</li> <li>support Pub/sub with consumer subscription and a push model</li> <li>support Redis streams it acts as Kafka 9offset, consumer group...), can be consumed either in blocking or nonblocking ways,  and support consumer parallelism. </li> <li>more polyvalent</li> <li>Value is structure data, like a Hash</li> </ul>"},{"location":"techno/redis/#redis-on-aws","title":"Redis on AWS","text":"<ul> <li>Use Amazon ElastiCache for Redis cluster</li> </ul>"},{"location":"techno/solr/","title":"Apache Solr","text":"<p>Solr is a search server running in cluster and built on top of Apache Lucene.</p> <p>It is used in document retrieval or analytical applications involving unstructured data, semi-structured data or a mix of unstructured and structured data.</p> <p>Supports simplest keyword searching through to complex queries on multiple fields and faceted search results</p> <p>It accesses to almost all of Lucene\u2019s text analysis features including tokenization, stemming, synonyms</p>"},{"location":"techno/solr/#getting-started","title":"Getting started","text":"<p>See docker compose file in solr to start one node and one zookeeper. The zookeeper is needed to let solr runs in cloud mode, which exposes the <code>api/solr</code> endpoints.</p> <p>The typical steps are:</p> <ol> <li>Create a collection</li> <li>Define schema for the collection</li> <li>Upload documents for indexing, and commit the changes</li> <li>Start querying the collection</li> </ol>"},{"location":"techno/solr/#concepts","title":"Concepts","text":"<ul> <li>The basic unit of information is a document, which is a set of data that describes something</li> <li>Documents are composed of fields, which hasa field type used to do better query.</li> <li>Schema stores the details about the fields and field types Solr is expected to understand. It can include normalization instruction.</li> <li>Faceting is the arrangement of search results into categories </li> </ul>"},{"location":"techno/solr/#assessment","title":"Assessment","text":"<ul> <li>What sorts of data do you need to index?</li> <li>What kind of query / search to perform?</li> <li>What are the criteria to assess search result quality?</li> </ul>"},{"location":"techno/wsl2/","title":"Windows Server Linux 2","text":""},{"location":"techno/wsl2/#enable-it","title":"Enable it","text":"<ul> <li>Run Powershell as administrator </li> <li>Product doc</li> <li><code>wsl --install</code></li> </ul> <p>Best practices for WSL</p> <ul> <li>Check WSL2 running: <code>wsl -l -v</code></li> </ul> <p>Dockerhttps://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers is also a WSL2 VM. Ensure that \"Use the WSL 2 based engine\" is checked in Settings &gt; General.</p>"},{"location":"techno/wsl2/#ubuntu","title":"Ubuntu","text":"<ul> <li><code>lsb_release -a</code> to see the ubunty release</li> <li>To authorise remote desktop to the ubuntu machine install <code>xrdp</code>, then use Windows Remote Desktop to connect to the host on port 3389. To get the ip address with <code>ip addr</code>   -&gt; 192.168.85.149</li> </ul>"},{"location":"techno/wsl2/#networking","title":"Networking","text":""},{"location":"techno/wsl2/#access-from-remote-host-to-ubuntu-wls2","title":"Access from remote host to Ubuntu WLS2","text":"<p>On the windows host we need to enable port proxy. Using a powershell terminal </p> <pre><code>netsh interface portproxy add v4tov4 listenport=3389  listenaddress=0.0.0.0 connectport=3389 connectaddress=192.168.85.149\n</code></pre>"},{"location":"techno/wsl2/#be-able-to-use-docker","title":"Be able to use docker","text":""},{"location":"techno/cp4a/","title":"Cloud Pak for Automation","text":"<p>Info</p> <p>Updated 3/17/2022</p> <p>IBM Cloud Pak for Business Automation is a set of integrated market-leading software, running on top of Red Hat OpenShift and therefore built for any hybrid cloud. Support following business automation capabilities \u2014 for content, decisions, process mining, RPA, workflows, and document processing, but add uniform platform experience, better component reuse.</p> <p></p> <p>Running in containers and Kubernetes is to improve cost of operation and deployment. For example using Liberty will reduce the packaging from multiple GB to only 800MB, start faster 10s, and simpler configuration.</p>"},{"location":"techno/cp4a/#value-propositions","title":"Value propositions","text":"<p>Companies need to apply intelligent automation across the enterprise to improve profitability, revenue growth</p> <ul> <li>Reduce customer service disruptions</li> <li>Improve management of IT service requests</li> <li>Empower LOBs through low-code tooling to quickly develop simple business application</li> <li>Identify gaps in operations workers</li> </ul>"},{"location":"techno/cp4a/#use-cases","title":"Use cases:","text":"<ul> <li>Digitize processes to maintain business continuity</li> <li>Approve operations tickets and exceptions</li> <li>Automate KYC regulatory compliance</li> <li>Quickly identify fraudulent transactions</li> <li>Reduce time from procure to pay</li> <li>Speed order to cash</li> <li>Speed employee onboarding</li> </ul>"},{"location":"techno/cp4a/#refund-request-automation","title":"Refund request automation","text":"<ul> <li>Model as-is processes to identify process opportunities</li> <li>Digitize and capture data from refund request forms</li> <li>Create customer facing application using low-code tools</li> <li>Mine process data from line of business systems to help analyze existing performance gaps</li> <li>Monitor customer support KPIs around refund processing with real time dashboard</li> <li>Automate important refund decisions: warranty checks, troubleshooting pathways, fraud detection, etc. - prior to approval</li> <li>Automatically route requests requiring manual review to the appropriate customer support representative(s)</li> <li>Use workflow to streamline the refund process and help ensure timely refunds for valid request</li> </ul>"},{"location":"techno/cp4a/#common-challenges","title":"Common challenges","text":"<ul> <li>Business data is inaccessible and siloed preventing a complete view of customers and hindering compliance initiatives</li> <li>Performance gaps caused by manual systems prevent efficiency and lack of reliable insights prevents operational visibility</li> <li>Execution on key initiatives is slowed because of the lack of the right tools or right skills</li> </ul>"},{"location":"techno/cp4a/#product-capabilities","title":"Product capabilities","text":"<ul> <li>Process mining: to discover inefficiencies in existing business process.</li> <li>Robotic process automation: digital employees</li> <li>Operational intelligence: gain insights with built-in AI to provide recommended actions</li> <li>Capture: extract information from document.</li> <li>Content: share and manage content - connect content to process</li> <li>Decision</li> <li>Workflow: choreograph human and systems. Improve consistency across buiness operations with increased visibility. </li> </ul>"},{"location":"techno/cp4a/#foundational-services","title":"Foundational services","text":"<ul> <li>Zen UI hostname is now the only hostname that is exposed externally - it is the unique front door to access any capabilities</li> <li>Authentication and single sign-on (SSO) are now provided by IBM Cloud Pak foundational services. If Zen cannot authenticate the user, then the request is redirected to the Identity and Access Management (IAM) service for authentication. If the user is authenticated, Zen issues a JWT token. Zen JWT tokens are understood and shared by all the Cloud Pak components.</li> </ul>"},{"location":"techno/cp4a/#business-automation-workflow-authoring-and-automation-workstream-services","title":"Business Automation Workflow Authoring and Automation Workstream Services","text":"<p>Workflow automation orchestrates multiple business processes (straight-through, human-assisted or case management) within operations and provides visibility into each step. Automation Workstream Services (AWS) provides a no-code solution for streamlining, automating, and accelerating activities in your daily work</p> <p>When selecting BAW, we can have optional components like Case, Content integratio, or workstreams.</p>"},{"location":"techno/cp4a/#odm-to-cp4automation","title":"ODM to CP4Automation","text":"<ul> <li>Access to New Value-Added Offerings and Containerized IBM Middleware</li> <li>Access to Industry-Leading Red Hat Cloud Platform &amp; Solutions</li> <li>License Flexibility</li> <li>Cloud Friendly metrics: VPC and MVS metrics</li> <li> <p>Full stack certification including open source support</p> </li> <li> <p>Monitor rules execution in real-time using dashboards (BAI only available on CP4A)</p> </li> <li>Collect operational data for analysis and ML to identify patterns for continuous improvement of rules. (BAI)</li> <li>Extend the value of rules by automating employee tasks and processes</li> <li>Gain enterprise-grade container security and simplified administration</li> <li>Improve the quality of automated decisions with data from unstructured documents (Data CAP)</li> </ul>"},{"location":"techno/cp4a/#business-automation-application","title":"Business automation application","text":"<p>Use Business Automation Application (BAA) to create and run business applications that use the capabilities of the platform. Build low-code business applications using Application Designer, test them with Playback Application Engine and run them in Application Engine. These applications are available to users in Business Automation Navigator.</p> <p>When selecting busness automation application, we can select business automation navigator and business orchestration optional components.</p>"},{"location":"techno/cp4a/#installation","title":"Installation","text":""},{"location":"techno/cp4a/#concepts","title":"Concepts","text":"<p>Docker images help to maintain product delivery and access to iFixes from a central registry.  Kubernetes helps to standardize on Operation and SRE skillset, innate HA, better application isolation,  and improved portability.  </p> <p>Container Federation is the concept to share capabilities between products:</p> <ul> <li>Task federation (PFS)</li> <li>Single UI (Navigator)</li> <li>Common content services (CPE)</li> <li>Aggregated monitoring/KPIs (BAI)</li> <li>Federated BPM Portal</li> <li>All application tier federated by default (BAW) </li> </ul>"},{"location":"techno/cp4a/#operators","title":"Operators","text":"<ul> <li>Operator is a long running process to perform products (Operands) deployment and Day 2 operations, like upgrades, failover, or scaling. Operator is constantly watching your cluster\u2019s desired state for the software installed. </li> <li>Operator Lifecycle Manager (OLM): Helps you to deploy, and update, and generally manage the lifecycle of all of the Operators (and their associated services) running across your clusters. The operator lifecycle manager (OLM) acts as the management system for the operators on that cluster. Cluster administrators control which operators are available and who can interact with the running operators.</li> </ul> <p>The following operators are installed with Cloud Pak for Automation</p> <ul> <li> <p>IBM\u00ae Automation Foundation Core: </p> <ul> <li>RPA-driven automation, process mining, mongoDB for Identity and Access Management (IAM), metering, OpenID,..  Zen UI.</li> </ul> </li> <li> <p>Cloud Pak foundational services: (bedrock - common services) Product doc. It includes IAM and certificate management. The services connect all IBM Cloud Paks with Operand Deployment Lifecycle Manager (ODLM) and use a specific namespace (ibm-common-services) to manage the service instances</p> </li> <li>IBM\u00ae Automation Insights foundation operator installs the required dependency operators, such as the IBM Events Operator, the Elasticsearch Operator and the Event Processing Operator.</li> <li>Cloud Pak for Business Automation includes Business Automation Studio and Business Automation Navigator to provide a unified authoring environment and an entry point to various low-code design tools with a single sign-on (SSO) experience and team management.</li> </ul>"},{"location":"techno/cp4a/#namespace","title":"Namespace","text":"<p>Divide workloads into dedicated namespaces for the application life cycle: dev, staging, production. Meter can be used to understand the deployments against entitlements.</p> <p>An administrator can configure the role and role binding resources for each application before any operator is deployed.  Each application must specify a serviceAccountName in its pod spec, and the service account must be created.</p>"},{"location":"techno/cp4a/#crd","title":"CRD","text":"<p>The Cloud Pak for Business Automation operator uses a custom resource definition (CRD), which describes  what the operator is meant to watch.</p>"},{"location":"techno/cp4a/#crs","title":"CRs","text":"<p>Then Automation is using a single CR to define the capabilities you want to deploy.</p> <p>Here is an important note on relationship between capabilities and operators. </p> <p>Depending on the selected capabilities, the needed components of the foundation are installed. The final custom resource file combines capabilities and components from one or more capabilities.</p>"},{"location":"techno/cp4a/#product-installation-general-steps","title":"Product installation general steps","text":"<ul> <li>Preparing OpenShift Cluster: The installation needs a dynamic storage class and a block storage class.  If any other Cloud Pak needs to be installed into the same cluster, you must use the same choice for the namespaces because IBM Automation foundation is a shared resource between Cloud Paks.</li> <li>Get entitlement key</li> <li>Install an instance of LDAP for your intended deployment. For dev purpose we use OpenLDAP and see DBA GitOps catalog for that.  For production deployment Tivoli Active Directory or Microsoft Active Directory are recommended.</li> <li>Install Openshift GitOps operator</li> <li>Install Cloud Pak Business Automation operator in <code>openshift-operators</code> monitoring All namespaces.</li> <li>Install operand using Custom Resource for each targeted environment (dev, staging, production). This is the tricky part as coherence is needed.</li> </ul>"},{"location":"techno/cp4a/#capacity","title":"Capacity","text":"<p>A cluster with all capabilities needs 11 nodes (see system requirements):</p> <ul> <li>Master (3 nodes): 4 vCPU and 8 Gi memory on each node.</li> <li>Worker (8 nodes): 16 vCPU and 32 Gi memory on each node.</li> </ul> <p>For ADP and deep learning processing of document, some Nodes need to get GPU and CPU must meet TensorFlow AVX requirements.</p> <p>For demo purpose 3 nodes are enough. </p> <ul> <li>Three worker nodes with 32 CPUs and 64Gi RAM (e.g., flavor c3c.32x64 on ROKS)</li> <li>One db2 worker node with 32 CPUs and 128Gi RAM (e.g., flavor b3c.32x128 on ROKS)</li> </ul> <p>Before 2021.3, We can only have one instance of Cloud pak automation on one cluster</p>"},{"location":"techno/cp4a/#starter-deployment-for-demo-purpose","title":"Starter deployment for demo purpose","text":"<p>This section is a summary of the product documentation with links to assets to deploy with CLI.</p>"},{"location":"techno/cp4a/#preparing","title":"Preparing","text":"<p>See the instructions in infra repo</p>"},{"location":"techno/cp4a/#installing","title":"Installing","text":"<p>The \"starter\" deployment provisions Db2\u00ae and OpenLDAP with the default values, so you do not need to prepare them in advance.</p> <p>Deployment is centralized by one unique CR that specifies the capabilities to use, and to configure how to access the external services like LDAP.</p> <ul> <li>Create a CP4BA deployment cluster CR: See example in BAW BAI CR in a dba-infra-gitops project. Can be done manually or with the <code>./cert-kubernetes/scripts/cp4a-deployment.sh</code> tool.</li> </ul> <pre><code>  # from dba-infra-gitops/    \n  oc apply -f environments/dba-dev/services/baw-bai-cr.yaml\n</code></pre> <ul> <li>Get the  <code>cp4ba-access-info</code> ConfigMaps for the different URLs to access the deployed capacities.</li> </ul> <pre><code>oc describe cm icp4adeploy-cp4ba-access-info\n</code></pre> <ul> <li>Troubleshooting: https://www.ibm.com/support/pages/node/6426995</li> </ul> <p>See also the SWAT team repository to setup CP4Automation for demonstration purpose.</p>"},{"location":"techno/cp4a/#air-gapped","title":"Air Gapped","text":"<ul> <li>Need to have an existing container image registry, protected with a TLS certificate signed by a Custom CA</li> <li> <p>Update pull-secret by adding the information fo your own registry</p> <p> * Mirror OpenShift Container Platform * Mirror RedHat Operator Catalog (OperatorHub) * Mirroring CloudPak Container Images</p> </li> </ul> <p>TBC</p> See documentation <ul> <li>OpenShift: Mirroring images for a disconnected installation</li> <li>Production deployment gitops note</li> </ul>"},{"location":"techno/cp4a/#production-deployment","title":"Production deployment","text":"<p>For production deployment see the product documentation</p>"},{"location":"techno/cp4a/#deploying-openldap","title":"Deploying OpenLDAP","text":"<ul> <li>Article from  Garage team on how to deploy OpenLDAP server on OpenShift</li> <li>OpenLDAP bitmani documentation</li> </ul> <p>See openLDAP instance configuration in ibm-cloud-architecture/dba-gitops-catalog</p> <pre><code>oc apply -k environments/openLDAP\n# Test it:\noc rsh $(oc get po -o name -n openldap| grep ldap) -n openldap\n# In pod shell\nldapsearch -x -H ldap://localhost:1389 dc=example,dc=org -D \"cn=admin,dc=example,dc=org\" -w adminpassword\n</code></pre>"},{"location":"techno/cp4a/#deploy-postgresql","title":"Deploy PostgreSQL","text":"<p>The postgreSQL operator defines a new Kubernetes resource called \"Cluster\" representing  a PostgreSQL cluster made up of a single primary and an optional number of replicas that co-exist in a chosen Kubernetes namespace  for High Availability and offloading of read-only queries..</p>"},{"location":"techno/cp4a/#deploy-foundation-operators","title":"Deploy Foundation operators","text":"<p>See the silent setup script in this doc</p>"},{"location":"techno/cp4a/#deploying-one-of-the-automation-capability","title":"Deploying one of the Automation capability","text":"<p>You can then generate the custom resource (CR) file by using another script <code>./cp4a-deployment.sh</code></p> <p>This script will ask:</p> <ul> <li>deployment type (demo, enterprise)</li> <li>OpenShift deployment type (ROKS, OCP, CNCF)</li> <li>Automation capability: <pre><code>1) FileNet Content Manager \n2) Operational Decision Manager \n3) Automation Decision Services \n4) Business Automation Application \n5) Business Automation Workflow \n(a) Workflow Authoring \n(b) Workflow Runtime \n6) Automation Workstream Services \n7) IBM Automation Document Processing \n(a) Development Environment \n(b) Runtime Environment\n</code></pre> </li> </ul>"},{"location":"techno/cp4a/#uninstall","title":"Uninstall","text":"<ul> <li>Start by deleting the operator</li> <li>You can delete your custom resource deployments by deleting the CR YAML file or the CR instance.</li> <li>After you uninstall, you might want to clean up certain files and secrets that you applied to the cluster for specific capabilities.</li> <li>Unsintall foundation services</li> <li></li> </ul>"},{"location":"techno/cp4a/#getting-started","title":"Getting Started","text":"<p>Once installed, all the URLs, user and password information, you will need are in the <code>icp4adeploy-cp4ba-access-info</code> config map</p> <pre><code>oc describe cm icp4adeploy-cp4ba-access-info \n</code></pre> <ul> <li>check the Common service operator full version and deployed namespace</li> </ul> <pre><code>oc get csv --all-namespaces | grep common-service-\n</code></pre> <p>TBC</p>"},{"location":"techno/cp4a/#day-1-day-2-demo-script","title":"Day 1 &amp; Day 2 Demo script","text":"<p>Day 1 is installation. Day 2 is about keeping steady state of the application and platform.</p> <ol> <li>Present Operator Hub Catalog - search for IBM product</li> <li>Present Gitops Catalog: dba-gitops-catalog and the the IBM catalog definition</li> <li> <p>In the <code>openshift-operators</code> project, explain that operators are running as pods and are monitoring all Namespaces. so Operands can be deployed anywhere. Some common services operators are running in the <code>ibm-common-services</code> project. It is possible to isolate the deployment in one project like in the cp4ba project in below figure (also techzone pre-deployed cp4ba).</p> <p></p> </li> <li> <p>Those operators were deployed from a set of subscriptions: See this subscription.yaml file.</p> </li> <li>Go to the <code>IBM Cloud Pak for Business Automation</code> operator UI to explain the operator UI, review the subscription yaml. </li> <li>Review the structure of the dba-infra-gitops as a GitOps repository for bootstrapping operators, secrets and then deploy to different environments. </li> <li> <p>Review of the BAW-BAI custom resource explain the configuration and options</p> <pre><code>sc_deployment_patterns: \"foundation,workflow-workstreams\"\n## The optional components for the \"foundation pattern\" are ums, bas and bai.  If the user selects any of those optional components,\n## it will be set here. Available option:\nsc_optional_components: \"baw_authoring,business_orchestration,workstreams,bai\"\n</code></pre> </li> <li> <p>Go to ArgoCD UI and demonstrate that argo applications are monitoring the git repository for any updates, and apply change to the target manifests.</p> <ul> <li>Get GitOps admin user's password</li> </ul> <pre><code>oc extract secret/openshift-gitops-cluster -n openshift-gitops --to=-\n</code></pre> <ul> <li>Get URL:</li> </ul> <pre><code> chrome https://$(oc get route openshift-gitops-server -o jsonpath='{.status.ingress[].host}'  -n openshift-gitops)\n</code></pre> <p></p> </li> <li> <p>Quick access to product Zen UI(cpd)</p> <p>A ConfigMap is created in the namespace to provide the cluster-specific details to access the services and applications. The IBM Cloud Pak Platform (Zen) UI is used to provide a role-based user interface for all Cloud Pak capabilities:</p> <ul> <li>Get the cpd URL from the access-info configmap in the OpenShift console.</li> <li>Use LDAP enterprise loing, <code>cp4admin</code> user</li> <li>In the main Automation Console, use Design &gt; Business automations menu to start the designer front end. Select Workflow, &gt; Hiring Sample, and open it.</li> </ul> <p></p> <ul> <li>You can run the process step by step using the top right arrow.</li> </ul> </li> <li> <p>Product update: </p> <ul> <li>An update to the custom resource (CR) overwrites the deployed resources. The operator applies the changes during the control loop (observe, analyze, act) that occurs as a result of constantly watching the state of the Kubernetes resources.</li> <li>To remove a capability from the deployment, locate the specific XXX_configuration section and delete this line along with all of its parameters.</li> <li>In some cases, changes that you make in the custom resource YAML by using the operator or directly in the environment are not automatically propagated to all pods</li> </ul> </li> <li> <p>Product upgrade </p> <ul> <li>Before you start an upgrade, define a backup recovery process and take snapshots to enable rollback if needed</li> <li>Preparing your deployed custom resource for an upgrade</li> <li>Transparent upgrade by changing the operator release / channel</li> <li>Modify the differemt Custom Resource</li> </ul> </li> <li> <p>Explain monitoring</p> </li> </ol> <p>See this prometheus introduction</p> <ul> <li> <p>As Cloud Pak for automation are custom applications in the context of OpenShift monitoring, we need to define  <code>User Project Monitoring</code>. To do so </p> <pre><code># in infra gitops project\noc apply -f config/monitoring/cluster-monitoring-config-cm.yaml -n openshift-\n# verify the monitoring pods\noc get pods -n openshift-user-workload-monitoring\n</code></pre> </li> <li> <p>the IBM Cloud Pak Foundational Services makes it easy to deploy an additional instance of Grafana.</p> <pre><code># verify Grafana services are deployed\noc get pods -n ibm-common-services -l app.kubernetes.io/managed-by=ibm-monitoring-grafana-operator\n</code></pre> <p>The Grafana based services are deployed and managed by the IBM Monitoring Grafana Operator.</p> </li> <li> <p>Verify grafana is running</p> <p><pre><code>oc get pods -w -n ibm-common-services -l app=grafana\n# Get the console url\noc get route -n ibm-common-services cp-console\n# login with OCP admin user, and then the monitoring option on the left.\n# This should take you to the Grafana UI. \n</code></pre>   * Enable monitoring in the custom resource if not already done.    * Notify Prometheus of additional targets where it can scrape custom metrics from. Use a ServiceMonitor provided by the Prometheus operator to specify additional endpoints for a Prometheus instance</p> <pre><code>oc apply -f environments/cp4ba/services/monitoring/cp4a-operator-monitor.yaml -n cp4ba\n</code></pre> </li> <li> <p>Define Grafana dashboard and set the organization to be <code>cp4ba</code>, and click on the sample dashboard.</p> <pre><code>oc apply -f environments/cp4ba/services/monitoring/cp4ba-sample-dashboard.yaml \n</code></pre> </li> </ul> <p>The simple dashboard visualizes the <code>ansible_operator_reconciles_count</code> counter and the rate of change for the <code>workqueue_unfinished_work_seconds</code> counter.</p> <p></p> <ul> <li>Go to Monitoring in OCP console Monitor&gt; </li> </ul> <p></p> More reading <ul> <li>Monitor Components of the IBM Cloud Pak For Business Automation</li> <li>Monitoring parameters</li> </ul>"},{"location":"techno/cp4a/#the-client-onboarding-demo","title":"The Client Onboarding demo","text":"<ul> <li>Pdf and instructions github.com/IBM/cp4ba-labs</li> <li>repository: dba-onboarding-automation</li> </ul> <p>The client application includes three pages to support following use cases:</p> <p></p> <p>The back-office workflow using the Workflow capability may involve an account manager, the client, and/or the client representative</p> <p></p> <p>Summary:</p> <ul> <li>'Client Onboarding app' is created with Automation Application Designer</li> <li>Workflow orchestrate back office services </li> <li>Decision to categorize client in different segment</li> <li>Use RPA to update older applications</li> </ul> <p>See this template-for-the-client-onboarding-demo</p>"},{"location":"techno/cp4a/ads/","title":"Automation Decision Service","text":"<p>Info</p> <p>Updated 11/19/21</p> <p>Automation Decision Services provides decision modeling capabilities that help business experts capture and automate repeatable decisions.  Automation Decision Services comes with two main components that can be installed separately:</p>"},{"location":"techno/cp4a/ads/#concepts","title":"Concepts","text":"<ul> <li>Decision Designer to develop decision model. It is connected to Github to manage decision artifacts. It is used to build and deploy the decision runtime.</li> <li>Decision Service: runtime to get ruleset and executes rule engine</li> </ul> <p>A decision service uses decision artifacts to define a business decision:</p> <ul> <li>decision model</li> <li>data model</li> <li>predictive scoring</li> <li>external libraries</li> <li>decision operations</li> </ul> <p>The decision model may include call to a predictive scoring done with ML model.</p>"},{"location":"techno/cp4a/ads/#decision-model","title":"Decision model","text":"<p>Decision model diagrams are composed of a set of nodes that are used as building blocks to represent decisions in a graphical way:</p> <ul> <li>Decision nodes represent the end decision, that is the decision that you want to automate, and the subdecisions that the end decision depends on.</li> <li>Data nodes represent the data definition that is needed to make a decision.</li> <li>Function nodes encapsulate computations from other decision models.</li> <li>Prediction nodes encapsulate predictions that you can call directly from your decision model.</li> <li>External libraries contain data types and functions to be used inside the decision models</li> </ul>"},{"location":"techno/cp4a/ads/#installation","title":"Installation","text":"<p>Being part of Cloud Pak for Automation, we need to install it on OpenShift.  The install doc is here.  Installation uses operator lifecycle manager (OLM).</p> <p>IBM Cloud Pak for Business Automation comes with the IBM Cloud\u00ae platform automation foundation which includes:</p> <ul> <li>Process Mining from myInvenio</li> <li>Robotic Process Automation</li> <li>MongoDB</li> <li>Zen User Interface</li> <li>Biz Automation Insight Apicurio Registry, Kafka, Flink, Elastic Search is now part of  Automation Foundation</li> <li>Common services: IAM, certificate management, User Management Services, Admin UI, license operator</li> </ul> <p>Summary of installation steps:</p> <ul> <li>Get IBM license entitled registry key</li> <li>Get the storage class name to use for dynamic storage</li> <li>Prepare storage for cloud pak operator</li> <li>Download the k8s certificates  and configuration to prepare the OCP cluster</li> <li>Define a project where the Cloud Pak will be installed and then modify the cluster_role_binding</li> <li>Run the <code>cert-kubernetes/descriptors/cp4a-clusteradmin-setup.sh</code> script</li> <li>Define the custom resource manifest to control the product to install, use create  or <code>oc create -f ...</code></li> <li>After the Automation Decision Services container is deployed to the cluster,  we need to take additional steps (add maven plugin) to be able to build and deploy decision services.</li> </ul>"},{"location":"techno/cp4a/ads/#getting-started","title":"Getting started","text":"<p>Get access to a CP4Automation console, use to the Business Automation Studio.</p> <p>Got to Design, and click Business automation &gt; Decision &gt; Create and select Decision automation</p> <ul> <li>Once project is created, need to add a decision service</li> </ul> <p></p> <p>Data enters through input data nodes, and is processed by rules in decision nodes. The rules define the logic of the decision. They are expressed in business rules and decision tables</p> <ul> <li>create a decision model using DMNotation</li> <li>Add data model to define input and output types</li> <li>Add rules and decision tables to the different decision nodes. Combine  node output to input to chain decisions.</li> <li>Define an operation used to call the service. </li> <li>Connect to github repository (first time the repository needs to be empty)</li> <li>Deploy to a run time server</li> <li>Get the service end point URL. Which is based on a schema like</li> </ul> <p><code>/deploymentSpaces/{deploymentSpaceId}/decisions/{decisionId}/operations/{operation}/execute/</code></p> <p>The <code>deploymentSpaceId</code> is <code>embedded</code>, the <code>decisionId</code> is coming from the deployment, it has a jar name inside,    and the operation is the  name of the decision service operation selected. </p> <p>The root URL is  the ads-runtime which we  can get from </p> <pre><code> ADS-runtime-access-info:\n ----\n Runtime URL: https://ads-runtime-ibm-cloudpaks.....-0000.us-east.containers.appdomain.cloud/ads/runtime/api/swagger-ui\n username: drs\n password: .....\n</code></pre> <p>The route is using the <code>icp4adeploy-ads-runtime-service</code> service, so if we deploy an app on the same cluster   we should be able to get the internal URL.</p> <ul> <li>Use the integrated swagger-ui to test the service. Use the Authorize to define the user to use to connect. This is the <code>drs</code> service user and the password is used.</li> </ul> <p>See this article with screen shots for a bigger example with predictive scoring</p>"},{"location":"techno/cp4a/ads/#client-app","title":"Client app","text":"<p>A decision runtime instance is deployed as a Kubernetes pod that is based on WebSphere\u00ae Application Server Liberty.  The runtime archive repository can be any HTTP-based server that is able to store files.  The runtime downloads decision service archives from the runtime archive repository. ach decision runtime instance is able to execute multiple distinct decisions.  The runtime caches decisions to lower the cost of loading the decision service archives. The <code>decisionId</code> is used as key for rule archive in the cache.</p> <p>Your client application calls a decision service through the decision runtime REST API.</p>"},{"location":"techno/cp4a/ads/#collaborating","title":"Collaborating","text":"<p>To be able to support CI/CD we need to get the ADS Maven plugin. This is done by performing the ADS post installation tasks:</p> <ul> <li>Get a UMS user</li> <li>Be sure to have installed a Nexus server to OpenShift</li> <li>Authenticate to ADS_BASE_URL with the access token of the UMS user.</li> </ul>"},{"location":"techno/cp4a/ads/#not-yet-there","title":"Not yet there","text":""},{"location":"techno/cp4a/ads/#useful-links","title":"Useful links","text":"<ul> <li>Product doc - getting started goes over a simple decision model to send a message according to some weather data.</li> <li>ODM Docker image</li> <li>ADS Compose</li> <li>Eclipse Oxygen needed for Decision Designer</li> <li>ADS samples repository</li> </ul>"},{"location":"techno/cp4a/ads/#older-info","title":"Older info","text":"<p>Launch locally ODM for older rule implementation.</p> <pre><code>docker run -e LICENSE=accept -p 9060:9060 -p 9443:9443  -m 2048M --memory-reservation 2048M -v $PWD:/config/dbdata/ -e SAMPLE=true ibmcom/odm:8.10\n</code></pre>"},{"location":"techno/cp4a/ads/#odm-cicd-articles","title":"ODM ci/cd articles","text":"<ul> <li>Peter Warder's article</li> <li>Rule Designer build automation tool</li> <li>Automated deployment of decision services</li> </ul>"},{"location":"techno/cp4a/bai/","title":"Business Automation Insight -&gt; Self driving enterprise","text":"<p>Goal: BAI processes event data (from IBM Business automation products) so that you can derive insights into the performance of your business.</p> <p>It will be the foundation for self driving enterprise.</p>"},{"location":"techno/cp4a/bai/#value-propositions","title":"Value propositions","text":"<ul> <li>AI continuously listen to events to correlate and derive business context to actionable insight in real time</li> <li>Deliver visibility, apply analytics or machine learning algorithms that add intelligence to the platform and  provide guidance to knowledge workers and business owners.</li> <li>Detect: the insights engine builds a 360\u00b0 view of the business to correlate events in-context, derives insights by applying analytics and detect business-relevant situations by applying rules, CEP and ML. </li> <li>Business owners keep control when automation fails</li> <li>Able to fully explain and account for its observations, correlations and actions</li> <li>Self-driving automation leapfrog traditional automation of tasks and workflow, by triggering actions  based on insights and their impact on objectives and KPIs and automates across Business and IT silos</li> </ul>"},{"location":"techno/cp4a/bai/#features","title":"Features","text":"<ul> <li>Ingest and process records from Kafka, store to Elastic Search</li> <li>Unify on single event framework for Automation and AI.</li> <li>Aggregate these events into business-relevant KPIs</li> <li>Monitoring with Kibana dashboards: Kibana uses index patterns to search indexes defined within Elasticsearch.  An index pattern can match the name of a single index, or include a wildcard (*) to match multiple indexes</li> <li>Store events to data lake like HDFS for off line queries</li> <li>Business Performance Center to visualize your data and monitor business performance</li> <li>Add anonymization rules to secure sensitive data</li> </ul> <p>21.0.x product documentation</p>"},{"location":"techno/cp4a/bai/#use-cases","title":"Use cases","text":"<ul> <li>Apply analytics with Brunel library to get insights on the decisions done in ODM, for example on loan origination, and the variables used to do eligibility.</li> <li>How to take better benefit of BPM process execution data and inject Machine Learning techniques to predict the duration of BPMN processes,  based on the data captured by the process</li> </ul>"},{"location":"techno/cp4a/bai/#architecture","title":"Architecture","text":"<p>The features above are supported by different components as illustrated below:</p> <p></p> <p>Event Source systems will be Business Automation products.</p> <p>Looking at the components, the BAI processing is based on Flink jobs: The distribution includes a set of jars for each supported event source.</p> <p></p> <p>The BAI operators creates a set up jobs to configure Elastic search and Kibana dashboard for Automation components like BAW, BPMN, ICM, Content.</p> <p></p> <p>Elasticsearch and kafka are deployed with IAF automation base. Flink cluster is defined with IAF Event Processor and BAI operator</p> <p>The Kubernetes jobs are here to create resources like Flink cluster, and for each job, like the BPMN job, contains the jars file for Flink job.  The Flink jobs are specifics for each component event processing needs.</p> <p>The management service is the facade for to submit Flink job.</p> <p>There is a generic Flink job, call the event-forwarder to process events from kafka and send telemetries to elasticsearch. </p>"},{"location":"techno/cp4a/bai/#event-sources","title":"Event Sources","text":"<p>Sources of events for BAI are any of:</p> <ul> <li>Components of the IBM Cloud Pak for Business Automation platform</li> <li>Any Kafka records in Kafka Topics</li> <li>Supports BPM events to monitor the lifecycles of processes and activities, looped activities,  tasks, gateways, timers, messages, and tracking groups. Aggregation of process time series, or from the business data</li> <li>Most of the older DBA products sent events in a predefined format, while ADS, Workforce Insight or custom event source uses Avro Schema.</li> </ul>"},{"location":"techno/cp4a/bai/#high-availability","title":"High Availability","text":""},{"location":"techno/cp4a/bai/#getting-started","title":"Getting started","text":"<p>It is possible to run BAI on linux or mac laptop, but the main deployment is on OpenShift.</p>"},{"location":"techno/cp4a/bai/#on-mac","title":"On mac","text":"<p>See the product documentation for MacOS deployment</p> <p>We need to get the images from Passport Advantage searching 'business automation insight', to get a zip named  <code>CP4Auto_20.0.3-bai4s.tgz</code>. Unzip and start the <code>bai-start --acceptLicense</code> with <code>--init</code> for the first time to get certificates and users set up. The error message about port numbers not set for any of the services started by docker compose is due to not executing  the <code>.env</code>. The questions asked help to populate this .env file.</p> <p>The env file needs to be modified to export all the env variables declared, as docker-compose will not see them if not exported</p> <pre><code>bai-for-server/bai-flink:20.0.3                           6123/tcp, 8081/tcp                                     data_processors-deployer_1\nbai-for-server/bai-admin:20.0.3                           0.0.0.0:6892-&gt;6892/tcp                                 data_admin_1\nconfluentinc/cp-schema-registry:5.5.1                     8081/tcp, 0.0.0.0:8084-&gt;8084/tcp                       data_schema-registry_1\nbai-for-server/bai-setup:20.0.3                                                                     data_setup_1\nconfluentinc/cp-kafka:5.5.1                               9092/tcp, 0.0.0.0:29092-&gt;29092/tcp                     data_kafka_1\nbai-for-server/bai-flink-taskmanager:20.0.3               6123/tcp, 8081/tcp                                     data_taskmanager_1\nbai-for-server/bai-monitoring-app:20.0.3                  9080/tcp, 0.0.0.0:9443-&gt;9443/tcp                       data_business-performance-center_1\nbai-for-server/bai-kibana:20.0.3                          0.0.0.0:5601-&gt;5601/tcp                                 data_kibana_1\nbai-for-server/bai-flink:20.0.3                           6123/tcp, 8081/tcp                                     data_jobmanager_1\nbai-for-server/bai-elasticsearch:20.0.3                   9200/tcp, 9300/tcp                                     data_elasticsearch_1\nbai-for-server/bai-management:20.0.3                                                                  data_management_1\nconfluentinc/cp-zookeeper:5.5.1                           2181/tcp, 2888/tcp, 0.0.0.0:2121-&gt;2121/tcp, 3888/tcp   data_zookeeper_1\n</code></pre>"},{"location":"techno/cp4a/bai/#on-roks","title":"On ROKS","text":"Documentation <ul> <li>Preparing to install Business Automation Insights</li> </ul> <p>Some capabilities to remember:</p> <ul> <li>Multiple installations of the Cloud Pak are supported, but each deployment must be installed in a different namespace and the operator needs to be installed for each namespace</li> <li>Install an instance of Lightweight Directory Access Protocol (LDAP) for your intended deployment. Not necessary for BAI.</li> </ul> <p>The questionnaire to assess before installation</p> <p>Starting with 21.0.2, BAI is part of Automation Foundation.</p> <ul> <li>You sould use a custom deployment for the AutomationBase resource to control your Kafka and ElasticSearch deployment.</li> </ul> <p><pre><code>oc edit automationbase foundation-iaf\n</code></pre> * When doing so modify the CP4BA custom resource to do not use the default deployment</p> <pre><code>shared_configuration:\n  sc_install_automation_base: false\n</code></pre> <ul> <li> <p>Define data permission access for Business Performance Center</p> </li> <li> <p>Create PVCs for storage and logs</p> </li> </ul> <pre><code># in ibm-cp-automation/inventory/cp4aOperatorSdk/files/deploy/crs/cert-kubernetes/descriptors\n# update with the storage classes (file) to use in operator-shared-pvc.yaml \noc apply -f operator-shared-pvc.yaml \n</code></pre>"},{"location":"techno/cp4a/bai/#samples","title":"Samples","text":"<p>Sample data for IBM Business Automation Insights</p> More reading <ul> <li>BAI product documentation</li> <li>ODM Kibana dashboard</li> <li>BPM or ODM sample dashboards</li> <li>ODM and BAI Getting Started document</li> <li>handle custom events</li> <li>Notebooks for Machine learning with Business Automation Insights</li> </ul>"},{"location":"techno/cp4a/odm/","title":"Operational decision management","text":"<p>Install Rule Designer from the Eclipse Marketplace: first install the compatible Eclipse 2022-06, then add the marketplace links to the repository to access ODM (Help &gt; Install New Software &gt;  search for decision).</p>"},{"location":"techno/cp4a/odm/#getting-started","title":"Getting Started","text":"<p>Git repo for getting started guide.</p>"},{"location":"techno/cp4a/odm/#deployment-to-openshift","title":"Deployment to OpenShift","text":"<p>Follow link for details about this product.</p> <ul> <li>Need LDAP</li> <li>Need a database like Postgresql</li> <li>You do not have to integrate with the User Management Service (UMS)</li> <li>Running the <code>./cp4a-deployment.sh</code> script and selecting ODM, you have the choice to select ODM sub components like<pre><code>1) Decision Center (Selected)\n2) Rule Execution Server (Selected)\n3) Decision Runner \n4) User Management Service \n5) Business Automation Insights\n</code></pre> </li> </ul> <p>When doing Enterprise deployment the LDAP type supported does not include OpenLDAP, we can select microsoft active directory and modify the generated CR.yaml.</p> <p>The CR has the following declarations to review</p> <pre><code>```yaml\nsc_deployment_type: \"enterprise\"\nsc_deployment_platform: \"ROKS\"\nsc_deployment_patterns: \"foundation,decisions\"\nsc_optional_components: \"decisionCenter,decisionServerRuntime\"\nodm_configuration:\n# To enable ODM Runtime.\ndecisionServerRuntime:\n    enabled: true\n    replicaCount: 2\n# To enable the Authoring part\ndecisionRunner:\n    enabled: false\n    replicaCount: 2\ndecisionCenter:\n    enabled: true\n    replicaCount: 2\n```\n</code></pre> <p>And more to update for LDAP and Postgresql DB</p>"},{"location":"techno/cp4a/odm/#other-deployments","title":"Other Deployments","text":"<ul> <li>Deploy on AWS ECS with RDS Postgres and secret management. </li> </ul>"},{"location":"techno/cp4a/process-mining/","title":"Process mining","text":"<p>Process mining is about obtaining knowledge of, and insights in, processes by means  of analyzing the event data, generated during the execution of the process.</p> <p>IBM Process Mining  automatically discovers, constantly monitors, and optimizes business processes.</p> <p>During process mining, specialized data mining algorithms are applied to identify trends,  patterns, and details contained in event logs recorded by an information system.</p>"},{"location":"techno/cp4a/process-mining/#features","title":"Features","text":"<ul> <li>Ingest data of process execution from business apps, or middleware or logs</li> <li>Simulation capability:  business analysts can use to test unlimited process change.  It combines historical data with contextual data, like decision routing rules, to create what-if scenarios that are then analyzed</li> <li>Calculate metrics</li> <li>Task Mining captures and sends real user interaction data on desktops to process mining server.</li> <li>Task mining helps to discover how much time users are allocating on the process and how much time the activities are idle because of context switches.  It can identify deviation and inefficiencies, and potential task to automate.</li> </ul>"},{"location":"techno/cp4a/process-mining/#architecture","title":"Architecture","text":"<ul> <li> <p>Process mining has 4 components</p> <ul> <li>Process Discovery</li> <li>BPA: for simulation and keep processes, roles and systems information</li> <li>Analytics</li> <li>Admin to manage tenants, groups, users,...</li> </ul> </li> <li> <p>Task Miner has an Agent running locally to the client Windows workstation to track user activities,  encrypt the data and sends to the Data Collector (at a configured threshold).</p> </li> <li> <p>On the server side, Task Mining has 3 components</p> <ul> <li>Data Collector: </li> <li>Data processor</li> <li>Task miner</li> <li>We can add a persistence layer to keep projects and anonymized collected data.</li> </ul> </li> </ul> <p></p> <p>Which can be seen for a production deployment as:</p> <p></p> <p>Important to not that process engine is supporting analysts users and data at rest processing.  So the impact on resource requirements are the number of concurrent users from 2 to 30 (?), and the complexity of the process. One key performance factor is the data quality.</p>"},{"location":"techno/cp4a/process-mining/#use-cases","title":"Use Cases","text":"<p>Understand the <code>procure to pay</code> process which connects the procurement and entire supply  chain processes within a company through the goods receipt process, and finally  to the payment issued to the vendor. The system is SAP, the top level activities are</p> <ul> <li>Purchase requisition</li> <li>purchase order</li> <li>send PO to Vendor</li> <li>Good receipt</li> <li>Invoice receipt</li> <li>payment</li> </ul> <p>Process mining helps business analysts to identify maverick buying by looking for:</p> <ul> <li>Orders without a purchase requisition</li> <li>Invoices without an order</li> </ul> <p></p> <p>We can use process mining to identify process path deviations and discover the root causes and the  impact of those deviations such as a cost associated with extra resources and process delays.  Process deviations are discovered by comparing model discovered from process mining data with a reference model  that could come from Blueworks live or other BPM tools.</p> <p></p> <p>The non-conformant activities can be identified by the red background color and  the non-conformant transitions are also set to red.</p>"},{"location":"techno/cp4a/process-mining/#getting-started","title":"Getting started","text":"<p>Access to a Process Mining Server via the NGInx server address. </p> <ul> <li>If needed create an organization</li> <li>Create a process </li> </ul> <p></p> <ul> <li>To analyze the process, you will need to upload a log file (.CSV or .XES) containing mined process data into the Data Source</li> <li>Once data loaded, maps identify data columns:  Process ID, Activity, Start time, End Time, Resource and Role</li> </ul> <p></p> <ul> <li>After mapping the logfile, you can visualize the process by creating process Model. Every time something changes in the Workspace or more data is added you will need to recreate the process Model.</li> </ul> <p></p> <p>Info</p> <p>The Model automatically displays the frequency analysis. The dark blue color  highlights the most frequent activities, whilst the bold arrows highlight the  most frequent transitions.</p> <ul> <li>The numbers next to the lines shows how many times that specific process flow  has been followed.</li> <li>The numbers within the rectangles shows the number of times that the activity is performed</li> <li>The description in the rectangles indicates the name of the activity and the roles by which the activity is carried out.</li> <li>The Activity border reflects the multilevel nature of the process.</li> <li>Control % of Activities (occurrences) and % of Relations by expanding Model details section</li> </ul> <ul> <li>Dashboard contains process and case statistics with perf stats over time.</li> </ul> <p></p> <ul> <li>BPMN view is built from the process data model. Some decision gateways are added to represent OR or XOR routing logic.  <code>Rule Discovery</code> option will analyze the data and try to define conditional statements for those routing decisions.  The problem will become, as those rules really relevant for the process flow. The BPMN flow can be exported and imported into BlueWorks Live or BAW. </li> <li>Activity Map are used to analyze human resources by name and job title. It highlights if employees are doing what they are supposed to be doing: activity by team, teams by activity...</li> </ul> <p></p> <ul> <li>Social net allows you to discover and analyze the relationships between users and groups that are formed within a process. In the <code>Doing similar tasks</code> view:</li> </ul> <p></p> <p>Info</p> <ul> <li>The bullets represent resources</li> <li>Resources are clustered by common activities carried out</li> <li>The bullet will be bigger for those resources who are sharing more activities</li> <li>Different colors identify different roles</li> </ul> <ul> <li>Analytics offers a set of predefined Dashboards to assess mavericks. </li> </ul>"},{"location":"techno/cp4a/process-mining/#installation","title":"Installation","text":"<p>On OpenShift, the process mining is installed via operator, with dependencies on common services and foundation core The operator supports instances of type <code>ProcessMining</code> CRD.</p> <ul> <li>Need storage class supporting the ReadWriteMany (RWX) mode</li> <li>Need MongoDB, DB2</li> <li>Need SSL</li> <li>Need entitlement key</li> </ul> <p>It is possible to deploy more than one operator instance in different namespaces.</p> <p>The Process Mining components adds two permissions to the Cloud Pak role management: \"Automation Analyst\" and \"Automation Administrator\"</p> <p>In order to offer Read Write Many (RWX) storage for the applications running on RedHat OpenShift cluster, we need to make OpenShift Data Foundation available in our RedHat OpenShift cluster. </p> <p>OpenShift Data Foundation (ODF) is a storage solution that consists of open source technologies Ceph, Noobaa, and Rook.  ODF allows you to provision and manage File, Block, and Object storage for your containerized workloads in Red Hat\u00ae OpenShift\u00ae on IBM Cloud\u2122 clusters.  Unlike other storage solutions where you might need to configure separate drivers and operators for each type of storage,  ODF is a unified solution capable of adapting or scaling to your storage needs.</p> <p>In order to install OpenShift Data Foundation (ODF) in our  RedHat OpenShift Kubernetes Service (ROKS) cluster on IBM Cloud on a Virtual Private Cloud (VPC) Gen 2,  we need to make sure that our OpenShift cluster counts with at least three worker nodes.  For high availability, we must create our OpenShift cluster with at least one worker node per zone  across three zones. Each worker node must have a minimum of 16 CPUs and 64 GB RAM.</p> <p>Important</p> <p>The <code>storageClass</code> used to configure OpenShift Data Foundation to request storage volumes must be of type <code>metro</code>. </p> <p>Info</p> <p>What <code>metro</code> means is that the <code>volumeBindingMode</code> of that <code>storageClass</code> will be set to <code>WaitForFirstConsumer</code> as opposed to  the default <code>Immediate</code>. And what that means is that the Persistent Volume creation and  allocation by the IBM Cloud Object Storage, as a result of its Persistent Volume Claim,  will not happen until the pod linked to that Persistent Volume Claim is scheduled.  This allows IBM Cloud Object Storage to know what Availability Zone of your MultiZone  Region cluster the pod requesting block storage ended up on and, as a result, to be  able to provision such storage in the appropriate place. Otherwise, if we used  a <code>storageClass</code> whose <code>volumeBindingMode</code> was the default <code>Immediate</code>, IBM Cloud Object Storage  would create and allocate the Persistent Volume in one of the Availability Zones which might  not be the same Availability Zone the pod requiring such storage ends up on as a result of the  OpenShift pod scheduler which would make the storage inaccessible to the pod.  See Kubernetes official documentation here for further detail.</p> <p>Important</p> <p>The <code>storageClass</code> you need to configure OpenShift Data Foundation to use with must not have Retain Reclaim policy.  Using <code>ibmc-file-gold-gid</code> seems to work</p> <p>Info</p> <p>If you retain the Persistent Volume, it might end up assigned to a pod in a different Availability Zone later, making that storage inaccessible to the pod allocated to. </p> <p>See the process mining subscription in dba-gitops-catalog</p>"},{"location":"techno/cp4a/process-mining/#integration-with-data-ingestion","title":"Integration with data ingestion","text":"<p>As of now there is no direct integration with real data streaming, like Kafka or MQ. So for a Kafka deployment we need to use S3 Sink with some record to CSV transformation.</p> <p></p>"},{"location":"techno/cp4a/process-mining/#read-more","title":"Read more","text":"<ul> <li>Academic papers on process mining</li> <li>Product marketing page</li> <li>Process mining REST API</li> </ul>"},{"location":"techno/data/","title":"Data related technologies","text":""},{"location":"techno/data/#apache-iceberg","title":"Apache Iceberg","text":"<p>Apache Iceberg is an open table format for huge analytic datasets. </p> <ul> <li>It adds tables to compute engines such as Spark, Trino, PrestoDB, Flink, Hive and Impala.</li> <li>It works just like a SQL table on cloud object storage.</li> <li>Iceberg solves correctness problems in eventually-consistent cloud object stores.</li> <li>It supports ten of petabytes of data, with potentials schema changes: column add, drop, rename, update, reorder, and certain data types upgrades.</li> <li> <p>It supports 'time travel' to go back to older version of the data.</p> <pre><code>SELECT * FROM iceberg_taxi_parquet\nFOR SYSTEM_TIME AS OF (current_timestamp \u2014 interval \u20181\u2019 hour)\n</code></pre> </li> </ul> <p>Iceberg has several catalog back-ends that can be used to track tables, like JDBC, Hive MetaStore and Amazon Glue.</p> <p>There\u2019re 3 layers for Iceberg:</p> <ol> <li>Catalog layer: Hive or Path based catalogs. </li> <li>Metadata layer: Each CRUD operation will generate a new metadata file which contains all the metadata info of table, including the schema of table, all the historical snapshots until now. Each version of snapshot has one manifest list file. Manifest file can be shared cross snapshot files and contains a collection of data files which store the table data.</li> <li>Data Layer: parquet files which contain all the historical data, including newly added records, updated record and deleted records.</li> </ol> <p>When a table is created, Iceberg creates a directory with the name of the table, and then it creates a metadata folder which contains all the metadata info. </p> <p>When records are added to the table, Iceberg creates one parquet file for each record. A new version of metadata file is created with information about a manifest list file (in avro format), which itself points to one manifest file which points to the parquet files</p> <p>When updating record, Iceberg creates snapshot to keep information of the new manifest file created for the update. The previous record is marked as deleted. Developers may query the history table of the database main table to see the different snapshots.</p>"},{"location":"techno/data/#interresting-content","title":"Interresting content","text":"<ul> <li>Getting started</li> <li>Icebert and Spark quickstart with local docker compose</li> <li>PyIceberg</li> <li>Medium article to use Iceberg with AWS Glue, and Athena.</li> </ul>"},{"location":"techno/data/#minio","title":"MinIO","text":"<p>A Hybrid/Multi-Cloud Object Storage on top of existing data storage like AWS S3, or local file systems, to provides storage consistency across every public cloud providers. MinIO is purpose-built to take full advantage of the Kubernetes architecture. Every new application is written for the AWS S3 API.</p>"},{"location":"techno/data/#apache-zeppelin","title":"Apache Zeppelin","text":"<p>Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala, Python, R, and any language / data processing backend...</p> <pre><code>docker run -u $(id -u) -p 8080:8080 --rm -v $PWD/logs:/logs -v $PWD/notebook:/notebook -e ZEPPELIN_LOG_DIR='/logs' -e ZEPPELIN_NOTEBOOK_DIR='/notebook'  -v $HOME/Code/Studies/spark-3.5.0-bin-hadoop:/opt/spark -e SPARK_HOME=/opt/spark --name zeppelin apache/zeppelin:0.10.0\n</code></pre> <p>Then open http://localhost:8080. Zeppelin is shipped with some built-in tutorials.</p> <p>See <code>startZeppelin.sh</code> in studies folder.</p>"},{"location":"techno/data/#concepts","title":"Concepts","text":"<ul> <li>Cell can have different interpreter configured with <code>%&lt;interpreter-name&gt;</code>. It tells Zeppelin which langage/backend to use to run the cell. (<code>%sh %python %sql %spark %pyspark</code>)</li> <li>Zeppelin has a Spark Context created, accessible via a session from the <code>spark</code> variable.</li> </ul>"},{"location":"techno/databricks/","title":"Databricks","text":"<p>Databricks helps organizations make their data ready for analytics, empowering data science teams to make data-driven decisions and rapidly adopt machine learning via there proprietary platform Data Lakehouse. The current Data Lakehouse platform tenets revolve around:</p> <ul> <li>Multi-tenant control plane ( 1 or more per region)</li> <li>Single-tenant data plane in Customers account</li> <li>Databricks workspace provides an interactive environment where users can collaborate, develop, and deploy data-driven applications. It offers a unified interface, supports multiple programming languages, and provides built-in libraries and frameworks. With features like version control, job scheduling, and integration with other tools and services.</li> <li>Each workspace maps to one VPC (one region)</li> <li>Multiple workspace maps to one VPC</li> <li>Cross account IAM roles used to launch and manage the clusters</li> <li>Clusters are used to connect to different data stores like Databases, streaming, on-prem, private hosted Github/code repos...</li> </ul> <p>Based on Apache Sparks, Delta Lake, and MLflow, with HDFS,. Once deployed on AWS, it uses resources like EC2, EKS, S3, IAM, EBS volumes...</p> <p>Workspace manages assets for a user:</p> <p></p> <ul> <li>Repos is to support integration with Git compatible repositories and clone locally in the workspace.</li> <li>Clusters to define different cluster run time to run notebooks or jobs.</li> <li>Secrets to keep keys and other sensitive information.</li> <li>Pipelines for data processing pipelines.</li> <li>Pools keeps idle VMs.</li> </ul>"},{"location":"techno/databricks/#value-propositions","title":"Value propositions","text":"<p>Data warehouse solutions were developed to address data silos done by using multiple, decentralized operational databases. The goal was to provide an architectural model for the flow of data from operational systems to decision support environments. Data warehouse has limitations to support big data, unstructured data, and to support ML use cases. Most of the time uses proprietary formats.</p> <p>Data lake, with Hadoop, was aiming to support big data processing, on parallel servers organized in cluster. Shortly after the introduction of Hadoop, Apache Spark was introduced. Spark was the first unified analytics engine that facilitated large scale data processing, SQL analytics, and AI Model Learning.</p> <p>Data lakes are difficult to set up, do not support transactions, do not enforce data quality, very difficult to mix append and read operations, batch and streaming jobs. We can add that modifying existing data is difficult, like a delete operation in the context of GDPR compliance. With data lakes, it is difficult to manage large metadata, and data catalogs. A lot of data lake projects became data swamp.</p> <p>A lakehouse is a new architecture that combines the best elements of data lakes and data warehouses. It enables users to do everything from BI, SQL analytics, data science, and ML on a single platform. It supports ACID transactions, large metadata, indexing, bloom filters, schema validation, governance to understand how data is used, direct access to source data, scalable. Data is saved in open data formats and supports structured and unstructured data.</p> <p></p> <p>Figure 1: Data lake - data pipeline</p> <p>The lakehouse, with Delta Lake, approach to data pipelines offers modern data engineering best practices for improved productivity, system stability, and data reliability, including streaming data to enable reliable real-time analytics.</p> <p>Big data and AI complexity slows innovation: managing big data infrastructure, define data pipelines to produce stale data with poor performance, isolated, without any collaboration between data scientists and data engineers. </p>"},{"location":"techno/databricks/#delta-lake","title":"Delta lake","text":"<p>Delta lake is an open approach to bring data management and governance on top of data lakes. It is a storage layer which offers the following characteristics:</p> <ul> <li>reliability via ACID transaction.</li> <li>performance via indexing to maximize the efficiency of the query.</li> <li>governance using Unity Catalog to enforce access control list on table.</li> <li>quality to support different business needs.</li> </ul> <p>Data Scientists can define flows and pipelines to process data from raw to filtered-cleaned-augmented, as a source of truth to business level aggregates.</p> <p>The persistence format of data on cloud object stores is Apache Parquet. Delta Lake persists transaction logs in the same folder as data, and it is easy, with Delta API, to get the state of the data inside parquet, for merge-on-read operation. The protocol supports addressing data in file.</p> <p>Delta Lake is available with multiple AWS services, such as AWS Glue Spark jobs, Amazon EMR, Amazon Athena, and Amazon Redshift Spectrum.</p>"},{"location":"techno/databricks/#deeper-dive","title":"Deeper Dive","text":"<ul> <li>Delta Lake documentation</li> <li>Crawl Delta Lake tables using AWS Glue crawlers</li> <li>Introducing native Delta Lake table support with AWS Glue crawlers</li> <li>Process Apache Hudi, Delta Lake, Apache Iceberg datasets at scale, part 1: AWS Glue Studio Notebook</li> </ul>"},{"location":"techno/databricks/#architecture","title":"Architecture","text":"<p>(src: Databricks)</p> <ul> <li>Control plane is managed by Databricks in their cloud account, and it includes backend services, webapp for workspaces, notebooks repository, job manager, cluster manager... It hosts everything except the Sparks Cluster.</li> <li>Each customer has his own workspace, any command runs in workspace will exist inside the control plane.</li> <li>Data plane is managed within the customer's cloud account. Data is own, isolated and secured by each customer. Data sources can be inside the customer account or as external services.</li> <li>There is a private network between the data and the control planes. A lot of control over how cloud accounts are integrated and secured.</li> </ul> <p>The Webapp is where customers access all the platform interfaces (APIs and UI):</p> <p></p> <p>(src: Databricks copyright)</p> <p>On AWS, the database is RDS and stores all the information about customers metadata, and workspace.</p> <p>Cluster manager is part of the control plane and helps admin to manage Spark Cluster. The Cluster manager is an extension of the Spark CM with nicer user interface.</p> <p></p> <p>(src: Databricks copyright)</p> <p>There are two types of cluster: 1/ All-purpose to be used to support interactive notebooks execution, 2/ Job clusters to run automated jobs.</p> <p>Workspace is a group of folders and files which are mostly notebooks.</p> <p>An admin end user once connected to the platform can do at least the following tasks:</p> <ul> <li>Manage users, groups, entitlements, instance profiles (which is is associate to a IAM role pass through attached to the EC2 instances supporting the cluster) .</li> <li>Create workspaces and defined access control.</li> <li>Manage all-purpose or job clusters.</li> <li>Define policy to control resources of the cluster: for example cluster mode specifies the level of isolation, runtime version, ML runtime with GPU access or not... It is possible to specify on-demand and spot instance composition for the number of node in the cluster. </li> <li>Submit job. It can also create a cluster to support the job, and then release the resources.</li> <li>Create data tables.</li> <li>Define MLflow: An open source platform for the machine learning lifecycle.</li> </ul> <p>Two types of compute resource:</p> <ul> <li>All purpose compute: shared cluster, ad-hoc work, multi tenant, more expensive.</li> <li>Job compute: single user, ephemeral clusters created for a job. Great isolation. Lower cost.</li> </ul>"},{"location":"techno/databricks/#unity-catalog","title":"Unity Catalog","text":"<p>The Unity Catalog offers a fine-grained governance for data lakes across cloud providers and is based on ANSI SQL. It provides a centrally shared (via delta sharing protocol), auditable (what is used, who used, data lineage), secured, management capability for all data types (tables, files, columns and ML models ). It integrates with existing data catalogs and storages.</p>"},{"location":"techno/databricks/#databricks-sql","title":"Databricks SQL","text":"<p>Run SQL queries through SQL optimized clusters, powered by Photon, that directly queries Delta Lake tables. </p> <p></p> <p>Note</p> <p>Photon is a vectorized query engine to take advantages of the new CPUs architecture for extremely fast parallel processing.</p> <p>The query editor is also integrated with visualization to present the data with diagrams and charts and build dashboards.</p> <p></p> <p>The SQL can be integrated with existing BI tools like Qlik, Tableau, DBT, Fivetran...</p> <p>To be able to execute SQL, workspace admin user needs to define an endpoint, a Catalog and a database. Serverless endpoints are managed by Databricks, or classical with remote access to data plane to customer's account.</p> <p>Catalog (<code>samples</code>) is needed to access Databases (<code>nyctaxi</code>):</p> <p></p> <p>Support all the management at the query level, looking at execution history, and fine-grained access control.</p> <p>SQL warehouse supports 3 types:</p> <ul> <li>Classic as entry level</li> <li>Pro</li> <li>Serverless: SQL warehouses run in the customer\u2019s Databricks account using serverless compute.</li> </ul> <p></p> <p>Creating a Warehouse helps to configure the cluster size and type, scaling characteristics (which lead to different pricing in DBU units), and the catalog:</p> <p></p> <p>Within a catalog, we can use Data Explorer to see the database, the tables (<code>trips</code>) and then the column declarations:</p> <p></p> <p>Using query we can define Alert.</p>"},{"location":"techno/databricks/#machine-learning","title":"Machine Learning","text":"<p>The libraries used are TensorFlow, XGBoost, Scikit-learn, pyTorch.</p> <p>The ML service uses MLFlow which includes the following components:</p> <ul> <li>Models: manage and deploy models from a variety of ML libraries to a variety of model serving and inference platforms. </li> <li>Projects: package ML code in a reusable, reproductible form to share.</li> <li>Model registry</li> <li>Model serving</li> <li>Feature serving, serves pre-computed features as well as compute on-demand features using a single REST API in real time (within milliseconds latency) for any AI applications.</li> </ul>"},{"location":"techno/databricks/#feature-serving","title":"Feature Serving","text":"<p>Serverless capability to serve pre-computed features as well as compute on-demand features using a single REST API.</p> <p>As ML usage increases across industries, the sophistication of ML pipelines is also increasing, with many customers moving from batch to real-time predictions. For real-time models to have the greatest business value, they need to be sensitive to the latest actions of the user. Feature engineering is at the core of modeling complex systems, with features being data signals used as inputs to ML models.</p> <p>When computing features from raw data, tradeoffs must be made regarding complexity, data size, freshness requirements, latency, cost and importance to predictions.</p> <p>Data freshness measures the time from a new event to the feature value being updated or available for inference.</p> <p>Architectures for computing features can be batch, streaming, or on-demand, with varying complexity and costs.</p> <ol> <li>Batch computation frameworks like Spark are efficient for slowly changing features (hours to days) or those requiring complex calculations on large data volumes. With batch, pipelines pre-compute features and materialize them in offline tables for low latency access by real-time models.</li> <li>On-demand computation computes features from latest signals at scoring time, suitable for simpler calculations on smaller data. On-demand is also suitable when features change value more frequently than being used in scoring. Examples of features best computed on-demand include user product viewing history and percentage of discounted items in a session.</li> <li>Streaming continuously pre-computes feature values on data (for data where data freshness is within minutes) streams asynchronously, for features requiring larger computation or higher data throughput than on-demand.</li> </ol> How to choose the best architecture? <p>The starting point for selecting a computation architecture is the data freshness and latency requirements. For less strict requirements, batch or streaming are first choices as they are simpler and can accommodate large computations and deliver predictable latency. For models that need to react quickly to user behavior or events, data freshness requirements are stricter and on-demand architecture is more appropriate.  Use spark structured streaming to stream the computation to offline store and online store. Use on-demand computation with MLflow pyfunc. Use Databricks Serverless realtime inference to perform low-latency predictions on your model.</p> <p>To avoid skew between online and offline computation of on-demand features, it is recommended to yse MLflow pyfunc to wrap model training/prediction with custom preprocessing logic, which helps reusing the same featurization code for both model training and prediction. Features are generated the same way both offline and online.</p> <p>Travel recommendation example notebook on AWS.</p>"},{"location":"techno/databricks/#sources","title":"Sources","text":"<ul> <li>Databricks enablement for admin.</li> <li>Product documentation.</li> </ul>"},{"location":"techno/databricks/#serverless-deployment","title":"Serverless deployment","text":"<p>The serverless deployment is running Databricks control plane as pods in EKS, and customers can run their Sparks job (SQL processing as of now), in nodes added dynamically inside EKS cluster. For security reason the Spark job manager runs in VM (k8s sandboxing pattern) to provide better isolation.</p> <p></p> <p>The goal is to support different node types: spot instance, or dedicated VM or even microVM on EC2 baremetal. In the serverless world it is possible to have thousand of VMs / nodes in kubernetes cluster and hundred of start/stop events per second.</p> <p>Each job processing is done on data that will most likely come from S3 buckets within Databricks account, in the same region, or copied from customer's S3 buckets to EBS volumes attached to the EC2. There are still some use cases where data will stay in S3 bucket of a customer's AWS account, while compute run on EKS clusters of Databricks.</p> <p>For real-time processing, Spark streaming may be used, connected to Kafka, Kinesis, and any queueing systems. Java or Scala based processing will take longer time to start than SQL based deployment.</p>"},{"location":"techno/databricks/#hands-on-enablement","title":"Hands on enablement","text":"<p>See my Apache Sparks study and ML study.</p> <ul> <li> <p>Example of python to read csv file and save it in Delta Lake format</p> <pre><code>from pyspark.sql.functions import avg\n\ndiamonds = spark.read.csv(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header=\"true\", inferSchema=\"true\")\ndiamonds.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/diamonds\")\ndisplay(diamonds.select(\"color\",\"price\").groupBy(\"color\").agg(avg(\"price\")).sort(\"color\"))\n</code></pre> </li> <li> <p>Then read it using SQL</p> <pre><code>CREATE TABLE diamonds USING DELTA LOCATION '/delta/diamonds/'\n</code></pre> </li> <li> <p>Learning portal.</p> </li> <li>Cloud Lakehouse labs Github</li> <li>Data engineering learning path - Github</li> </ul>"},{"location":"techno/decisions/drools/","title":"Drools business rule management systems","text":""},{"location":"techno/decisions/drools/#introduction","title":"Introduction","text":"<p>Open Source - https://www.drools.org/ under Apache license. The commercial version is RedHat Decision Manager. It is essentially a forward-chaining and backward-chaining inference-based rule engine, which supports Decision Management Notation decisions engine.</p> <p>The last KIE (Knowledge Is Everything) Docker image is from quay.io.</p> <pre><code>```sh\ndocker run -p 8080:8080 -p 8001:8001 -d --name business-central-workbench quay.io/kiegroup/business-central-workbench:latest\n```\n</code></pre> <p>See the  KIE Business-Central Workbench at http://localhost:8080/business-central. Business Central is the graphical user interface where you create and manage business rules. You can install it in JBoss EAP instance or on OpenShift. </p> <p>KIE Server is the server where rules and other artifacts are executed. A KIE container is a specific version of a project. Git repositories are used internally within Business Central to store all processes, rules, and other artifacts that are created in the authoring environment.</p> <p>KIE includes multiple projects: Kogito (for cloud native deployment (Quarkus - Springboot)), OptaPlanner, jBPM. See the examples in Kogito Central git repository. Kogito originates from KIE.</p> <p>For older Drools version see this blog.</p>"},{"location":"techno/decisions/drools/#getting-started","title":"Getting started","text":"<p>Getting started version 8.44</p> <p>There are different examples in the Kogito git repo for quarkus or springboot.</p>"},{"location":"techno/decisions/drools/#dmn-support","title":"DMN support","text":"<p>Decision Model and Notation (DMN) is a standard established by the Object Management Group (OMG) for describing and modeling operational decisions. It defines a XML schema to share model between platforms. Drools supports DMN.</p>"},{"location":"techno/decisions/drools/#key-constructs","title":"Key constructs","text":"<ul> <li>Rules are DRL or DMN artifacts. They have conditions adn actions parts. Use Facts to pass data to the rule engine's working memory. Production memory is where rules are stored in the Drools rule engine. Agenda keeps activated rules before execution.</li> <li>KIE session: In Drools, a KIE session stores and executes runtime data. Session can be stateless or stateful (data is retained between session invocations).</li> </ul> Some KIE session code Drools 8 <p>DRL rules are in the resources folder of the project, which is in the class path. Stateless call: <pre><code>// code compiles all the rule files found on the class path and adds the result of this compilation, \n// a KieModule object, in the KieContainer\nKieServices kieServices = KieServices.Factory.get();\nKieContainer kContainer = kieServices.getKieClasspathContainer();\nStatelessKieSession kSession = kContainer.newStatelessKieSession();\nApplicant applicant .... // the domain object model\nksession.execute(applicant);\n// passing multiple objects\nksession.execute(Arrays.asList(new Object[] { application, applicant })); \n</code></pre></p> <p>There are other way to pass parameters, like using Command and BatchExecutor.</p> <p>With stateful KIE session, the engine relies on the <code>modify</code> statement in rules to notify the Drools rule engine of changes. It  reasons over the changes and assesses impact on subsequent rule executions.</p> <ul> <li>DataSource:</li> <li>DataStore: </li> <li>RuleUnitData: Rule Units is a recommended style for implementing rules in Drools 8</li> <li>RuleUnitInstance</li> <li>RuleUnitProvider </li> </ul>"},{"location":"techno/decisions/drools/#getting-started-kogito-quarkus","title":"Getting started Kogito Quarkus","text":"<p>See documentation from kogito kie</p> <ol> <li>Create quarkus project</li> </ol>"},{"location":"techno/decisions/drools/#sources","title":"Sources","text":"<ul> <li>Drools 7.74 docs</li> <li>Drools 8.44 product documentation</li> <li>Kie Group - Drools git repo.</li> </ul>"},{"location":"techno/decisions/drools/#next-steps","title":"Next Steps","text":"<ul> <li> Develop a prototype using kogito and DRL based decision service </li> <li> Develop a prototype using kogito and DMN based decision service </li> </ul>"},{"location":"techno/integration/apic/","title":"Note on API Connect","text":""},{"location":"techno/integration/apic/#documentations","title":"Documentations","text":"<ul> <li>Cloud Pak foundational service rroduct documentation</li> <li>CP4I - Manage API product documentation</li> <li>EDA article on API as a pattern</li> <li>Dev practice for API</li> <li>See older Product tutorials</li> </ul>"},{"location":"techno/integration/apic/#there-are-four-different-components-in-api-connect-v10x","title":"There are four different Components in API Connect v10.x","text":"<ul> <li>Management System (API Manager) provides two functional roles: the API Manager and the Cloud Manager. It contains a persistent database that is used to store the configuration data about the system. It provides a rich set of RESTFul and CLI commands to automate API Management tasks for your organization. It maintains and manages the connection to the user registries that validate both providers and consumers of APIs.</li> <li>Gateway Service: is the runtime component for the API, enforcing the security constraints, rate limits, invoking backend services, ... </li> <li>Analytics Service: built on-top of Elastic Stack, store event logs, visualization and aggregation on metric data.</li> <li>Developer Portal Service: expose API to developers.</li> </ul> <p>The above screen shot is coming from the Cloud Admin console &gt; Topology, and  represents the registered services for the minimum configuration</p>"},{"location":"techno/integration/apic/#cloud-manager","title":"Cloud Manager","text":"<p>API Cloud Manager controls the infrastructure of the API Cloud.</p>"},{"location":"techno/integration/apic/#api-manager","title":"API Manager","text":"<p>API manager controls the creation, publication, and management of APIs</p>"},{"location":"techno/integration/apic/#concepts-to-keep-in-mind","title":"Concepts to keep in mind","text":"<ul> <li>API Connect users belong to organizations</li> <li>A provider organization (often shortened as p-org) is a group of people who create, publish, and maintain APIs that are then used by people in a consumer organization.</li> <li>You publish APIs by adding them to a Product and then publishing the Product to a Catalog. To be able to publish Products to a Catalog, the Catalog must be assigned at least one gateway service so that the APIs in the Product are available to be called at a gateway service endpoint.</li> <li>A catalog is a staging target that behaves as a logical partition of the gateway and the developer portal. Typically, an API provider organization uses a development catalog for testing APIs under development and a production catalog for hosting APIs that are ready for full use.</li> <li>A Space provides a level of isolation within a Catalog. In API Connect this capability is often referred to as syndication to describe the management and control of APIs that can be given to individuals, groups</li> </ul>"},{"location":"techno/integration/apic/#catalog","title":"Catalog","text":"<ul> <li>a Catalog has a one to one relationship with a Portal Site</li> <li>Catalogs provide isolated API run-time environments within a pOrg</li> <li>Catalogs can impact API consumption, and also impact logical partitioning on the API Gateway</li> </ul> <p>Within a Catalog there is the following functionality:</p> <ul> <li>Configure Gateway Services for the Catalog</li> <li>Configuring the Developer Portal Service for API consumers</li> <li>Managing API Consumer organizations</li> <li>API lifecycle management and approvals (API stage versus API publishing)</li> <li>API consumer (Application developer) on-boarding and user registries</li> <li>API endpoints - The URL for API calls and the Developer Portal are specific to a particular Catalog</li> <li>TLS Client Profiles to be used in the Catalog</li> <li>OAuth providers to secure access to APIs in the Catalog</li> <li>User defined policies - Each Catalog can also have user defined policies to extend the out of the box policies available to build APIs</li> </ul>"},{"location":"techno/integration/apic/#high-availability","title":"High availability","text":"<p>See this whitepaper</p>"},{"location":"techno/integration/apic/#demo","title":"Demo","text":"<p>The following are high level steps to get started on a new API C installation</p>"},{"location":"techno/integration/apic/#cloud-manager_1","title":"Cloud Manager","text":"<ul> <li>Verify cluster is installed</li> </ul> <pre><code>oc get apic\n</code></pre> <ul> <li>URL:  endpoint that is entered during installation, followed by /admin <pre><code># Get url\noc get routes | grep mgmt-admin \n# add /admin\nchrome http://eda-apic-mgmt-admin-cp4i.itzroks-270002161e-w57vvf-6ccd7f378ae819553d37d5f2ee142bd6-0000.sjc04.containers.appdomain.cloud/admin\n# Use common service indentification to reach the Cloud Manager UI\n</code></pre> <p>See accessing cloud manager user inteface in non CP4I installation. The secret and admin user is defined in LUR, but not used in the context of CP4I, as IAM  is used in this case. </p> <ul> <li>Configure email server to be able to send notifications to invited user. (mailserver.hursley.ibm.com on port 25)</li> <li>Select it as notification server.</li> <li>Create a provider organization from the Cloud Admin console. Select the Common Services User Registry</li> <li>Add users to the Common Services User Registry, use an existing email address of a user defined in tha IAM. One Automation Developer, </li> </ul>"},{"location":"techno/integration/apic/#develop-apis-and-products","title":"Develop APIs and products","text":"<ul> <li>Go to the API management console https://....containers.appdomain.cloud/integration/apis/cp4i/eda-apic/manager/</li> <li>Select the organization in which you want to define catalog and api product</li> <li>Define a catalog</li> <li>Go to your service (quarkus app) download the openapi doc (/q/openapi), add .yaml extension to the openapi file, and upload it</li> </ul> <p>The product 'orderlifecycleapis:1.0.0 (OrderLifeCycleAPIs)' cannot be staged because the API 'orderentitylifecycle:1.0.0 (Order entity life cycle)' is not enforced but has x-ibm-configuration.gateway value set to 'event-gateway' (it has to be absent or empty).</p>"},{"location":"techno/integration/cp4i/","title":"Cloud Pak for Integration","text":"<p>Info</p> <p>Updated 4/29/2022</p>"},{"location":"techno/integration/cp4i/#addressing-integration-challenges","title":"Addressing integration challenges","text":"<ul> <li>70% of digital transformation projects fail due to lack of integratio quality (2019).</li> <li>Manual tasks requiring expert integration skills -&gt; cp4i will bring automate integrations powered by AI</li> <li>Lack of operational visibility to improve integrations -&gt; improve quality with continuous feedback based on real-world data</li> <li>Only one integration style -&gt; Use varied integration methods and styles to you advantages</li> <li> <p>Drive new engagement models and digital transformation</p> <ul> <li>Manage APIs, define new channels</li> <li>respond to events in real-time for engaging experiences</li> <li>Access and move data in fast, secure way</li> </ul> </li> <li> <p>Accelerate integration while reducing costs</p> <ul> <li>AI powered integration: recommend integration patterns, flow mapping, and config changes</li> <li>Reuse integration: decompose larger integration into independant integrations. Integration asset repository.</li> <li>Simplify operation: easy to build with CI/CD workflows. Easy to deploy to same platform, no more central ESB</li> </ul> </li> <li> <p>Reduce exposure to business and security risks</p> <ul> <li>Secure access to critical assets</li> <li>Protect transactional integrity</li> <li>Balance workloads</li> </ul> </li> </ul> <p>The CP4I view is for the developer to use the right tool for the right problem.</p> <p></p>"},{"location":"techno/integration/cp4i/#cp4i","title":"CP4I","text":"<p>All components of CP4I can be combined in any way as required and are deployed 100% as containers. Each capability is deployed and managed by a corresponding operator.</p> <p>CP4I also provides an optional Platform Navigator, an over-arching Management UI layer that provides a common UI experience for various installed integration capabilities. Platform Navigator is not required to deploy individual CP4I capabilities, as each integration capability can be deployed independently leveraging its cloud native deployment operators.</p> <p>Currently, foundational services mostly are focused on identity and access management for Platform Navigator UI, single sign-on experience for UI, license metering</p>"},{"location":"techno/integration/cp4i/#20214-release","title":"2021.4 release","text":"<ul> <li>Event Endpoint Management to support buying at the API calls per month level to access kafka events or at the resource usage level.</li> <li>AI-driven API Test generation is designed to automate the process of generating these API test cases.  Watson Insights for suggested tests are generated through analysing production OpenTracing data.  This helps to determine distinct behaviours in an API implementation</li> <li>Support OpenShift on IBM z Integrated Facility for Linux, and on IBM Power Systems</li> </ul>"},{"location":"techno/integration/cp4i/#202221","title":"2022.2.1","text":"<ul> <li>no more dependencies on RWX storage</li> <li>interactive install guide: from the IBM Docs</li> <li>combined integration</li> <li>enhanced observability</li> </ul>"},{"location":"techno/integration/cp4i/#foundational-services-bedrock-overview","title":"Foundational services (bedrock) overview","text":"<ul> <li>layer between OCP and the cloud pak</li> <li> <p>Origanized into 5 distinct pillars</p> </li> <li> <p>application services</p> </li> <li>data &amp; event services</li> <li>operational services</li> <li>security services</li> <li> <p>user experience services</p> </li> <li> <p>Want to be able to start small.</p> </li> <li>Certification for k8s process in place and key for foundational services layer</li> <li>pluggability: taking advantage of existing vendor's public cloud.</li> <li>Based on operators</li> </ul> <p></p>"},{"location":"techno/integration/cp4i/#demo-videos","title":"Demo videos","text":"<ul> <li>Natural Language Integration\u00a0- Describe desired integration flow simply using natural language. Demo video</li> <li>Transformation\u00a0Generator\u00a0-\u00a0Define the format of source and target data, and automatically generate the complex mapping transformation. Demo</li> <li>AI for API Testing\u00a0- Automatically observe API traffic in a prod environment, identify where it lacks test coverage, and build tests with no human input. Demo</li> <li>Mapping\u00a0Assist\u00a0- Intelligent automation that maps a source to target using semantic understanding. Demo</li> <li>Cloud Native HA for MQ\u00a0-\u00a0Automatically replicates message data between MQ cloud instances, maintaining continual availability of the messaging service-\u00a0all without the cost and complexity of shared external storage.\u00a0Demo</li> </ul>"},{"location":"techno/integration/cp4i/#messaging-positioning","title":"Messaging positioning","text":"<ul> <li>75% of companies will have adopted cloud techno and hybrid multi-cloud. </li> <li>digital interactions with end-users have increased since the beginning of covid. </li> <li>Messaging connects those digital endpoints and their data \u000bin the most flexible, highly available, and scalable way</li> </ul> <p>Messaging spans three pillars: </p> <ol> <li>Business Critical Communication: Application and systems across enterprises, within their data centers or clouds need to be able to communicate reliably, securely and simply. Technologies such as Hypertext Transfer Protocol (HTTP) and REST APIs provide communication of data between applications, however they often lack the consistent quality of service required for business critical communications such as exactly once or guaranteed delivery. With Business Critical Communications, two applications are communicating with one sending a request message, directed towards the other application, to complete a specific function. The source application is knowledgeable that the other application exists and that it can complete this particular function. This could be part of an overall chained business process so we sometimes refer to this as conversational messaging</li> <li>Event Driven Enterprise: In the case of Event Driven Enterprises, events are emitted from applications to an event stream, the source application has no knowledge of who or what is listening to the events being emitted, or even if anyone is. The source application is normally completely disconnected from the target application. In addition, the event stream stores all emitted events, and this stream history is available to consuming applications, in the cases of failure.</li> <li>High Speed Transfer: Sometimes it may be necessary to move data over long distances or poor networks \u2013 and distance typically degrades conditions on all networks \u2013 while data is in transit, it\u2019s important for it to be secure\u2026. between on-premises and hybrid cloud environments regardless of size, distance, or network conditions.</li> </ol>"},{"location":"techno/integration/cp4i/#why-mq","title":"Why MQ","text":"<p>Secure </p>"},{"location":"techno/integration/cp4i/#installation-steps-overview","title":"Installation Steps Overview","text":"<p>There are different sources for installing Cloud Pak for integration.</p> <ul> <li>Product documentation</li> <li>Installation guide web app</li> <li>Joel Gomez's Tinkering CP4I site</li> <li>The EDA gitops catalog for operators, and operand definitions. The readme for this project is kept up to date.</li> <li>Real-time inventory gitops</li> <li>KC Solution gitOps</li> </ul>"},{"location":"techno/integration/cp4i/#decide-if-the-operators-are-installed-at-namespace-scope-or-at-cluster-level","title":"Decide if the operators are installed at namespace scope or at cluster level.","text":"<p>With namespace scope, each project effectively behaves as a different tenant. There can be one Platform Navigator installed in each namespace, and that Platform Navigator owns only the instances in that namespace. A single instance of IBM Cloud Pak foundational services is installed in the <code>ibm-common-services</code> namespace.</p> <p>Here are the operators to be installed:</p> <ul> <li>IBM Cloud Pak for Integration: Top level Cloud Pak for Integration operator that install all other Cloud Pak for Integration operators automatically</li> <li>IBM Cloud Pak for Integration Platform Navigator: Provides a dashboard and central services</li> <li>IBM Automation foundation assets: Stores, manages, retrieves and searches for integration assets</li> <li>IBM Cloud Pak for Integration Operations Dashboard tracing across instances to allow troubleshooting</li> <li>Then any product specific operator.</li> </ul>"},{"location":"techno/integration/cp4i/#prepare-the-openshift","title":"Prepare the OpenShift","text":"<ol> <li> <p>Prepare a suitable Red Hat OpenShift cluster with suitable storage.</p> <p>IBM Cloud storage supports</p> </li> <li> <p>Make the operators available to the cluster. </p> <p>IBM Operator Catalog provides certified operators for IBM products that can be deployed using the Red Hat OpenShift Operator Hub.</p> </li> <li> <p>Install the operators using the Operator Hub and Operator Lifecycle Manager.</p> </li> <li> <p>Deploy capabilities and runtimes using the operators.</p> <p><code>Operators &gt; OperatorHub &gt; Integration &amp; Delivery &gt; IBM Cloud Pak for Integration operator</code>.</p> </li> </ol>"},{"location":"techno/integration/cp4i/#considerations","title":"Considerations","text":"<p>The first four steps are done by cluster administrator, while developer or solution administrator deploys operator runtimes and define operands.</p> <p>If the operators are installed at namespace scope, each namespace effectively behaves as a different tenant.</p> <p>All IBM Cloud Paks installed in a cluster must be installed in the same mode, we cannot mix cross namespaces or within a namespace.</p> <p>When Cloud Pak for Integration is installed in the All namespaces (the operator is installed in <code>openshift-operators</code>),  there can be only one Platform Navigator per cluster, and all Cloud Pak instances are owned by that Platform Navigator.</p> <p>A single instance of IBM Cloud Pak foundational services is installed in the <code>ibm-common-services</code> namespace if the foundational services  operator is not already installed on the cluster.</p> <p>Operators need a small set of cluster level permissions to allow manipulation of resources defined at cluster scope, such as reading Custom Resource Definitions.</p> Other resources <ul> <li>Product documentation</li> <li>Red Hat marketplace for CP4I</li> </ul>"},{"location":"techno/integration/eepm/","title":"Event endpoint management","text":"<p>Event Endpoint management is a platform to manage asyncAPI definitions and enforces consumer applications to go to event gateway to control consumer traffic.</p> <p></p> <p>Next section is for the flows: 1,2,3,4. </p>"},{"location":"techno/integration/eepm/#process-of-managing-asyncapi","title":"Process of managing AsyncAPI","text":"<p>A Kafka topic owner can define an asyncAPI bottom up (using editors and may tools to extract data from schema, cluster and topic definition), or uses API management to define an asyncAPI document from an existing deployed topic:</p> <ul> <li>From the API Connect home page</li> </ul> <p></p> <ul> <li>He defines asyncAPI with basic name, version and summary</li> </ul> <p></p> <ul> <li>completed with Kafka server bootstrap URL, topic name and schema definition:</li> </ul> <p></p> <ul> <li>complement with how to securely access the Kafka cluster, using a user with consumer ACL on specific topic, and the CA certificate in .pem format:</li> </ul> <p></p> <ul> <li>As the API needs to be visible in the Catalog, he publishes it as a product:</li> </ul> <p></p> <ul> <li>The outcome is the address of the event gateway and API key and secret to be used by future applications:</li> </ul> <p></p> <ul> <li>Newly create product is in the sandbox catalog:</li> </ul> <p></p> <p></p> <p>Once in the catalog the product is visible in the API portal so other developers can subscribe to the product. See tech-adamy lab for that.</p>"},{"location":"techno/integration/eepm/#event-endpoint-management-deployment","title":"Event Endpoint Management deployment","text":""},{"location":"techno/integration/eepm/#cp4i-installation","title":"CP4I Installation","text":"<ul> <li>Product documentation - installation</li> <li>IBM recommends installing the operators for Platform Navigator, Automation Foundation assets, and Operations Dashboard because they assist in the deployment and management of the other capabilities.</li> <li>Typically, a cluster administrator installs the operators, and an automation administrator creates the custom resources</li> <li>Select installation mode for the operators: All namespace (in the <code>openshift-operators</code>) or specific namespace: operator only processes resources created in that namespace</li> <li>If the operators are installed at cluster scope, the entire cluster effectively behaves as one large tenant.</li> <li>If the operators are installed at namespace scope, each namespace effectively behaves as a different tenant.</li> <li>With All namespace there can be only one Platform Navigator installed per cluster, and all Cloud Pak instances are owned by that Platform Navigator.</li> <li>Verify the cluster scope permission needed per capabilities in this product doc</li> <li>For both installation modes, by default a single instance of IBM Cloud Pak foundational services is installed in the <code>ibm-common-services</code> namespace if the foundational services operator is not already installed on the cluster.</li> </ul>"},{"location":"techno/integration/eepm/#eepm-deployment","title":"EEPM deployment","text":"<ul> <li>Product documentation - deployment</li> <li>Install an instance of the Event Endpoint Management capability in a single namespace.</li> <li>Event Endpoint Management is installed and configured by the API Connect operator through the EventEndpointManager custom resource type</li> <li>There are different deployment profiles: minimum with one node, and then 3 nodes </li> <li> <p>Use yaml from one of the solution, be sure to include the namespace for the target project where EEPM will run.</p> <pre><code># For rt-inventory demo\noc apply -k environments/rt-inventory-dev/services/event-endpoint/overlays\n</code></pre> </li> <li> <p>Verify</p> <pre><code>oc get eventendpointmanager eda-eepm -ojsonpath={.status.phase}\n</code></pre> <p>The installation creates the following pods</p> pods description postgres-operator a dedicated operator to manage local postgresql servers postgreql server backrest repo an postgresql server for REST  repository pgbouncer LDAP nats operator NATS middleware operator nats cluster NATS servers </li> </ul> <p>...wait some long time...</p>"},{"location":"techno/integration/eepm/#getting-started","title":"Getting Started","text":""},{"location":"techno/integration/eepm/#api-connect-components","title":"API Connect components","text":"<p>An IBM API Connect cluster (defined through its APIConnectCluster Custom Resource Definition - CRD), deploys the following subsystems:</p> <ul> <li>Management Cluster (ManagementCluster CRD)</li> <li>Portal Cluster (PortalCluster CRD)</li> <li>Gateway Cluster (GatewayCluster CRD)</li> <li>Analytics Cluster (AnalyticsCluster CRD)</li> </ul> <p></p> Info <p>The reason for the IBM API Connect subsystems to be called clusters is because each of those can be easily deployed highly available by using the profile property. As a result, it is common to see and strongly recommended, specially on production environments, to deploy all of these IBM API Connect subsystems using their highly available profile, which will deploy 3 replicas of each componet.</p> <ul> <li>Key initial configuration tasks that you must complete in the Cloud Manager user interface after installing and deploying IBM\u00ae API Connect</li> </ul>"},{"location":"techno/integration/eepm/#registering-event-gateway-service","title":"Registering event gateway service","text":"<p>Access to your event sources can be controlled by the Event Gateway Service. The service enforces runtime policies to secure and control access to Kafka topics hosted on one or more backend Kafka clusters. The Evt Gtw Service needs to be in the catalog. See these instructions to register the service.</p> <pre><code># example for getting the URL\noc get eventgatewaycluster apim-demo-egw -ojsonpath='{.status.endpoints[?(@.name==\"eventGateway\")].uri}'\n</code></pre> <p>To see current TLS certificated used by the service, go to Cloud Manager &gt; Manage resources &gt; TLS &gt; keystore</p>"},{"location":"techno/integration/eepm/#connecting-the-consumer-app","title":"Connecting the consumer app","text":"<p>See the tech-academy lab for subsribing to the AsyncAPI and get server credential.</p> More Reading <ul> <li>AsyncAPI - summary</li> <li>Developer ibm share-event-based-apis-with-event-endpoint-management</li> <li>Dale Lane blog- A Kafka Developer\u2019s Guide to AsyncAPI</li> </ul>"},{"location":"techno/integration/mq/","title":"IBM MQ compendium","text":"<p>IBM MQ is the enterprise solution to exchange message over queues.</p> <p>As it supports loosely coupling communication between applications, via asynchronous protocol, and message exchange, it has to be part of any modern digital, responsive solutions, and so it makes sense to write about it in the context of EDA.</p> <p>It is used for years to support asynchronous communication in mainframe applications. </p>"},{"location":"techno/integration/mq/#overview","title":"Overview","text":"<ul> <li>My own MQ summary in the EDA reference architecture site to get summary of the main concepts, the benefits in an EDA, and installation with Cloud Pak for Integration.</li> </ul> <p>A queue manager can be thought of in two parts: the data stored on disk, and the running processes that allow access to the data. </p> <p>Queue managers can be connected together via network channels to allow messages to flow between disparate systems and applications on different platforms including on-premise and cloud systems</p>"},{"location":"techno/integration/mq/#mq-value-propositions","title":"MQ value propositions","text":"<ul> <li>No data loss, no duplicate</li> <li>Integrate with transaction</li> <li>Scale horizontally: add more queue managers to share tasks and distribute the messages across them.  MQ Clusters will even intelligently route messages to where they\u2019re needed.  The world is full of horizontally scaled MQ systems that handle billions of messages a day.</li> <li>High availability with replicated Queue managers. Active/active horizontal scaling for always on systems</li> <li>Lightweight and scale to run in any size</li> <li>Containerized, runs in orchestration like OpenShift</li> </ul>"},{"location":"techno/integration/mq/#comparing-mq-pubsub-and-kafka","title":"Comparing MQ Pub/Sub and Kafka","text":"<p>Assess the following characteristics:</p> <ul> <li>Event History: does the solution need to be able to retrieve historical events either during normal and/or failure situations? If so for how long and how much data?</li> <li>Fined Grained Subscriptions: Should applications receive all event types from a topic? (Even events that are irrelevant to them ) With MQ topics, they are hierarchical, so it is possible for consumer apps to subscribe to different level of the topic. With Kafka topics  are partitioned and consumers get all the events from one to many partition.</li> <li>Scalable Consumption: if 100 consumers subscribe to all events on a topic, IBM MQ will create 100 messages for each published event (with the exception of multicast Pub/Sub). Each of event will be stored and,  if required, persisted to disk using system resources.</li> <li>Transactional Behavior: With pub/sub thee tx requirement is less critical, than in queues</li> </ul>"},{"location":"techno/integration/mq/#high-availability","title":"High Availability","text":"<p>High availability for IBM MQ in containers proposes 3 HA configurations.</p> <p></p> <p>See my summary here</p> <ul> <li>Native HA queue managers involve an active and two replica Kubernetes Pods, which run as part of a Kubernetes StatefulSet with exactly three replicas each with their own set of Kubernetes Persistent Volumes. It only runs in the same k8s cluster.</li> </ul> <p></p> <p>Any queue manager can be moved to a different Kubernetes Node, as long as it keeps the same data (provided by Kubernetes Persistent Volumes) and is still addressable across the network by client applications.</p> <p>If the PVs are served via a SAN then there will be replications done at the SAN level too.</p> <ul> <li>Multi instance active - standby topology with PV RWM settings.</li> </ul>"},{"location":"techno/integration/mq/#clustering-and-distributed-queuing","title":"Clustering and distributed queuing","text":"<p>Distributed queuing means sending messages from one queue manager to another. Clustering QM simplifies the communication configuration between QMs.</p> <p></p> <p>Each queue manager has a definition for each of its queues (local or remote).</p> <p>If the messages are destined for a remote queue, the local queue manager holds them on a transmission queue, which persists them in a message store, until they can be forwarded to the remote queue manager.</p> <p>Each queue manager contains communications software, known as the moving service, that the queue manager uses to communicate with other queue managers.</p> <p>A channel is a one-way communication link between two queue managers.</p> <p>The software that handles the sending and receiving of messages is called the Message Channel Agent (MCA). There is a message channel agent (MCA) at each end of a channel.</p>"},{"location":"techno/integration/mq/#replicated-data-queue-managers","title":"Replicated Data Queue Managers","text":"<p>MQ Advanced supports synchronous replication and fast quorum based take over for HA scenarios, recovery in seconds.  It also supports asynchronous replication between quorum groups to support long distance DR deployments.</p> <p>All nodes support concurrently running multiple different active queue managers with bidirectional asynchronous replication, supporting active/active HA and DR topologies.</p> <p>Use queue manager leader and two replicas. So messages are replicated in three locations.  Those are exact replicas, maintaining configuration, message order, transactional state. Quorum ensures consistency and rapid failure (within a second) and recovery.</p> <p>The  MQ operator helps to deploy MQ manager with declarative manifest, with native HA, cross availability zones, with all the networking services, and storage needed. Applications connected to MQ manager do not know how many queue managers are behind their request.</p>"},{"location":"techno/integration/mq/#always-on-mq","title":"Always on MQ","text":"<p>MQ provides a uniform cluster consisting of multiple active queue managers acting as a single messaging service.</p> <p>Each Queue manager has the same resource, queues, channels...</p> <p>Applications can connect to any of the queue managers within the uniform cluster. This removes any dependency on a specific queue manager, resulting in better availability and workload balancing of messaging traffic. </p> <p>The queue managers  are configured almost identically, so that an application can interact with them as a single group.</p>"},{"location":"techno/integration/mq/#insight-to-your-data","title":"Insight to your data","text":"<p>Stream MQ data by adding a new queue and specify the original queue the  name of the streaming queue</p>"},{"location":"techno/integration/mq/#mqsc-commands","title":"MQSC commands","text":"<p>Use MQSC commands to manage queue manager objects, including the queue manager itself, queues, process definitions, channels, client connection channels, listeners, services, namelists, clusters, and authentication information objects.</p> <ul> <li>Example of common commands: (start a bash in the docker image and use <code>runmqsc</code> tool)</li> </ul> <pre><code>display queue(rawtx)\n</code></pre> <p>Those MQSC commands can be define in a config map to be loaded inside the docker image. See example of such map here</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mq-mqsc-config\ndata:\n  example.mqsc: |\n    DEFINE QLOCAL('ITEMS') REPLACE\n    DEFINE CHANNEL('DEV.ADMIN.SVRCONN') CHLTYPE(SVRCONN) REPLACE\n    DEFINE QLOCAL('DEV.DEAD.LETTER.QUEUE') REPLACE\n    ALTER QMGR DEADQ('DEV.DEAD.LETTER.QUEUE')\n    DEFINE CHANNEL(DEV.APP.SVRCONN) CHLTYPE(SVRCONN) \n    ALTER QMGR CHLAUTH (DISABLED)\n    REFRESH SECURITY TYPE(CONNAUTH)\n</code></pre>"},{"location":"techno/integration/mq/#amqp","title":"AMQP","text":"<p>Advanced Message Queuing Protocol is a standard to integrate with messaging product. IBM MQ supports AMQP 1.0.</p> <ul> <li>MQ supports AMQP communication on port 5672, and defining a AMQP Channel. </li> <li>The Apache Qpid JMS library uses the AMQP 1.0 protocol to provide an implementation of the JMS 2 specification.</li> <li>AMQP channel are managed in the same way as other MQ channels</li> <li> <p>Using IBM MQ, Apache Qpid JMS applications can do publish/subscribe messaging and point-to-point messaging</p> </li> <li> <p>Good article \"Developing JMS apps with Quarkus and GraalVM\". it includes how to build a custom MQ docker image.</p> </li> <li>Building an image with custom MQSC and INI files, using the Red Hat OpenShift CLI</li> </ul>"},{"location":"techno/integration/mq/#build-custom-mq-docker-image","title":"Build custom MQ docker image","text":"<p>Here are the step to build a custom image for MQ</p> <ul> <li>Clone https://github.com/ibm-messaging/mq-container.git</li> <li>Select the branch for the version of MQ to be used: <code>git checkout 9.2.5</code>  or <code>git checkout master</code></li> <li>For production image download last MQ release tar file from ibm support or ppa (Select something like <code>IBM MQ 9.3 Long Term Support Release for Containers for Linux on x86 64-bit Multilingual</code>), then copy the <code>tar.gz</code> file  to <code>mq-container/downloads</code> folder</li> <li>For developer image no need to download it, go to developerwork site to get the image</li> <li> <p>Edit the <code>install-mq.sh</code> file and change the following variable to use AMQP channel</p> <pre><code>export genmqpkg_incamqp=1\n</code></pre> </li> <li> <p>Set up AMQP authority, channel, and service properties by adding the contents of the add-dev.mqsc.tpl file  to the bottom of the <code>/incubating/mqadvanced-server-dev/10-dev.mqsc.tpl</code> file in your cloned repository</p> </li> <li> <p>Start the build</p> </li> </ul> <pre><code>export MQ_ARCHIVE_DEV=IBM_MQ_9.3_LIN_X86-64_NOINST.tar.gz \nexport MQ_VERSION=9.3 \nexport LTS=true \nexport DOCKER_BUILDKIT=0 \nexport REGISTRY_USER=jbcodeforce  \nexport REGISTRY_PASS='quotethepasswordandescape$with\\$'\n# Development image\nmake build-devserver\n# Production image\nmake build-advancedserver\n</code></pre> <p>If you get this error: <code>Error response from daemon: network with name build already exists</code> do</p> <pre><code>docker stop build-server \ndocker network rm build\n</code></pre> <p>Error <code>This is due that docker build does not support</code>--network` anymore.</p>"},{"location":"techno/integration/mq/#configuring-the-queue-manager","title":"Configuring the Queue Manager","text":"<p>It is recommended that you configure MQ in your own custom image by  adding your own MQSC file into the <code>/etc/mqm</code> directory on the image. This file will be run when your queue manager is created. See this tech note.</p> <p>Another way is to use <code>mqsc</code> inside of the running container:</p> <pre><code>docker exec -ti ibmmq bash\ndspmq\n</code></pre>"},{"location":"techno/integration/mq/#compendium","title":"Compendium","text":"<ul> <li>Release 9.3</li> <li>Developer IBM articles - query</li> <li>Getting started with MQ</li> <li>Run IBM\u00ae MQ in a container with relevant developer article. and Usage of the docker image.</li> <li>MQ download </li> <li>MQ fundamentals: nice set of diagrams and explanation of queues, MQ managers...</li> </ul> <p>With links to supporting programming languages. * MQ Cheat sheet * High availability * IBM MQ samples and patterns best source to get you jump straight in and play * AsyncAPI MQ Bindings * Administering a queue manager using IBM MQ Console - IBM Cloud * First demo on docker * Develop a JMS point to point application The code of this IBM tutorial is also in this repository under the <code>democlient/MQJMSClient</code> folder so we can test the configuration.</p>"},{"location":"techno/integration/mq/#coding-and-personal-studies","title":"Coding and personal studies","text":""},{"location":"techno/integration/mq/#mq-messaging-coding-challenge","title":"MQ messaging coding challenge","text":"<p>See the MQ Challenges in java note and the Java-studies/mqChallenges.</p> <p>Some comments:</p> <ul> <li>When a publisher publishes a message to a topic string, one or more subscribers for that topic string receives the  message</li> <li>A JMS application can use the JMS destination object which maps to a topic in the same way as it would use  the destination to map to a queue, in a point to point scenario. For the publication to reach the subscriber  successfully, both the publisher and the subscriber must match same topic string. The subscriber will get  publications only from the time they subscribe to a topic.</li> <li>If a publication is sent before the subscription by a specific application is created, that application will not get it.</li> <li>Request response or request reply is an integration or messaging pattern where the application that sends a message to another application, requires a reply of some sort from the receiving application.</li> <li>This is often based on the point to point messaging style and can be synchronous (the sending application waits for the response before it times out) and asynchronous (also called request/callback, where the sending application disconnects but sets up a callback to handle a reply).</li> <li>The sending application usually sets a reply-to-destination and a correlation-id so that a response can get back to the right sender application.</li> <li>For the event booking service the reply-to destination has been defined administratively on the queue manager. However, the requester could dynamically create a temporary destination from its JMS session to complete the exchange.</li> </ul> <p>JMS topic subscription code is in TicketSubscriber.java</p>"},{"location":"techno/integration/mq/#code-repositories","title":"Code repositories","text":"<ul> <li>refarch-eda-store-simulator a quarkus app with JMS producer to MQ </li> <li>The Real time inventory gitops includes MQ broker deployment descriptor, kafka connect definitions...</li> <li>eda-lab-mq-to-kafka: A hands-on lab series to demonstrate end-to-end integration between a JMS application using JMS to MQ and then Kafka Connect and Kafka topics, sending sold item data from different stores to MQ and Kafka using an MQ Kafka connectors.</li> <li>refarch-container-inventory older JEE app still need work</li> <li>refarch-mq-messaging, need a deep refresh.</li> <li>mqChallenges in Java Studies repo with the challenge code and other quarkus - mq one.</li> <li>AMQP for life insurance demo - simulator</li> <li> <p>Saga with MQ - gitops</p> <ul> <li>eda-kc-order-cmd-mq</li> <li>eda-kc-reefer-kn-mq</li> <li>eda-kc-voyage-ms-mq</li> </ul> </li> </ul>"},{"location":"techno/scaleway/","title":"Summary of Scaleway cloud","text":"<p>Cloud provider in Europe</p>"},{"location":"techno/scaleway/#compute","title":"Compute","text":"<p>VM or Bare metal, see offerings, with pricing information.</p> <p>There is a scaleway CLI which can run in a docker image</p> <pre><code>docker run -i --rm scaleway/cli:latest\n</code></pre>"},{"location":"techno/scaleway/#container","title":"Container","text":""},{"location":"techno/scaleway/#container-registry","title":"Container Registry","text":"<p>A fully-managed mutualized container registry. It uses the concept of namespace for isolation per region. A namespace can either be public or private. Anyone will be able to pull container images from a public namespace. Privacy policies may be set at image level.</p> <p>The command to push image from local computer.</p> <pre><code>docker login rg.fr-par.scw.cloud/mynamespace -u nologin -p [SCW_SECRET_KEY]\n# Push \ndocker tag my_new_image:latest rg.fr-par.scw.cloud/mynamespace/my_new_image:latest\ndocker push rg.fr-par.scw.cloud/mynamespace/my_new_image:latest\n</code></pre>"},{"location":"techno/scaleway/#serverless-container","title":"Serverless container","text":"<p>Documentation.</p>"},{"location":"techno/scaleway/#kubernetes","title":"Kubernetes","text":"<p>For K8s the service to manage clusters is Kapsule. See Kubernetes doc. But to create clusters including Instances from external cloud providers, the product is Kubernetes Kosmos.</p>"}]}